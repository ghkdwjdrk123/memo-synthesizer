# ë…¸ì…˜ ê¸°ë°˜ ì•„ì´ë””ì–´ ì¡°í•© ì„œë¹„ìŠ¤ MVP

## Project Overview

ë…¸ì…˜ ë©”ëª¨ì—ì„œ ì‚¬ê³  ë‹¨ìœ„ë¥¼ ì¶”ì¶œí•˜ê³ , ì•½í•œ ì—°ê²°ì„ ì°¾ì•„ ê¸€ê°ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤

**íŒŒì´í”„ë¼ì¸:** RAW â†’ NORMALIZED â†’ ZK â†’ Essay

## Language

Always all output must write in Korean.

---
language: korean
---

# ğŸš¨ ìµœìš°ì„  ê·œì¹™

| ê·œì¹™ | ì„¤ì • |
|-----|------|
| **ì¶œë ¥ ì–¸ì–´** | í•œêµ­ì–´ (ì˜ˆì™¸ ì—†ìŒ) |
| **ì½”ë“œ ì£¼ì„** | í•œêµ­ì–´ |
| **ì»¤ë°‹ ë©”ì‹œì§€** | í•œêµ­ì–´ |

> âš ï¸ ì´ ê·œì¹™ì€ /compact, ì„¸ì…˜ ì¬ê°œ ë“± ëª¨ë“  ìƒí™©ì—ì„œ ìœ ì§€ë©ë‹ˆë‹¤.


## Tech Stack

- **Backend:** FastAPI, Python 3.11+
- **Frontend:** Next.js 14 (App Router), TypeScript, Tailwind CSS
- **Database:** Supabase (PostgreSQL + pgvector)
- **LLM:** Claude 3.5 Sonnet (Anthropic), text-embedding-3-small (OpenAI)
- **External API:** Notion API

## Directory Structure

```
backend/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ supabase_service.py             # DB CRUD + pgvector + RPC í˜¸ì¶œ
â”‚   â”œâ”€â”€ ai_service.py                   # LLM calls
â”‚   â”œâ”€â”€ notion_service.py               # Notion API
â”‚   â”œâ”€â”€ candidate_mining_service.py     # ìƒ˜í”Œë§ ê¸°ë°˜ í›„ë³´ ë§ˆì´ë‹
â”‚   â”œâ”€â”€ distribution_service.py         # ì „ì—­ ë¶„í¬ ê³„ì‚°
â”‚   â””â”€â”€ rate_limiter.py                 # API rate limiting
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ pipeline.py                 # Pipeline endpoints
â”‚   â””â”€â”€ essays.py                   # Essay CRUD
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ raw.py                      # RawNote models
â”‚   â”œâ”€â”€ normalized.py               # ThoughtUnit models
â”‚   â”œâ”€â”€ zk.py                       # ThoughtPair models
â”‚   â”œâ”€â”€ essay.py                    # Essay models
â”‚   â””â”€â”€ processing.py               # ProcessingStatus
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.py               # JSON parsing
â”‚   â””â”€â”€ error_handlers.py           # Exception handling
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ unit/
    â””â”€â”€ integration/

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx          # Main dashboard
â”‚   â”‚   â””â”€â”€ login/page.tsx    # Login page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ EssayCard.tsx
â”‚   â”‚   â”œâ”€â”€ PipelineControl.tsx
â”‚   â”‚   â””â”€â”€ DatabaseSelector.tsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts            # Backend client
â”‚   â”‚   â””â”€â”€ types.ts          # TypeScript types
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ usePipeline.ts
â”‚       â””â”€â”€ useAuth.ts
â””â”€â”€ __tests__/
```

## Database Schema

```sql
-- 1. RAW ë ˆì´ì–´: Notion ì›ë³¸
CREATE TABLE raw_notes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    notion_page_id TEXT UNIQUE NOT NULL,
    notion_url TEXT NOT NULL,
    title TEXT,
    content TEXT,
    properties_json JSONB DEFAULT '{}'::jsonb,
    notion_created_time TIMESTAMPTZ NOT NULL,
    notion_last_edited_time TIMESTAMPTZ NOT NULL,
    imported_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_raw_notes_notion_page_id ON raw_notes(notion_page_id);

-- 2. NORMALIZED ë ˆì´ì–´: ì‚¬ê³  ë‹¨ìœ„ + ì„ë² ë”©
CREATE TABLE thought_units (
    id SERIAL PRIMARY KEY,
    raw_note_id UUID NOT NULL REFERENCES raw_notes(id) ON DELETE CASCADE,
    claim TEXT NOT NULL,
    context TEXT,
    embedding vector(1536),  -- OpenAI text-embedding-3-small
    embedding_model TEXT DEFAULT 'text-embedding-3-small',
    extracted_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_thought_units_raw_note ON thought_units(raw_note_id);
CREATE INDEX idx_thought_units_embedding ON thought_units
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- 3. ZK ë ˆì´ì–´: ì„ íƒëœ í˜ì–´
CREATE TABLE thought_pairs (
    id SERIAL PRIMARY KEY,
    thought_a_id INTEGER NOT NULL REFERENCES thought_units(id),
    thought_b_id INTEGER NOT NULL REFERENCES thought_units(id),
    similarity_score FLOAT NOT NULL CHECK (similarity_score >= 0 AND similarity_score <= 1),
    connection_reason TEXT,
    selected_at TIMESTAMPTZ DEFAULT NOW(),
    is_used_in_essay BOOLEAN DEFAULT FALSE,
    CONSTRAINT different_thoughts CHECK (thought_a_id != thought_b_id),
    CONSTRAINT ordered_pair CHECK (thought_a_id < thought_b_id),
    UNIQUE(thought_a_id, thought_b_id)
);
CREATE INDEX idx_thought_pairs_unused ON thought_pairs(is_used_in_essay)
WHERE is_used_in_essay = FALSE;

-- 4. Essay ê²°ê³¼ë¬¼
CREATE TABLE essays (
    id SERIAL PRIMARY KEY,
    type TEXT DEFAULT 'essay',
    title TEXT NOT NULL,
    outline JSONB NOT NULL,  -- ["1ë‹¨: ...", "2ë‹¨: ...", "3ë‹¨: ..."]
    used_thoughts_json JSONB NOT NULL,
    reason TEXT NOT NULL,
    pair_id INTEGER NOT NULL REFERENCES thought_pairs(id),
    generated_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_essays_generated_at ON essays(generated_at DESC);

-- 5. ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
CREATE TABLE processing_status (
    id SERIAL PRIMARY KEY,
    raw_note_id UUID NOT NULL REFERENCES raw_notes(id) ON DELETE CASCADE,
    step TEXT NOT NULL,  -- 'extract_thoughts', 'create_embedding', 'select_pairs', 'generate_essay'
    status TEXT NOT NULL,  -- 'pending', 'processing', 'completed', 'failed'
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(raw_note_id, step)
);
CREATE INDEX idx_processing_status_step_status ON processing_status(step, status);

-- 6. thought_units.rand_key: ê²°ì •ë¡ ì  ìƒ˜í”Œë§ìš© (NEW)
-- thought_units í…Œì´ë¸”ì— rand_key ì»¬ëŸ¼ ì¶”ê°€
ALTER TABLE thought_units ADD COLUMN rand_key DOUBLE PRECISION DEFAULT random();
CREATE INDEX idx_thought_units_rand_key ON thought_units (rand_key);

-- 7. similarity_samples: ì „ì—­ ë¶„í¬ ìŠ¤ì¼€ì¹˜ìš© ìƒ˜í”Œ ì €ì¥ (NEW)
CREATE TABLE similarity_samples (
    id BIGSERIAL PRIMARY KEY,
    run_id UUID NOT NULL,
    similarity FLOAT NOT NULL CHECK (similarity >= 0 AND similarity <= 1),
    src_id INTEGER,  -- ë””ë²„ê¹…ìš© (ì„ íƒì )
    dst_id INTEGER,  -- ë””ë²„ê¹…ìš© (ì„ íƒì )
    seed INTEGER,
    policy TEXT DEFAULT 'random_pairs',
    created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_ss_run_id ON similarity_samples (run_id);
CREATE INDEX idx_ss_created_at ON similarity_samples (created_at DESC);

-- 8. pair_candidates: ë§ˆì´ë‹ëœ í›„ë³´ í˜ì–´ (NEW)
CREATE TABLE pair_candidates (
    id SERIAL PRIMARY KEY,
    thought_a_id INTEGER NOT NULL REFERENCES thought_units(id),
    thought_b_id INTEGER NOT NULL REFERENCES thought_units(id),
    similarity FLOAT NOT NULL,
    raw_note_id_a UUID,
    raw_note_id_b UUID,
    llm_score INTEGER,
    llm_status TEXT DEFAULT 'pending',
    llm_attempts INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    CONSTRAINT pc_different CHECK (thought_a_id != thought_b_id),
    CONSTRAINT pc_ordered CHECK (thought_a_id < thought_b_id),
    UNIQUE(thought_a_id, thought_b_id)
);
CREATE INDEX idx_pc_similarity ON pair_candidates (similarity);
CREATE INDEX idx_pc_llm_status ON pair_candidates (llm_status);

-- 9. pair_mining_progress: ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ ì¶”ì  (NEW)
CREATE TABLE pair_mining_progress (
    id SERIAL PRIMARY KEY,
    run_id UUID DEFAULT gen_random_uuid(),
    last_src_id INTEGER NOT NULL DEFAULT 0,
    total_src_processed INTEGER NOT NULL DEFAULT 0,
    total_pairs_inserted BIGINT NOT NULL DEFAULT 0,
    avg_candidates_per_src FLOAT,
    src_batch INTEGER NOT NULL DEFAULT 30,
    dst_sample INTEGER NOT NULL DEFAULT 1200,
    k_per_src INTEGER NOT NULL DEFAULT 15,
    p_lo FLOAT NOT NULL DEFAULT 0.10,
    p_hi FLOAT NOT NULL DEFAULT 0.35,
    max_rounds INTEGER NOT NULL DEFAULT 3,
    seed INTEGER NOT NULL DEFAULT 42,
    status TEXT NOT NULL DEFAULT 'in_progress',
    started_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    error_message TEXT
);

-- pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;
```

## API Endpoints

```
# Pipeline
POST /pipeline/import-from-notion    # Step 1: RAW ìˆ˜ìš© (Database or Parent Page mode)
POST /pipeline/extract-thoughts      # Step 2: NORMALIZED ìƒì„±
POST /pipeline/select-pairs          # Step 3: ZK í˜ì–´ ì„ íƒ
POST /pipeline/generate-essays       # Step 4: Essay ìƒì„±
POST /pipeline/run-all               # ì „ì²´ íŒŒì´í”„ë¼ì¸

# Candidate Mining (ìƒ˜í”Œë§ ê¸°ë°˜)
POST /mine-candidates/batch          # ë‹¨ì¼ ë°°ì¹˜ ë§ˆì´ë‹ (30 src Ã— 1200 dst)
POST /mine-candidates/full           # ì „ì²´ ë§ˆì´ë‹ (ëª¨ë“  thought ì²˜ë¦¬)
GET  /mine-candidates/progress       # ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ ì¡°íšŒ

# Distribution Sketch (ì „ì—­ ë¶„í¬ ê·¼ì‚¬)
POST /distribution/sketch/build      # ë¶„í¬ ìƒ˜í”Œ ìˆ˜ì§‘ (~100K ìƒ˜í”Œ)
POST /distribution/sketch/calculate  # ë¶„í¬ ê³„ì‚° (p0-p100)
GET  /distribution                   # ìºì‹œëœ ë¶„í¬ ì¡°íšŒ

# Essays
GET  /essays                         # Essay ëª©ë¡ ì¡°íšŒ
GET  /essays/{id}                    # Essay ìƒì„¸ ì¡°íšŒ

# Health
GET  /health                         # ì„œë²„ ìƒíƒœ
```

## Import Modes

The `/pipeline/import-from-notion` endpoint supports two modes:

### Database Mode
- **Config:** Set `NOTION_DATABASE_ID` in .env
- **Behavior:** Fetches pages from database using `query_database()`
- **Content:** Reads from properties["ë³¸ë¬¸"] field

### Parent Page Mode (NEW)
- **Config:** Set `NOTION_PARENT_PAGE_ID` in .env
- **Behavior:**
  1. Fetches child pages using `fetch_child_pages_from_parent()`
  2. For each child page, calls `fetch_page_blocks()` to get content
  3. Stores content in properties["ë³¸ë¬¸"] (same as Database mode)
- **Performance:** ~3-5 minutes for 724 pages (rate limited)
- **Error Handling:** Failures on individual pages log warnings but don't stop import

**Note:** Either `NOTION_DATABASE_ID` or `NOTION_PARENT_PAGE_ID` must be set (validated at startup)

## Incremental Import (RPC-based Change Detection)

The import process uses **PostgreSQL RPC** for efficient change detection:

### Performance
- **Change detection time:** ~0.2s (constant, scales to 100k pages)
- **Accuracy:** 100% (unchanged pages correctly detected)
- **Improvement:** 270x faster than full table scan (60s â†’ 0.2s)

### Behavior
1. **First run:** Imports all pages from Notion
2. **Subsequent runs:** Only imports new/updated pages
   - Unchanged pages: **skipped** (no DB write, no content fetch)
   - Updated pages: Re-imported with new content
   - New pages: Imported as usual

### Implementation
- **RPC Function:** `get_changed_pages(pages_data jsonb)`
- **Location:** Supabase PostgreSQL (public schema)
- **Input:** Array of `{id, last_edited_time}` from Notion API
- **Output:** `{new_page_ids[], updated_page_ids[], unchanged_count}`
- **Fallback:** Falls back to full table scan if RPC fails

### Success Rate Calculation
```python
# Skipped pages count as success (duplicate prevention is intentional)
success_count = imported_pages + skipped_pages
success_rate = (success_count / total_pages * 100)

# Job status
if success_rate >= 90:
    status = "completed"
else:
    status = "failed"
```

### SQL Schema
```sql
-- RPC function (deployed in Supabase)
CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb AS $$
  -- See: backend/docs/supabase_import_jobs.sql
$$ LANGUAGE plpgsql;
```

## ìƒ˜í”Œë§ ê¸°ë°˜ í›„ë³´ ë§ˆì´ë‹ ì•„í‚¤í…ì²˜

ê¸°ì¡´ Distance Table ë°©ì‹(ì „ìŒ ê³„ì‚° O(NÂ²))ì„ ëŒ€ì²´í•˜ëŠ” ìƒ˜í”Œë§ ê¸°ë°˜ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ ê°œìš”

| ì¶• | ëª©ì  | RPC | ë³µì¡ë„ |
|----|------|-----|--------|
| **(A) Candidate Mining** | srcë‹¹ 10-20ê°œ í›„ë³´ ìƒì„± | `mine_candidate_pairs()` | O(NÃ—k) |
| **(B) Distribution Sketch** | ì „ì—­ ë¶„í¬ ê·¼ì‚¬ (p0-p100) | `build_distribution_sketch()` | O(ìƒ˜í”Œìˆ˜) |

### ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | Distance Table (íê¸°) | ìƒ˜í”Œë§ ê¸°ë°˜ (í˜„ì¬) |
|------|----------------------|-------------------|
| ì´ˆê¸° êµ¬ì¶• | ~7ë¶„ | ~3ì´ˆ |
| ì €ì¥ ê³µê°„ | 178MB | ~5MB |
| ë³µì¡ë„ | O(NÂ²) | O(NÃ—k) |
| ì¦ë¶„ ê°±ì‹  | í•„ìš” | ë¶ˆí•„ìš” |

---

## rand_key ê¸°ë°˜ ê²°ì •ë¡ ì  ìƒ˜í”Œë§

### rand_keyë€?

`thought_units` í…Œì´ë¸”ì˜ ê° rowì— ì €ì¥ëœ 0~1 ì‚¬ì´ì˜ ëœë¤ ê°’ì…ë‹ˆë‹¤.

```sql
ALTER TABLE thought_units ADD COLUMN rand_key DOUBLE PRECISION DEFAULT random();
CREATE INDEX idx_thought_units_rand_key ON thought_units (rand_key);
```

### ì™œ rand_keyë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

**ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì :**
```sql
-- âŒ ORDER BY random(): ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼, ë¹„íš¨ìœ¨ì 
SELECT * FROM thought_units ORDER BY random() LIMIT 200;

-- âŒ TABLESAMPLE: ë¹„ê²°ì •ë¡ ì , seed ì¬í˜„ ì–´ë ¤ì›€
SELECT * FROM thought_units TABLESAMPLE BERNOULLI(10);
```

**rand_key ë°©ì‹ì˜ ì¥ì :**
```sql
-- âœ… rand_key: ê²°ì •ë¡ ì , ì¸ë±ìŠ¤ í™œìš©, ì¬í˜„ ê°€ëŠ¥
SELECT * FROM thought_units
WHERE rand_key >= 0.000042  -- seed ê¸°ë°˜ ì‹œì‘ì 
ORDER BY rand_key
LIMIT 200;
```

### Seed â†’ ì‹œì‘ì  ë³€í™˜

```
seed=42
   â†“
(42 % 1000000) / 1000000.0 = 0.000042
   â†“
rand_key >= 0.000042 ì¸ rowë¶€í„° ìˆœì„œëŒ€ë¡œ ì„ íƒ
```

### ìƒ˜í”Œë§ íë¦„ë„

```
seed=42
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rand_key ì¶• (0 ~ 1)                                      â”‚
â”‚                                                          â”‚
â”‚ 0.000042                              0.500042           â”‚
â”‚    â†“                                     â†“               â”‚
â”‚    [====== src 200ê°œ ======]    [====== dst 500ê°œ ======]â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
CROSS JOIN: 200 Ã— 500 = 100,000 í˜ì–´
    â†“
ê° í˜ì–´ì˜ cosine similarity ê³„ì‚°
    â†“
similarity_samples í…Œì´ë¸”ì— ì €ì¥
    â†“
PERCENTILE_CONTë¡œ p0 ~ p100 ê³„ì‚°
```

---

## ì „ì—­ ë¶„í¬ ìŠ¤ì¼€ì¹˜ (Distribution Sketch)

### ëª©ì 

ì „ìŒ ê³„ì‚°(NÂ²) ì—†ì´ ìœ ì‚¬ë„ ë¶„í¬ë¥¼ **ê·¼ì‚¬**í•©ë‹ˆë‹¤.

### RPC: build_distribution_sketch()

```sql
SELECT build_distribution_sketch(
    p_seed := 42,           -- ê²°ì •ë¡ ì  ìƒ˜í”Œë§ ì‹œë“œ
    p_src_sample := 200,    -- src ìƒ˜í”Œ í¬ê¸°
    p_dst_sample := 500,    -- dst ìƒ˜í”Œ í¬ê¸°
    p_rounds := 1,          -- ìƒ˜í”Œë§ ë¼ìš´ë“œ
    p_exclude_same_memo := TRUE,  -- ê°™ì€ ë©”ëª¨ ì œì™¸
    p_policy := 'random_pairs'
);
```

### ìƒ˜í”Œ ìˆ˜ ê³„ì‚°

```
ì´ ìƒ˜í”Œ ìˆ˜ = src_sample Ã— dst_sample Ã— rounds

ì˜ˆì‹œ:
- 200 Ã— 500 Ã— 1 = 100,000 ìƒ˜í”Œ
- 100 Ã— 500 Ã— 2 = 100,000 ìƒ˜í”Œ
```

### RPC: calculate_distribution_from_sketch()

ì €ì¥ëœ ìƒ˜í”Œì—ì„œ ë°±ë¶„ìœ„ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

```sql
SELECT calculate_distribution_from_sketch();
-- ê²°ê³¼: p0, p10, p20, ..., p90, p100, mean, stddev
```

### ìƒ˜í”Œ í™•ì¸ ì¿¼ë¦¬

```sql
-- í˜„ì¬ ìƒ˜í”Œ ìˆ˜ í™•ì¸
SELECT COUNT(*) FROM similarity_samples;

-- ìµœì‹  run_idì˜ ìƒ˜í”Œ ìˆ˜
SELECT run_id, COUNT(*), MIN(created_at), MAX(created_at)
FROM similarity_samples
GROUP BY run_id
ORDER BY MAX(created_at) DESC
LIMIT 1;
```

---

## í›„ë³´ ë§ˆì´ë‹ (Candidate Mining)

### ëª©ì 

ê° thought(src)ì— ëŒ€í•´ ì ì ˆí•œ ìœ ì‚¬ë„ ë²”ìœ„ì˜ í›„ë³´ kê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### RPC: mine_candidate_pairs()

```sql
SELECT mine_candidate_pairs(
    p_last_src_id := 0,     -- í‚¤ì…‹ í˜ì´ì§• (OFFSET ê¸ˆì§€)
    p_src_batch := 30,      -- ë°°ì¹˜ë‹¹ src ìˆ˜
    p_dst_sample := 1200,   -- dst ìƒ˜í”Œ í¬ê¸°
    p_k := 15,              -- srcë‹¹ í›„ë³´ ìˆ˜
    p_lo := 0.10,           -- í•˜ìœ„ ë¶„ìœ„ìˆ˜ (ë°´ë“œ í•˜í•œ)
    p_hi := 0.35,           -- ìƒìœ„ ë¶„ìœ„ìˆ˜ (ë°´ë“œ ìƒí•œ)
    p_seed := 42,           -- ìƒ˜í”Œë§ ì‹œë“œ
    p_max_rounds := 3       -- ìµœëŒ€ ì¬ì‹œë„
);
```

### ë°´ë“œ í•„í„°ë§ ì›ë¦¬

```
ì „ì²´ ìœ ì‚¬ë„ ë¶„í¬:
|  ë§¤ìš° ë‚®ìŒ  |  ë‚®ìŒ  |  ì¤‘ê°„  |  ë†’ìŒ  |  ë§¤ìš° ë†’ìŒ  |
0.0         0.10    0.20    0.35    0.50         1.0
            â†‘                â†‘
          p_lo             p_hi
            â””â”€â”€â”€â”€â”€ ë°´ë“œ â”€â”€â”€â”€â”€â”˜
            (ì°½ì˜ì  ì¡°í•© ì˜ì—­)
```

- **p_lo (0.10)**: ë„ˆë¬´ ê´€ë ¨ ì—†ëŠ” í˜ì–´ ì œì™¸
- **p_hi (0.35)**: ë„ˆë¬´ ìœ ì‚¬í•œ í˜ì–´ ì œì™¸ (ìƒˆë¡œìš´ í†µì°° ì—†ìŒ)

### íŒŒë¼ë¯¸í„° ê¶Œì¥ê°’

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… |
|---------|--------|------|------|
| src_batch | 30 | 20-40 | ë°°ì¹˜ë‹¹ src ìˆ˜ |
| dst_sample | 1200 | 800-1500 | dst ìƒ˜í”Œ í¬ê¸° |
| k | 15 | 10-20 | srcë‹¹ í›„ë³´ ìˆ˜ |
| p_lo | 0.10 | 0.05-0.15 | í•˜ìœ„ ë¶„ìœ„ìˆ˜ |
| p_hi | 0.35 | 0.25-0.45 | ìƒìœ„ ë¶„ìœ„ìˆ˜ |

### í‚¤ì…‹ í˜ì´ì§•

OFFSET ëŒ€ì‹  `id > last_src_id` ë°©ì‹ìœ¼ë¡œ í˜ì´ì§•:

```sql
-- âŒ OFFSET: ëŠë¦¬ê³  ë¶ˆì•ˆì •
SELECT * FROM thought_units OFFSET 1000 LIMIT 30;

-- âœ… í‚¤ì…‹ í˜ì´ì§•: ë¹ ë¥´ê³  ì•ˆì •ì 
SELECT * FROM thought_units WHERE id > 1000 ORDER BY id LIMIT 30;
```

---

## íŒŒì¼ ìœ„ì¹˜

### SQL Migrations
- `backend/docs/supabase_migrations/015_add_rand_key.sql`
- `backend/docs/supabase_migrations/016_create_mining_progress.sql`
- `backend/docs/supabase_migrations/017_create_similarity_samples.sql`
- `backend/docs/supabase_migrations/018_mine_candidate_pairs_rpc.sql`
- `backend/docs/supabase_migrations/019_build_distribution_sketch_rpc.sql`
- `backend/docs/supabase_migrations/020_calculate_distribution_from_sketch_rpc.sql`

### Python Services
- `backend/services/candidate_mining_service.py` - í›„ë³´ ë§ˆì´ë‹ ì„œë¹„ìŠ¤
- `backend/services/distribution_service.py` - ë¶„í¬ ê³„ì‚° ì„œë¹„ìŠ¤
- `backend/services/supabase_service.py` - RPC í˜¸ì¶œ ë©”ì„œë“œ

### í†µí•© Migration
- `backend/docs/supabase_migrations/MIGRATION_COMBINED_015_020.sql` - ëª¨ë“  DDL + RPC í†µí•©

## LLM Tasks

### 1. extract_thoughts (Step 2)
- **Input:** raw_note (title, content)
- **Output:** 1-5ê°œì˜ ThoughtUnit (claim, context)
- **Model:** Claude 3.5 Sonnet

### 2. score_pairs (Step 3)
- **Input:** í›„ë³´ í˜ì–´ ëª©ë¡ (similarity 0.05~0.35, ë‚®ì€ ìœ ì‚¬ë„ = ì„œë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´)
- **Output:** ê° í˜ì–´ì˜ ë…¼ë¦¬ì  í™•ì¥ ê°€ëŠ¥ì„± ì ìˆ˜ (0-100)
- **Model:** Claude 3.5 Sonnet

### 3. generate_essay (Step 4)
- **Input:** ì„ íƒëœ í˜ì–´ + ì¶œì²˜ ì •ë³´
- **Output:** Essay (title, outline[3], used_thoughts, reason)
- **Model:** Claude 3.5 Sonnet

## TypeScript Types (Frontend)

```typescript
interface NotionCredentials {
  apiKey: string;
  databaseId: string;
}

interface ThoughtUnit {
  id: number;
  claim: string;
  context: string | null;
  raw_note_id: string;
}

interface UsedThought {
  thought_id: number;
  claim: string;
  source_title: string;
  source_url: string;
}

interface Essay {
  id: number;
  type: string;
  title: string;
  outline: string[];  // ì •í™•íˆ 3ê°œ
  used_thoughts: UsedThought[];
  reason: string;
  generated_at: string;
}

interface PipelineResult {
  success: boolean;
  step1_imported: number;
  step2_thoughts: number;
  step3_pairs: number;
  step4_essays: number;
  errors: string[];
}
```

## Pydantic Schemas (Backend)

```python
# schemas/normalized.py
class ThoughtUnit(BaseModel):
    claim: str = Field(..., min_length=10, max_length=500)
    context: str | None = Field(None, max_length=200)

class ThoughtExtractionResult(BaseModel):
    thoughts: list[ThoughtUnit] = Field(..., min_length=1, max_length=5)

# schemas/essay.py
class UsedThought(BaseModel):
    thought_id: int
    claim: str
    source_title: str
    source_url: str = Field(..., pattern=r'^https?://')

class Essay(BaseModel):
    type: str = Field(default="essay")
    title: str = Field(..., min_length=5, max_length=100)
    outline: list[str] = Field(..., min_length=3, max_length=3)
    used_thoughts: list[UsedThought] = Field(..., min_length=1)
    reason: str = Field(..., max_length=300)
```

## Configuration

```bash
# Environment Variables
NOTION_API_KEY=secret_xxx
NOTION_DATABASE_ID=xxx
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=xxx
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx

# Rate Limits
RATE_LIMIT_ANTHROPIC=5    # req/sec
RATE_LIMIT_OPENAI=10      # req/sec
RATE_LIMIT_NOTION=3       # req/sec

# Batch Processing
BATCH_SIZE=10
MAX_RETRIES=3

# pgvector
EMBEDDING_DIMENSION=1536

# ìƒ˜í”Œë§ ê¸°ë°˜ ë§ˆì´ë‹ (NEW)
MINING_SRC_BATCH=30            # ë°°ì¹˜ë‹¹ src ìˆ˜
MINING_DST_SAMPLE=1200         # dst ìƒ˜í”Œ í¬ê¸°
MINING_K_PER_SRC=15            # srcë‹¹ í›„ë³´ ìˆ˜
MINING_P_LO=0.10               # ë°´ë“œ í•˜í•œ (í•˜ìœ„ ë¶„ìœ„ìˆ˜)
MINING_P_HI=0.35               # ë°´ë“œ ìƒí•œ (ìƒìœ„ ë¶„ìœ„ìˆ˜)
MINING_SEED=42                 # ê²°ì •ë¡ ì  ìƒ˜í”Œë§ ì‹œë“œ

# ë¶„í¬ ìŠ¤ì¼€ì¹˜ (NEW)
SKETCH_SRC_SAMPLE=200          # src ìƒ˜í”Œ í¬ê¸°
SKETCH_DST_SAMPLE=500          # dst ìƒ˜í”Œ í¬ê¸°
SKETCH_ROUNDS=1                # ìƒ˜í”Œë§ ë¼ìš´ë“œ (200Ã—500Ã—1 = 100K ìƒ˜í”Œ)
```

## Code Conventions

- Python: Type hints required, async/await for I/O
- TypeScript: Strict mode, no any
- Error handling: Never bare except, always log with context
- Batch processing: Always gc.collect() after batch
- API keys: Never log, mask in error messages

## Agent Usage Policy

**MANDATORY: Always use specialized agents when available**

The following agents MUST be used proactively for their respective tasks:

### 1. test-automator (REQUIRED for all implementations)
- **When to use:** BEFORE completing ANY feature or code change
- **Why:** No feature is complete without tests
- **Example:** After implementing pagination logic, IMMEDIATELY use test-automator to create and run tests
- **Command:** Use Task tool with `subagent_type="test-automator"`

### 2. code-reviewer (REQUIRED before completion)
- **When to use:** After implementation, BEFORE merging or marking complete
- **Why:** Ensures code quality, security, and performance
- **Example:** After Phase 1 implementation passes tests, use code-reviewer to audit the changes

### 3. debugger (USE when errors occur)
- **When to use:** Encountering errors, unexpected behavior, or test failures
- **Why:** Systematic debugging approach with proper tooling
- **Example:** API 429 errors, import failures, pagination bugs

### 4. Explore (USE for codebase analysis)
- **When to use:** Need to quickly find files by patterns or search code
- **Why:** Faster than manual Glob/Grep for complex searches
- **Example:** "Find all files that handle Notion API pagination"

### 5. prompt-engineer (REQUIRED for LLM work)
- **When to use:** Working with Claude/OpenAI API calls, JSON output, prompts
- **Why:** Specialized in LLM prompt design and output formatting
- **Example:** Designing prompts for thought extraction or essay generation

### 6. supabase-specialist (REQUIRED for database work)
- **When to use:** Supabase operations, pgvector similarity search, PostgreSQL queries, schema changes
- **Why:** Expert in Supabase patterns, RLS policies, and vector operations
- **Example:** Implementing batch upsert, pgvector similarity queries, stored procedures

### 7. nextjs-developer (REQUIRED for frontend work)
- **When to use:** Next.js 14 frontend, React components, API integration, .tsx files
- **Why:** Specialized in Next.js App Router, Server Components, and client patterns
- **Example:** Creating essay viewer components, implementing pipeline controls

### 8. fastapi-architect (REQUIRED for backend work)
- **When to use:** FastAPI backend, async patterns, pipeline architecture, service layer design
- **Why:** Expert in FastAPI patterns, dependency injection, and async operations
- **Example:** Designing router structure, implementing async services, pipeline endpoints

### 9. Plan (USE for implementation planning)
- **When to use:** Before starting complex implementations, need architectural decisions
- **Why:** Creates detailed step-by-step plans with file locations and code examples
- **Example:** Planning Phase 2 (block content collection) or Phase 3 (rate limiting)

### 10. file-organizer (USE for cleanup)
- **When to use:** Need to clean up temporary files or organize project structure
- **Why:** Safe file operations with user confirmation
- **Example:** Removing unused test files, organizing old plan files

### General Rules
- **Proactive Usage:** Don't wait for user to ask - use agents automatically when applicable
- **Parallel Execution:** When possible, run multiple agents in parallel (single message, multiple Task calls)
- **Documentation:** Always document agent usage in commit messages and plan files
- **Test First:** ALWAYS use test-automator after any code change before marking complete
- **Review Last:** ALWAYS use code-reviewer before final completion of major features

## Plan File Management

**IMPORTANT: Plan File Synchronization Rule**

All plan files must be synchronized between two locations:
1. **User home directory:** `~/.claude/plans/` (Claude Code default location)
2. **Project directory:** `<project_root>/.claude/plans/` (for version control and IDE visibility)

**When to synchronize:**
- When a new plan file is created
- When an existing plan file is updated
- When exiting plan mode

**How to synchronize:**
```bash
# After creating or updating plan files in ~/.claude/plans/
cp ~/.claude/plans/*.md <project_root>/.claude/plans/
```

**Why this matters:**
- Project .claude/plans/ is visible in Cursor IDE
- User ~/.claude/plans/ is where Claude Code stores plans by default
- Keeping both in sync ensures plans are accessible everywhere and can be version controlled
