# ë…¸ì…˜ ê¸°ë°˜ ì•„ì´ë””ì–´ ì¡°í•© ì„œë¹„ìŠ¤ MVP

## Project Overview

ë…¸ì…˜ ë©”ëª¨ì—ì„œ ì‚¬ê³  ë‹¨ìœ„ë¥¼ ì¶”ì¶œí•˜ê³ , ì•½í•œ ì—°ê²°ì„ ì°¾ì•„ ê¸€ê°ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤

**íŒŒì´í”„ë¼ì¸:** RAW â†’ NORMALIZED â†’ ZK â†’ Essay

## Language

Always all output must write in Korean.

---
language: korean
---

# ğŸš¨ ìµœìš°ì„  ê·œì¹™

| ê·œì¹™ | ì„¤ì • |
|-----|------|
| **ì¶œë ¥ ì–¸ì–´** | í•œêµ­ì–´ (ì˜ˆì™¸ ì—†ìŒ) |
| **ì½”ë“œ ì£¼ì„** | í•œêµ­ì–´ |
| **ì»¤ë°‹ ë©”ì‹œì§€** | í•œêµ­ì–´ |

> âš ï¸ ì´ ê·œì¹™ì€ /compact, ì„¸ì…˜ ì¬ê°œ ë“± ëª¨ë“  ìƒí™©ì—ì„œ ìœ ì§€ë©ë‹ˆë‹¤.


## Tech Stack

- **Backend:** FastAPI, Python 3.11+
- **Frontend:** Next.js 14 (App Router), TypeScript, Tailwind CSS
- **Database:** Supabase (PostgreSQL + pgvector)
- **LLM:** Claude 3.5 Sonnet (Anthropic), text-embedding-3-small (OpenAI)
- **External API:** Notion API

## Directory Structure

```
backend/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ supabase_service.py   # DB CRUD + pgvector
â”‚   â”œâ”€â”€ ai_service.py         # LLM calls
â”‚   â”œâ”€â”€ notion_service.py     # Notion API
â”‚   â””â”€â”€ rate_limiter.py       # API rate limiting
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ pipeline.py           # Pipeline endpoints
â”‚   â””â”€â”€ essays.py             # Essay CRUD
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ raw.py                # RawNote models
â”‚   â”œâ”€â”€ normalized.py         # ThoughtUnit models
â”‚   â”œâ”€â”€ zk.py                 # ThoughtPair models
â”‚   â”œâ”€â”€ essay.py              # Essay models
â”‚   â””â”€â”€ processing.py         # ProcessingStatus
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.py         # JSON parsing
â”‚   â””â”€â”€ error_handlers.py     # Exception handling
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ unit/
    â””â”€â”€ integration/

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx          # Main dashboard
â”‚   â”‚   â””â”€â”€ login/page.tsx    # Login page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ EssayCard.tsx
â”‚   â”‚   â”œâ”€â”€ PipelineControl.tsx
â”‚   â”‚   â””â”€â”€ DatabaseSelector.tsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts            # Backend client
â”‚   â”‚   â””â”€â”€ types.ts          # TypeScript types
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ usePipeline.ts
â”‚       â””â”€â”€ useAuth.ts
â””â”€â”€ __tests__/
```

## Database Schema

```sql
-- 1. RAW ë ˆì´ì–´: Notion ì›ë³¸
CREATE TABLE raw_notes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    notion_page_id TEXT UNIQUE NOT NULL,
    notion_url TEXT NOT NULL,
    title TEXT,
    content TEXT,
    properties_json JSONB DEFAULT '{}'::jsonb,
    notion_created_time TIMESTAMPTZ NOT NULL,
    notion_last_edited_time TIMESTAMPTZ NOT NULL,
    imported_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_raw_notes_notion_page_id ON raw_notes(notion_page_id);

-- 2. NORMALIZED ë ˆì´ì–´: ì‚¬ê³  ë‹¨ìœ„ + ì„ë² ë”©
CREATE TABLE thought_units (
    id SERIAL PRIMARY KEY,
    raw_note_id UUID NOT NULL REFERENCES raw_notes(id) ON DELETE CASCADE,
    claim TEXT NOT NULL,
    context TEXT,
    embedding vector(1536),  -- OpenAI text-embedding-3-small
    embedding_model TEXT DEFAULT 'text-embedding-3-small',
    extracted_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_thought_units_raw_note ON thought_units(raw_note_id);
CREATE INDEX idx_thought_units_embedding ON thought_units
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- 3. ZK ë ˆì´ì–´: ì„ íƒëœ í˜ì–´
CREATE TABLE thought_pairs (
    id SERIAL PRIMARY KEY,
    thought_a_id INTEGER NOT NULL REFERENCES thought_units(id),
    thought_b_id INTEGER NOT NULL REFERENCES thought_units(id),
    similarity_score FLOAT NOT NULL CHECK (similarity_score >= 0 AND similarity_score <= 1),
    connection_reason TEXT,
    selected_at TIMESTAMPTZ DEFAULT NOW(),
    is_used_in_essay BOOLEAN DEFAULT FALSE,
    CONSTRAINT different_thoughts CHECK (thought_a_id != thought_b_id),
    CONSTRAINT ordered_pair CHECK (thought_a_id < thought_b_id),
    UNIQUE(thought_a_id, thought_b_id)
);
CREATE INDEX idx_thought_pairs_unused ON thought_pairs(is_used_in_essay)
WHERE is_used_in_essay = FALSE;

-- 4. Essay ê²°ê³¼ë¬¼
CREATE TABLE essays (
    id SERIAL PRIMARY KEY,
    type TEXT DEFAULT 'essay',
    title TEXT NOT NULL,
    outline JSONB NOT NULL,  -- ["1ë‹¨: ...", "2ë‹¨: ...", "3ë‹¨: ..."]
    used_thoughts_json JSONB NOT NULL,
    reason TEXT NOT NULL,
    pair_id INTEGER NOT NULL REFERENCES thought_pairs(id),
    generated_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_essays_generated_at ON essays(generated_at DESC);

-- 5. ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
CREATE TABLE processing_status (
    id SERIAL PRIMARY KEY,
    raw_note_id UUID NOT NULL REFERENCES raw_notes(id) ON DELETE CASCADE,
    step TEXT NOT NULL,  -- 'extract_thoughts', 'create_embedding', 'select_pairs', 'generate_essay'
    status TEXT NOT NULL,  -- 'pending', 'processing', 'completed', 'failed'
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(raw_note_id, step)
);
CREATE INDEX idx_processing_status_step_status ON processing_status(step, status);

-- pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;
```

## API Endpoints

```
# Pipeline
POST /pipeline/import-from-notion    # Step 1: RAW ìˆ˜ìš© (Database or Parent Page mode)
POST /pipeline/extract-thoughts      # Step 2: NORMALIZED ìƒì„±
POST /pipeline/select-pairs          # Step 3: ZK í˜ì–´ ì„ íƒ
POST /pipeline/generate-essays       # Step 4: Essay ìƒì„±
POST /pipeline/run-all               # ì „ì²´ íŒŒì´í”„ë¼ì¸

# Hybrid C Strategy (NEW)
POST /pipeline/collect-candidates    # ì „ì²´ í›„ë³´ ìˆ˜ì§‘ ë° pair_candidates ì €ì¥
POST /pipeline/sample-initial        # ì´ˆê¸° 100ê°œ ìƒ˜í”Œ í‰ê°€
POST /pipeline/score-candidates      # ë°°ì¹˜ í‰ê°€ (ë°±ê·¸ë¼ìš´ë“œ)
GET  /essays/recommended             # AI ì¶”ì²œ Essay í›„ë³´ ì¡°íšŒ

# Essays
GET  /essays                         # Essay ëª©ë¡ ì¡°íšŒ
GET  /essays/{id}                    # Essay ìƒì„¸ ì¡°íšŒ

# Health
GET  /health                         # ì„œë²„ ìƒíƒœ
```

## Import Modes

The `/pipeline/import-from-notion` endpoint supports two modes:

### Database Mode
- **Config:** Set `NOTION_DATABASE_ID` in .env
- **Behavior:** Fetches pages from database using `query_database()`
- **Content:** Reads from properties["ë³¸ë¬¸"] field

### Parent Page Mode (NEW)
- **Config:** Set `NOTION_PARENT_PAGE_ID` in .env
- **Behavior:**
  1. Fetches child pages using `fetch_child_pages_from_parent()`
  2. For each child page, calls `fetch_page_blocks()` to get content
  3. Stores content in properties["ë³¸ë¬¸"] (same as Database mode)
- **Performance:** ~3-5 minutes for 724 pages (rate limited)
- **Error Handling:** Failures on individual pages log warnings but don't stop import

**Note:** Either `NOTION_DATABASE_ID` or `NOTION_PARENT_PAGE_ID` must be set (validated at startup)

## Incremental Import (RPC-based Change Detection)

The import process uses **PostgreSQL RPC** for efficient change detection:

### Performance
- **Change detection time:** ~0.2s (constant, scales to 100k pages)
- **Accuracy:** 100% (unchanged pages correctly detected)
- **Improvement:** 270x faster than full table scan (60s â†’ 0.2s)

### Behavior
1. **First run:** Imports all pages from Notion
2. **Subsequent runs:** Only imports new/updated pages
   - Unchanged pages: **skipped** (no DB write, no content fetch)
   - Updated pages: Re-imported with new content
   - New pages: Imported as usual

### Implementation
- **RPC Function:** `get_changed_pages(pages_data jsonb)`
- **Location:** Supabase PostgreSQL (public schema)
- **Input:** Array of `{id, last_edited_time}` from Notion API
- **Output:** `{new_page_ids[], updated_page_ids[], unchanged_count}`
- **Fallback:** Falls back to full table scan if RPC fails

### Success Rate Calculation
```python
# Skipped pages count as success (duplicate prevention is intentional)
success_count = imported_pages + skipped_pages
success_rate = (success_count / total_pages * 100)

# Job status
if success_rate >= 90:
    status = "completed"
else:
    status = "failed"
```

### SQL Schema
```sql
-- RPC function (deployed in Supabase)
CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb AS $$
  -- See: backend/docs/supabase_import_jobs.sql
$$ LANGUAGE plpgsql;
```

## LLM Tasks

### 1. extract_thoughts (Step 2)
- **Input:** raw_note (title, content)
- **Output:** 1-5ê°œì˜ ThoughtUnit (claim, context)
- **Model:** Claude 3.5 Sonnet

### 2. score_pairs (Step 3)
- **Input:** í›„ë³´ í˜ì–´ ëª©ë¡ (similarity 0.05~0.35, ë‚®ì€ ìœ ì‚¬ë„ = ì„œë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´)
- **Output:** ê° í˜ì–´ì˜ ë…¼ë¦¬ì  í™•ì¥ ê°€ëŠ¥ì„± ì ìˆ˜ (0-100)
- **Model:** Claude 3.5 Sonnet

### 3. generate_essay (Step 4)
- **Input:** ì„ íƒëœ í˜ì–´ + ì¶œì²˜ ì •ë³´
- **Output:** Essay (title, outline[3], used_thoughts, reason)
- **Model:** Claude 3.5 Sonnet

## TypeScript Types (Frontend)

```typescript
interface NotionCredentials {
  apiKey: string;
  databaseId: string;
}

interface ThoughtUnit {
  id: number;
  claim: string;
  context: string | null;
  raw_note_id: string;
}

interface UsedThought {
  thought_id: number;
  claim: string;
  source_title: string;
  source_url: string;
}

interface Essay {
  id: number;
  type: string;
  title: string;
  outline: string[];  // ì •í™•íˆ 3ê°œ
  used_thoughts: UsedThought[];
  reason: string;
  generated_at: string;
}

interface PipelineResult {
  success: boolean;
  step1_imported: number;
  step2_thoughts: number;
  step3_pairs: number;
  step4_essays: number;
  errors: string[];
}
```

## Pydantic Schemas (Backend)

```python
# schemas/normalized.py
class ThoughtUnit(BaseModel):
    claim: str = Field(..., min_length=10, max_length=500)
    context: str | None = Field(None, max_length=200)

class ThoughtExtractionResult(BaseModel):
    thoughts: list[ThoughtUnit] = Field(..., min_length=1, max_length=5)

# schemas/essay.py
class UsedThought(BaseModel):
    thought_id: int
    claim: str
    source_title: str
    source_url: str = Field(..., pattern=r'^https?://')

class Essay(BaseModel):
    type: str = Field(default="essay")
    title: str = Field(..., min_length=5, max_length=100)
    outline: list[str] = Field(..., min_length=3, max_length=3)
    used_thoughts: list[UsedThought] = Field(..., min_length=1)
    reason: str = Field(..., max_length=300)
```

## Configuration

```bash
# Environment Variables
NOTION_API_KEY=secret_xxx
NOTION_DATABASE_ID=xxx
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=xxx
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx

# Rate Limits
RATE_LIMIT_ANTHROPIC=5    # req/sec
RATE_LIMIT_OPENAI=10      # req/sec
RATE_LIMIT_NOTION=3       # req/sec

# Batch Processing
BATCH_SIZE=10
MAX_RETRIES=3

# pgvector
SIMILARITY_MIN=0.05  # ë‚®ì€ ìœ ì‚¬ë„ = ì„œë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´, ì°½ì˜ì  ì¡°í•©
SIMILARITY_MAX=0.35  # ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ìƒˆë¡œìš´ í†µì°°ì´ ì—†ìŒ
EMBEDDING_DIMENSION=1536
```

## Code Conventions

- Python: Type hints required, async/await for I/O
- TypeScript: Strict mode, no any
- Error handling: Never bare except, always log with context
- Batch processing: Always gc.collect() after batch
- API keys: Never log, mask in error messages

## Agent Usage Policy

**MANDATORY: Always use specialized agents when available**

The following agents MUST be used proactively for their respective tasks:

### 1. test-automator (REQUIRED for all implementations)
- **When to use:** BEFORE completing ANY feature or code change
- **Why:** No feature is complete without tests
- **Example:** After implementing pagination logic, IMMEDIATELY use test-automator to create and run tests
- **Command:** Use Task tool with `subagent_type="test-automator"`

### 2. code-reviewer (REQUIRED before completion)
- **When to use:** After implementation, BEFORE merging or marking complete
- **Why:** Ensures code quality, security, and performance
- **Example:** After Phase 1 implementation passes tests, use code-reviewer to audit the changes

### 3. debugger (USE when errors occur)
- **When to use:** Encountering errors, unexpected behavior, or test failures
- **Why:** Systematic debugging approach with proper tooling
- **Example:** API 429 errors, import failures, pagination bugs

### 4. Explore (USE for codebase analysis)
- **When to use:** Need to quickly find files by patterns or search code
- **Why:** Faster than manual Glob/Grep for complex searches
- **Example:** "Find all files that handle Notion API pagination"

### 5. prompt-engineer (REQUIRED for LLM work)
- **When to use:** Working with Claude/OpenAI API calls, JSON output, prompts
- **Why:** Specialized in LLM prompt design and output formatting
- **Example:** Designing prompts for thought extraction or essay generation

### 6. supabase-specialist (REQUIRED for database work)
- **When to use:** Supabase operations, pgvector similarity search, PostgreSQL queries, schema changes
- **Why:** Expert in Supabase patterns, RLS policies, and vector operations
- **Example:** Implementing batch upsert, pgvector similarity queries, stored procedures

### 7. nextjs-developer (REQUIRED for frontend work)
- **When to use:** Next.js 14 frontend, React components, API integration, .tsx files
- **Why:** Specialized in Next.js App Router, Server Components, and client patterns
- **Example:** Creating essay viewer components, implementing pipeline controls

### 8. fastapi-architect (REQUIRED for backend work)
- **When to use:** FastAPI backend, async patterns, pipeline architecture, service layer design
- **Why:** Expert in FastAPI patterns, dependency injection, and async operations
- **Example:** Designing router structure, implementing async services, pipeline endpoints

### 9. Plan (USE for implementation planning)
- **When to use:** Before starting complex implementations, need architectural decisions
- **Why:** Creates detailed step-by-step plans with file locations and code examples
- **Example:** Planning Phase 2 (block content collection) or Phase 3 (rate limiting)

### 10. file-organizer (USE for cleanup)
- **When to use:** Need to clean up temporary files or organize project structure
- **Why:** Safe file operations with user confirmation
- **Example:** Removing unused test files, organizing old plan files

### General Rules
- **Proactive Usage:** Don't wait for user to ask - use agents automatically when applicable
- **Parallel Execution:** When possible, run multiple agents in parallel (single message, multiple Task calls)
- **Documentation:** Always document agent usage in commit messages and plan files
- **Test First:** ALWAYS use test-automator after any code change before marking complete
- **Review Last:** ALWAYS use code-reviewer before final completion of major features

## Plan File Management

**IMPORTANT: Plan File Synchronization Rule**

All plan files must be synchronized between two locations:
1. **User home directory:** `~/.claude/plans/` (Claude Code default location)
2. **Project directory:** `<project_root>/.claude/plans/` (for version control and IDE visibility)

**When to synchronize:**
- When a new plan file is created
- When an existing plan file is updated
- When exiting plan mode

**How to synchronize:**
```bash
# After creating or updating plan files in ~/.claude/plans/
cp ~/.claude/plans/*.md <project_root>/.claude/plans/
```

**Why this matters:**
- Project .claude/plans/ is visible in Cursor IDE
- User ~/.claude/plans/ is where Claude Code stores plans by default
- Keeping both in sync ensures plans are accessible everywhere and can be version controlled
