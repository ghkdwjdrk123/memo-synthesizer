"""
Supabase ì„œë¹„ìŠ¤.

PostgreSQL + pgvectorë¥¼ ì‚¬ìš©í•œ ë°ì´í„° CRUD.
"""

import logging
import time
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from uuid import UUID

from supabase import create_async_client, AsyncClient

from config import settings
from schemas.raw import RawNote, RawNoteCreate
from schemas.normalized import ThoughtUnitCreate
from schemas.zk import (
    ThoughtPairCreate,
    PairCandidateCreate,
    PairCandidateBatch,
    ThoughtPairCreateExtended,
)

logger = logging.getLogger(__name__)


class SupabaseService:
    """Supabase CRUD + ì—°ê²° í’€ë§."""

    def __init__(self):
        """
        Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”.

        HTTP ì—°ê²° í’€ë§ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”.
        """
        # Supabase async í´ë¼ì´ì–¸íŠ¸
        self.client: AsyncClient = None
        self._initialized = False

    async def _ensure_initialized(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìµœì´ˆ í˜¸ì¶œ ì‹œ)"""
        if not self._initialized:
            self.client = await create_async_client(
                settings.supabase_url, settings.supabase_key
            )
            self._initialized = True

    async def close(self):
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ."""
        # Supabase ë‚´ì¥ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì‹œ ë³„ë„ ì¢…ë£Œ ë¶ˆí•„ìš”
        pass

    # ============================================================
    # RAW Notes CRUD
    # ============================================================

    async def upsert_raw_note(self, note: RawNoteCreate) -> dict:
        """
        RAW note ì €ì¥ (notion_page_id ê¸°ì¤€ upsert).

        Args:
            note: ì €ì¥í•  RAW note

        Returns:
            ì €ì¥ëœ note ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (datetime â†’ ISO string)
            data = note.model_dump(mode='json')

            # Supabase upsert (conflict on notion_page_id)
            response = await (
                self.client.table("raw_notes")
                .upsert(data, on_conflict="notion_page_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Raw note upserted: {note.notion_page_id} (title: {note.title[:50] if note.title else 'N/A'}, content: {len(note.content) if note.content else 0} chars)"
                )
                return response.data[0]
            else:
                raise Exception("Upsert returned no data")

        except Exception as e:
            logger.error(f"Failed to upsert raw note {note.notion_page_id}: {e}")
            raise

    async def get_raw_note_ids(self) -> List[str]:
        """
        ëª¨ë“  í™œì„± RAW noteì˜ ID ëª©ë¡ ì¡°íšŒ (ë©”ëª¨ë¦¬ ì ˆì•½).

        Returns:
            UUID ëª©ë¡ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("id")
                .eq("is_deleted", False)
                .execute()
            )

            ids = [row["id"] for row in response.data]
            logger.info(f"Retrieved {len(ids)} active raw note IDs")
            return ids

        except Exception as e:
            logger.error(f"Failed to get raw note IDs: {e}")
            raise

    async def get_raw_notes_by_ids(self, note_ids: List[str]) -> List[dict]:
        """
        ID ëª©ë¡ìœ¼ë¡œ í™œì„± RAW notes ì¡°íšŒ (ë°°ì¹˜ë³„ full content ë¡œë“œ).

        Args:
            note_ids: UUID ëª©ë¡

        Returns:
            RAW note ë°ì´í„° ëª©ë¡ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("*")
                .in_("id", note_ids)
                .eq("is_deleted", False)
                .execute()
            )

            logger.info(f"Retrieved {len(response.data)} active raw notes by IDs")
            return response.data

        except Exception as e:
            logger.error(f"Failed to get raw notes by IDs: {e}")
            raise

    async def get_raw_note_count(self) -> int:
        """í™œì„± RAW notes ì´ ê°œìˆ˜ ì¡°íšŒ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)."""
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("id", count="exact")
                .eq("is_deleted", False)
                .execute()
            )

            count = response.count if response.count else 0
            logger.info(f"Total active raw notes: {count}")
            return count

        except Exception as e:
            logger.error(f"Failed to get raw note count: {e}")
            return 0

    # ============================================================
    # Thought Units CRUD
    # ============================================================

    async def insert_thought_unit(self, thought: ThoughtUnitCreate) -> dict:
        """
        Thought unit í•œ ê°œ ì €ì¥.

        Args:
            thought: ì €ì¥í•  ì‚¬ê³  ë‹¨ìœ„ (ì„ë² ë”© í¬í•¨)

        Returns:
            ì €ì¥ëœ thought unit ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            data = thought.model_dump(mode='json')

            # pgvectorëŠ” list[float]ë¥¼ ìë™ìœ¼ë¡œ vector íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            response = await (
                self.client.table("thought_units")
                .insert(data)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Thought unit inserted: ID={response.data[0]['id']}, "
                    f"raw_note_id={thought.raw_note_id}, "
                    f"claim={thought.claim[:50]}..."
                )
                return response.data[0]
            else:
                raise Exception("Insert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to insert thought unit for raw_note {thought.raw_note_id}: {e}"
            )
            raise

    async def insert_thought_units_batch(
        self, thoughts: List[ThoughtUnitCreate]
    ) -> List[dict]:
        """
        ì—¬ëŸ¬ thought units ë°°ì¹˜ ì €ì¥.

        Args:
            thoughts: ì €ì¥í•  ì‚¬ê³  ë‹¨ìœ„ ëª©ë¡

        Returns:
            ì €ì¥ëœ thought units ë°ì´í„° ëª©ë¡

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        if not thoughts:
            logger.warning("Batch insert called with empty list")
            return []

        try:
            # ëª¨ë“  thoughtë¥¼ JSON ì§ë ¬í™”
            data = [thought.model_dump(mode='json') for thought in thoughts]

            # ë°°ì¹˜ insert
            response = await (
                self.client.table("thought_units")
                .insert(data)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Batch inserted {len(response.data)} thought units "
                    f"(raw_note_ids: {set(t.raw_note_id for t in thoughts)})"
                )
                return response.data
            else:
                raise Exception("Batch insert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to batch insert {len(thoughts)} thought units: {e}"
            )
            raise

    async def get_thought_units_by_raw_note(self, raw_note_id: str) -> List[dict]:
        """
        íŠ¹ì • raw_noteì˜ ëª¨ë“  thought units ì¡°íšŒ.

        Args:
            raw_note_id: ì›ë³¸ ë©”ëª¨ UUID

        Returns:
            Thought units ëª©ë¡
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_units")
                .select("*")
                .eq("raw_note_id", raw_note_id)
                .order("id")
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} thought units for raw_note {raw_note_id}"
            )
            return response.data

        except Exception as e:
            logger.error(
                f"Failed to get thought units for raw_note {raw_note_id}: {e}"
            )
            raise

    async def get_all_thought_units_with_embeddings(self) -> List[dict]:
        """
        ì„ë² ë”©ì´ ìˆëŠ” ëª¨ë“  thought units ì¡°íšŒ (ìœ ì‚¬ë„ ê²€ìƒ‰ìš©).

        Returns:
            Thought units ëª©ë¡ (ì„ë² ë”© í¬í•¨)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_units")
                .select("id, raw_note_id, claim, context, embedding, embedding_model, extracted_at")
                .not_.is_("embedding", "null")
                .order("id")
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} thought units with embeddings"
            )
            return response.data

        except Exception as e:
            logger.error(f"Failed to get thought units with embeddings: {e}")
            raise

    # ============================================================
    # Thought Pairs CRUD (Step 3: ZK ë ˆì´ì–´)
    # ============================================================

    async def find_candidate_pairs(
        self,
        min_similarity: float = 0.05,
        max_similarity: float = 0.35,
        top_k: int = 30,
        limit: int = 20
    ) -> List[dict]:
        """
        Top-K ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í›„ë³´ í˜ì–´ ì¡°íšŒ (HNSW ì¸ë±ìŠ¤ í™œìš©).

        Args:
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ (ê¸°ë³¸ 0.05, ë‚®ì€ ìœ ì‚¬ë„ = ì„œë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´)
            max_similarity: ìµœëŒ€ ìœ ì‚¬ë„ (ê¸°ë³¸ 0.35, ì•½í•œ ì—°ê²° = ì°½ì˜ì  í™•ì¥ ê°€ëŠ¥)
            top_k: ê° thoughtë‹¹ ê²€ìƒ‰í•  ìƒìœ„ Kê°œ (ê¸°ë³¸ 30)
            limit: ìµœì¢… ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 20)

        Returns:
            í›„ë³´ ìŒ ëª©ë¡ (thought_a_id, thought_b_id, similarity_score, thought_a_claim, thought_b_claim)

        Performance:
            - ë³µì¡ë„: O(n Ã— K) (ê¸°ì¡´ O(nÂ²)ì—ì„œ 98% ê°œì„ )
            - ì‹¤í–‰ ì‹œê°„: ~5ì´ˆ (ê¸°ì¡´ 60ì´ˆ+ íƒ€ì„ì•„ì›ƒ)
            - HNSW ì¸ë±ìŠ¤ ìë™ í™œìš©

        Raises:
            Exception: Stored Procedure í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "find_similar_pairs_topk",
                {
                    "min_sim": min_similarity,
                    "max_sim": max_similarity,
                    "top_k": top_k,
                    "lim": limit
                }
            ).execute()

            logger.info(
                f"Found {len(response.data)} candidate pairs with weak connections "
                f"(similarity: {min_similarity:.2f}-{max_similarity:.2f}, top_k={top_k})"
            )
            return response.data

        except Exception as e:
            error_msg = str(e)
            if "function" in error_msg.lower() and "does not exist" in error_msg.lower():
                logger.error(
                    "Stored procedure 'find_similar_pairs_topk' not found. "
                    "Please run docs/supabase_migrations/005_create_topk_function.sql"
                )
                raise Exception(
                    "Stored procedure 'find_similar_pairs_topk' not found. "
                    "Please run migration 005"
                )
            logger.error(f"Failed to find candidate pairs: {e}")
            raise

    async def insert_thought_pair(self, pair: ThoughtPairCreate) -> dict:
        """
        Thought pair í•œ ê°œ ì €ì¥ (UPSERT).

        Args:
            pair: ì €ì¥í•  í˜ì–´ ë°ì´í„°

        Returns:
            ì €ì¥ëœ thought pair ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            data = pair.model_dump(mode='json')

            # UPSERT (thought_a_id, thought_b_id ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€)
            # is_used_in_essayëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
            response = await (
                self.client.table("thought_pairs")
                .upsert(data, on_conflict="thought_a_id,thought_b_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Thought pair upserted: ID={response.data[0]['id']}, "
                    f"thoughts=({pair.thought_a_id}, {pair.thought_b_id}), "
                    f"similarity={pair.similarity_score:.3f}"
                )
                return response.data[0]
            else:
                raise Exception("Upsert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to upsert thought pair ({pair.thought_a_id}, {pair.thought_b_id}): {e}"
            )
            raise

    async def insert_thought_pairs_batch(
        self, pairs: List[ThoughtPairCreate]
    ) -> List[dict]:
        """
        ì—¬ëŸ¬ thought pairs ë°°ì¹˜ ì €ì¥ (UPSERT).

        Args:
            pairs: ì €ì¥í•  í˜ì–´ ëª©ë¡

        Returns:
            ì €ì¥ëœ thought pairs ë°ì´í„° ëª©ë¡

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        if not pairs:
            logger.warning("Batch insert called with empty pairs list")
            return []

        try:
            # ëª¨ë“  pairë¥¼ JSON ì§ë ¬í™”
            data = [pair.model_dump(mode='json') for pair in pairs]

            # ë°°ì¹˜ UPSERT
            response = await (
                self.client.table("thought_pairs")
                .upsert(data, on_conflict="thought_a_id,thought_b_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Batch upserted {len(response.data)} thought pairs "
                    f"(avg similarity: {sum(p.similarity_score for p in pairs) / len(pairs):.3f})"
                )
                return response.data
            else:
                raise Exception("Batch upsert returned no data")

        except Exception as e:
            logger.error(f"Failed to batch upsert {len(pairs)} thought pairs: {e}")
            raise

    async def get_unused_thought_pairs(self, limit: int = 10) -> List[dict]:
        """
        ë¯¸ì‚¬ìš© thought pairs ì¡°íšŒ (ì—ì„¸ì´ ìƒì„±ìš©).

        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 10)

        Returns:
            ë¯¸ì‚¬ìš© thought pairs ëª©ë¡ (similarity_score ASC ì •ë ¬ - ë‚®ì€ ìœ ì‚¬ë„ë¶€í„°)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_pairs")
                .select("*")
                .eq("is_used_in_essay", False)
                .order("similarity_score", desc=False)  # ë‚®ì€ ìœ ì‚¬ë„ë¶€í„° ì„ íƒ (ì°½ì˜ì  ì¡°í•©)
                .limit(limit)
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} unused thought pairs (limit: {limit})"
            )
            return response.data

        except Exception as e:
            logger.error(f"Failed to get unused thought pairs: {e}")
            raise

    async def update_pair_used_status(
        self, pair_id: int, is_used: bool = True
    ) -> dict:
        """
        Thought pair ì‚¬ìš© ìƒíƒœ ì—…ë°ì´íŠ¸.

        Args:
            pair_id: í˜ì–´ ID
            is_used: ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)

        Returns:
            ì—…ë°ì´íŠ¸ëœ thought pair ë°ì´í„°

        Raises:
            Exception: DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_pairs")
                .update({"is_used_in_essay": is_used})
                .eq("id", pair_id)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Updated thought pair {pair_id} used status: {is_used}"
                )
                return response.data[0]
            else:
                raise Exception(f"Thought pair {pair_id} not found")

        except Exception as e:
            logger.error(f"Failed to update pair {pair_id} used status: {e}")
            raise

    async def get_pair_with_thoughts(self, pair_id: int) -> dict:
        """
        Thought pair + ì‚¬ê³  ë‹¨ìœ„ + ì›ë³¸ ë©”ëª¨ ì •ë³´ JOIN ì¡°íšŒ.

        Args:
            pair_id: í˜ì–´ ID

        Returns:
            í˜ì–´ ì •ë³´ + ì–‘ìª½ thoughtì˜ claim/context + ì›ë³¸ ë©”ëª¨ title/url

        Raises:
            Exception: DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # Step 1: í˜ì–´ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
            pair_response = await (
                self.client.table("thought_pairs")
                .select("*")
                .eq("id", pair_id)
                .single()
                .execute()
            )

            if not pair_response.data:
                raise Exception(f"Thought pair {pair_id} not found")

            pair_data = pair_response.data

            # Step 2: ë‘ ê°œì˜ thought units ì¡°íšŒ
            thought_a_response = await (
                self.client.table("thought_units")
                .select("id, claim, context, raw_note_id")
                .eq("id", pair_data["thought_a_id"])
                .single()
                .execute()
            )

            thought_b_response = await (
                self.client.table("thought_units")
                .select("id, claim, context, raw_note_id")
                .eq("id", pair_data["thought_b_id"])
                .single()
                .execute()
            )

            thought_a = thought_a_response.data
            thought_b = thought_b_response.data

            # Step 3: ë‘ ê°œì˜ raw notes ì¡°íšŒ
            raw_note_a_response = await (
                self.client.table("raw_notes")
                .select("id, title, notion_url")
                .eq("id", thought_a["raw_note_id"])
                .single()
                .execute()
            )

            raw_note_b_response = await (
                self.client.table("raw_notes")
                .select("id, title, notion_url")
                .eq("id", thought_b["raw_note_id"])
                .single()
                .execute()
            )

            raw_note_a = raw_note_a_response.data
            raw_note_b = raw_note_b_response.data

            # Step 4: ê²°ê³¼ ì¡°í•©
            result = {
                "pair_id": pair_data["id"],
                "similarity_score": pair_data["similarity_score"],
                "connection_reason": pair_data["connection_reason"],
                "is_used_in_essay": pair_data["is_used_in_essay"],
                "selected_at": pair_data["selected_at"],
                "thought_a": {
                    "id": thought_a["id"],
                    "claim": thought_a["claim"],
                    "context": thought_a["context"],
                    "source_title": raw_note_a["title"],
                    "source_url": raw_note_a["notion_url"]
                },
                "thought_b": {
                    "id": thought_b["id"],
                    "claim": thought_b["claim"],
                    "context": thought_b["context"],
                    "source_title": raw_note_b["title"],
                    "source_url": raw_note_b["notion_url"]
                }
            }

            logger.info(
                f"Retrieved pair {pair_id} with full thought details "
                f"(thoughts: {thought_a['id']}, {thought_b['id']})"
            )
            return result

        except Exception as e:
            logger.error(f"Failed to get pair {pair_id} with thoughts: {e}")
            raise

    # ========================
    # Essay CRUD Methods (Step 4)
    # ========================

    async def insert_essay(self, essay: "EssayCreate") -> dict:
        """
        essays í…Œì´ë¸”ì— ë‹¨ì¼ ì—ì„¸ì´ ì €ì¥.

        Args:
            essay: EssayCreate ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            {
                "id": int,
                "type": str,
                "title": str,
                "outline": list[str],
                "used_thoughts_json": list[dict],
                "reason": str,
                "pair_id": int,
                "generated_at": str (ISO format)
            }

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSONB í•„ë“œ ì§ë ¬í™”
            essay_dict = {
                "type": essay.type,
                "title": essay.title,
                "outline": essay.outline,  # list â†’ JSONB (ìë™)
                "used_thoughts_json": [t.model_dump() for t in essay.used_thoughts],  # JSONB
                "reason": essay.reason,
                "pair_id": essay.pair_id
            }

            response = await self.client.table("essays")\
                .insert(essay_dict)\
                .execute()

            inserted = response.data[0]
            logger.info(f"Inserted essay ID {inserted['id']} for pair {essay.pair_id}")
            return inserted

        except Exception as e:
            logger.error(f"Failed to insert essay: {e}")
            logger.error(f"Essay data: {essay_dict}")
            raise

    async def insert_essays_batch(self, essays: List["EssayCreate"]) -> List[dict]:
        """
        ì—¬ëŸ¬ ì—ì„¸ì´ ë°°ì¹˜ ì €ì¥.

        Args:
            essays: EssayCreate ëª¨ë¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ëœ ì—ì„¸ì´ ë¦¬ìŠ¤íŠ¸

        Note:
            - UPSERTëŠ” í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€ëŠ” pair_id ì™¸ë˜í‚¤ë¡œ ë³´ì¥)
            - ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¡¤ë°±
        """
        await self._ensure_initialized()

        if not essays:
            return []

        try:
            essays_dict = [
                {
                    "type": e.type,
                    "title": e.title,
                    "outline": e.outline,
                    "used_thoughts_json": [t.model_dump() for t in e.used_thoughts],
                    "reason": e.reason,
                    "pair_id": e.pair_id
                }
                for e in essays
            ]

            response = await self.client.table("essays")\
                .insert(essays_dict)\
                .execute()

            inserted = response.data
            logger.info(f"Batch inserted {len(inserted)} essays")
            return inserted

        except Exception as e:
            logger.error(f"Failed to batch insert essays: {e}")
            raise

    async def get_essays(
        self,
        limit: int = 10,
        offset: int = 0
    ) -> List[dict]:
        """
        essays í…Œì´ë¸” ì¡°íšŒ (ìµœì‹ ìˆœ).

        Args:
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜ (ê¸°ë³¸ 10)
            offset: ê±´ë„ˆë›¸ ê°œìˆ˜ (í˜ì´ì§€ë„¤ì´ì…˜)

        Returns:
            ì—ì„¸ì´ ë¦¬ìŠ¤íŠ¸ (JSONB í•„ë“œ ìë™ íŒŒì‹±ë¨)
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("essays")\
                .select("*")\
                .order("generated_at", desc=True)\
                .limit(limit)\
                .offset(offset)\
                .execute()

            essays = response.data
            logger.info(f"Retrieved {len(essays)} essays")
            return essays

        except Exception as e:
            logger.error(f"Failed to get essays: {e}")
            raise

    async def get_essay_by_id(self, essay_id: int) -> dict:
        """
        ë‹¨ì¼ ì—ì„¸ì´ ì¡°íšŒ.

        Args:
            essay_id: ì—ì„¸ì´ ID

        Returns:
            ì—ì„¸ì´ ë°ì´í„°

        Raises:
            Exception: ì—ì„¸ì´ê°€ ì—†ê±°ë‚˜ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("essays")\
                .select("*")\
                .eq("id", essay_id)\
                .single()\
                .execute()

            essay = response.data
            logger.info(f"Retrieved essay ID {essay_id}")
            return essay

        except Exception as e:
            logger.error(f"Failed to get essay {essay_id}: {e}")
            raise

    # ============================================================
    # Import Jobs CRUD (Background Task Tracking)
    # ============================================================

    async def create_import_job(self, job: "ImportJobCreate") -> dict:
        """
        Create new import job record.

        Args:
            job: Import job creation data

        Returns:
            dict: Created job record with UUID

        Raises:
            Exception: Job creation failed
        """
        await self._ensure_initialized()

        try:
            data = {
                "status": "pending",
                "mode": job.mode,
                "config_json": job.config_json
            }
            response = await self.client.table("import_jobs").insert(data).execute()
            created = response.data[0]
            logger.info(f"Created import job {created['id']} (mode: {job.mode})")
            return created
        except Exception as e:
            logger.error(f"Failed to create import job: {e}")
            raise

    async def update_import_job(self, job_id: str, updates: "ImportJobUpdate") -> dict:
        """
        Update import job progress.

        Args:
            job_id: UUID of import job
            updates: Fields to update

        Returns:
            dict: Updated job record

        Raises:
            Exception: Job not found or update failed
        """
        await self._ensure_initialized()

        try:
            data = updates.model_dump(exclude_none=True, mode='json')
            response = await self.client.table("import_jobs")\
                .update(data).eq("id", job_id).execute()

            if not response.data:
                raise Exception(f"Import job {job_id} not found")

            return response.data[0]
        except Exception as e:
            logger.error(f"Failed to update import job {job_id}: {e}")
            raise

    async def get_import_job(self, job_id: str) -> dict:
        """
        Retrieve import job by ID.

        Args:
            job_id: UUID of import job

        Returns:
            dict: Job record

        Raises:
            Exception: Job not found
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("import_jobs")\
                .select("*").eq("id", job_id).single().execute()

            if not response.data:
                raise Exception(f"Import job {job_id} not found")

            return response.data
        except Exception as e:
            logger.error(f"Failed to get import job {job_id}: {e}")
            raise

    async def get_pages_to_fetch(
        self,
        notion_pages: List[Dict[str, Any]]
    ) -> tuple[List[str], List[str], List[str]]:
        """
        Compare Notion pages with DB using server-side RPC.

        Uses PostgreSQL function for efficient change detection.
        Falls back to full table scan if RPC fails.

        Args:
            notion_pages: List of page metadata from Notion API
                Each page must have: id, last_edited_time

        Returns:
            Tuple of (new_page_ids, updated_page_ids, deleted_page_ids)

        Performance:
            - RPC mode: ~150ms (constant time, scales to 100k pages)
            - Fallback mode: ~110ms (current size)
            - Network: Only changed pages (0.5KB vs 60KB)

        Example:
            >>> pages = [{"id": "abc", "last_edited_time": "2024-01-15T14:30:00.000Z"}]
            >>> new, updated, deleted = await service.get_pages_to_fetch(pages)
            >>> print(f"New: {len(new)}, Updated: {len(updated)}, Deleted: {len(deleted)}")
        """
        await self._ensure_initialized()

        # Prepare data for RPC
        pages_json = []
        force_new_ids = []  # Pages with invalid timestamps â†’ treat as new

        for p in notion_pages:
            page_id = p.get("id")
            last_edited = p.get("last_edited_time")

            if not page_id:
                logger.warning("Page missing 'id' field, skipping")
                continue

            if not last_edited:
                logger.warning(f"Page {page_id} missing 'last_edited_time', treating as new")
                force_new_ids.append(page_id)
                continue

            try:
                # Parse ISO 8601 timestamp
                notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))

                # Truncate to seconds (match SQL function behavior)
                notion_time = notion_time.replace(microsecond=0)

                pages_json.append({
                    "id": page_id,
                    "last_edited": notion_time.isoformat()
                })
            except (ValueError, AttributeError, TypeError) as e:
                logger.warning(f"Invalid timestamp for {page_id}: {e}, treating as new")
                force_new_ids.append(page_id)

        if not pages_json and not force_new_ids:
            logger.warning("No valid pages to check")
            return [], [], []

        logger.info(f"Change detection: checking {len(pages_json)} pages via RPC (sample: {[p['id'] for p in pages_json[:3]]})")

        # Try RPC change detection (Solution 3)
        try:
            start_time = time.time()

            response = await self.client.rpc('get_changed_pages', {
                'pages_data': pages_json
            }).execute()

            elapsed = time.time() - start_time

            # Validate response structure
            if not response.data or not isinstance(response.data, dict):
                raise ValueError("Invalid RPC response format: expected dict")

            result = response.data

            # Check for SQL function error
            if 'error' in result:
                raise ValueError(f"SQL function error: {result['error']} (SQLSTATE: {result.get('error_detail', 'unknown')})")

            # Extract results
            new_page_ids = result.get('new_page_ids', [])
            updated_page_ids = result.get('updated_page_ids', [])
            deleted_page_ids = result.get('deleted_page_ids', [])

            # Validate types
            if not isinstance(new_page_ids, list):
                raise ValueError(f"Invalid type for new_page_ids: {type(new_page_ids)}")
            if not isinstance(updated_page_ids, list):
                raise ValueError(f"Invalid type for updated_page_ids: {type(updated_page_ids)}")
            if not isinstance(deleted_page_ids, list):
                raise ValueError(f"Invalid type for deleted_page_ids: {type(deleted_page_ids)}")

            # Add force_new pages
            new_page_ids.extend(force_new_ids)

            # Validate UUIDs
            import re
            UUID_PATTERN = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)

            for page_id in new_page_ids + updated_page_ids + deleted_page_ids:
                if not UUID_PATTERN.match(page_id):
                    raise ValueError(f"Invalid UUID format: {page_id}")

            logger.info(
                f"âœ… RPC change detection completed in {elapsed:.2f}s: "
                f"{len(new_page_ids)} new, {len(updated_page_ids)} updated, "
                f"{len(deleted_page_ids)} deleted, "
                f"{result.get('unchanged_count', len(pages_json) - len(new_page_ids) - len(updated_page_ids))} unchanged"
            )

            return new_page_ids, updated_page_ids, deleted_page_ids

        except Exception as rpc_error:
            logger.error(f"âŒ RPC change detection failed: {rpc_error}, falling back to full table scan")

            # Fallback: Full table scan (ë°©ì‹ A)
            try:
                logger.info("Using fallback: full table scan")

                response = await (
                    self.client.table("raw_notes")
                    .select("notion_page_id, notion_last_edited_time")
                    .execute()
                )

                # Build existing_map from DB
                existing_map = {}
                for row in response.data:
                    db_page_id = row["notion_page_id"]
                    db_time = row["notion_last_edited_time"]

                    # Parse timestamp
                    if isinstance(db_time, str):
                        db_time = datetime.fromisoformat(db_time.replace("Z", "+00:00"))

                    # Ensure timezone-aware
                    if db_time.tzinfo is None:
                        db_time = db_time.replace(tzinfo=timezone.utc)

                    # Truncate to seconds
                    db_time = db_time.replace(microsecond=0)
                    existing_map[db_page_id] = db_time

                # Build page_map from Notion pages
                page_map = {}
                for p_json in pages_json:
                    page_id = p_json["id"]
                    notion_time = datetime.fromisoformat(p_json["last_edited"])
                    page_map[page_id] = notion_time

                # Compare
                new_ids = []
                updated_ids = []

                for page_id, notion_time in page_map.items():
                    if page_id not in existing_map:
                        new_ids.append(page_id)
                    elif notion_time > existing_map[page_id]:
                        updated_ids.append(page_id)

                # Add force_new pages
                new_ids.extend(force_new_ids)

                # Detect deleted pages (in DB but not in Notion)
                all_notion_ids = set(page_map.keys())
                deleted_ids = [
                    db_id for db_id in existing_map.keys()
                    if db_id not in all_notion_ids
                ]

                logger.info(
                    f"âœ… Fallback completed: {len(new_ids)} new, {len(updated_ids)} updated, "
                    f"{len(deleted_ids)} deleted, "
                    f"{len(page_map) - len(new_ids) - len(updated_ids)} unchanged"
                )

                return new_ids, updated_ids, deleted_ids

            except Exception as fallback_error:
                logger.error(f"âŒ Fallback also failed: {fallback_error}, treating all as new (last resort)")

                # Last resort: treat all as new
                all_ids = [p["id"] for p in pages_json] + force_new_ids
                return all_ids, [], []

    async def validate_rpc_function_exists(self) -> bool:
        """
        Check if RPC function is deployed in Supabase.

        Returns:
            bool: True if function exists and works, False otherwise
        """
        try:
            await self._ensure_initialized()

            # Test with empty array
            response = await self.client.rpc('get_changed_pages', {
                'pages_data': []
            }).execute()

            # Validate response
            if not response.data or not isinstance(response.data, dict):
                logger.warning("âš ï¸  RPC function returned unexpected format")
                return False

            logger.info("âœ… RPC function 'get_changed_pages' is available and working")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  RPC function 'get_changed_pages' not available: {e}")
            logger.warning("   Import will use fallback mode (full table scan)")
            return False

    async def soft_delete_raw_note(self, notion_page_id: str) -> None:
        """
        Mark a page as deleted without removing from DB (soft delete).

        This preserves the page and all downstream data (thought_units, essays)
        while marking it as deleted in Notion.

        Args:
            notion_page_id: Notion page ID to soft delete

        Raises:
            No exceptions - failures are logged only
        """
        await self._ensure_initialized()

        try:
            await self.client.table("raw_notes").update({
                "is_deleted": True,
                "deleted_at": datetime.now(timezone.utc).isoformat()
            }).eq("notion_page_id", notion_page_id).execute()

            logger.warning(f"ğŸ—‘ï¸  Soft deleted page: {notion_page_id} (essays preserved)")

        except Exception as e:
            logger.error(f"Failed to soft delete page {notion_page_id}: {e}")

    async def increment_job_progress(
        self,
        job_id: str,
        imported: bool = False,
        skipped: bool = False,
        failed_page: Optional[Dict[str, str]] = None
    ) -> None:
        """
        Atomically increment job progress counters.

        This method NEVER raises exceptions - failures are logged only.
        This ensures import continues even if progress tracking fails.

        Args:
            job_id: UUID of import job
            imported: True if page was successfully imported
            skipped: True if page was skipped
            failed_page: Dict with page_id and error_message if page failed
        """
        await self._ensure_initialized()

        try:
            job = await self.get_import_job(job_id)
            updates = {"processed_pages": job["processed_pages"] + 1}

            if imported:
                updates["imported_pages"] = job["imported_pages"] + 1
            if skipped:
                updates["skipped_pages"] = job["skipped_pages"] + 1
            if failed_page:
                current_failed = job.get("failed_pages", [])
                current_failed.append(failed_page)
                updates["failed_pages"] = current_failed

            await self.client.table("import_jobs").update(updates).eq("id", job_id).execute()
        except Exception as e:
            # âœ… CRITICAL: Don't raise - just log
            # Import continues even if progress tracking fails
            logger.error(f"Failed to increment job {job_id} progress: {e}")

    # ============================================================
    # Pair Candidates CRUD (í•˜ì´ë¸Œë¦¬ë“œ C ì „ëµ)
    # ============================================================

    async def insert_pair_candidates_batch(
        self,
        candidates: List[PairCandidateCreate],
        batch_size: int = 1000
    ) -> PairCandidateBatch:
        """
        30,000ê°œ í›„ë³´ë¥¼ pair_candidates í…Œì´ë¸”ì— ëŒ€ëŸ‰ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬).

        Args:
            candidates: ì €ì¥í•  í›„ë³´ ìŒ ëª©ë¡ (ì˜ˆ: 30,000ê°œ)
            batch_size: ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  ê°œìˆ˜ (ê¸°ë³¸ 1000ê°œ)

        Returns:
            PairCandidateBatch: {
                inserted_count: int,    # ì„±ê³µì ìœ¼ë¡œ ì €ì¥ëœ ê°œìˆ˜
                duplicate_count: int,   # ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µëœ ê°œìˆ˜
                error_count: int        # ì‹¤íŒ¨í•œ ê°œìˆ˜
            }

        Performance:
            - 30,000ê°œ ì €ì¥ < 3ë¶„
            - ON CONFLICT DO NOTHING (ì¤‘ë³µ ìë™ ë¬´ì‹œ)

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ (ì „ì²´ ë°°ì¹˜ ë¡¤ë°± ì•„ë‹˜)
        """
        await self._ensure_initialized()

        if not candidates:
            logger.warning("insert_pair_candidates_batch called with empty list")
            return PairCandidateBatch(
                inserted_count=0,
                duplicate_count=0,
                error_count=0
            )

        total_candidates = len(candidates)
        inserted_count = 0
        duplicate_count = 0
        error_count = 0

        logger.info(
            f"Starting batch insert: {total_candidates} candidates "
            f"(batch_size={batch_size})"
        )

        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for i in range(0, total_candidates, batch_size):
            batch = candidates[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (total_candidates + batch_size - 1) // batch_size

            try:
                # JSON ì§ë ¬í™”
                data = [candidate.model_dump(mode='json') for candidate in batch]

                # Supabase upsert (ì¤‘ë³µ ì‹œ ë¬´ì‹œ)
                # ON CONFLICT (thought_a_id, thought_b_id) DO NOTHING
                response = await (
                    self.client.table("pair_candidates")
                    .upsert(data, on_conflict="thought_a_id,thought_b_id", ignore_duplicates=True)
                    .execute()
                )

                # SupabaseëŠ” ì¤‘ë³µ ë¬´ì‹œ ì‹œ dataê°€ ë¹ˆ ë°°ì—´ë¡œ ë°˜í™˜ë¨
                current_inserted = len(response.data) if response.data else 0
                current_duplicate = len(batch) - current_inserted

                inserted_count += current_inserted
                duplicate_count += current_duplicate

                logger.info(
                    f"Batch {batch_num}/{total_batches}: "
                    f"{current_inserted} inserted, {current_duplicate} duplicates"
                )

            except Exception as e:
                error_count += len(batch)
                logger.error(
                    f"Failed to insert batch {batch_num}/{total_batches} "
                    f"({len(batch)} candidates): {e}"
                )

        result = PairCandidateBatch(
            inserted_count=inserted_count,
            duplicate_count=duplicate_count,
            error_count=error_count
        )

        logger.info(
            f"Batch insert completed: {inserted_count} inserted, "
            f"{duplicate_count} duplicates, {error_count} errors "
            f"(total: {total_candidates})"
        )

        return result

    async def get_pending_candidates(
        self,
        limit: int = 100,
        similarity_range: tuple[float, float] = (0.05, 0.35)
    ) -> List[dict]:
        """
        ë°°ì¹˜ ì›Œì»¤ê°€ ë¯¸í‰ê°€ í›„ë³´ ì¡°íšŒ (Claude í‰ê°€ìš©).

        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 100)
            similarity_range: (min_similarity, max_similarity) ë²”ìœ„ (ê¸°ë³¸ 0.05-0.35)

        Returns:
            List[dict]: ë¯¸í‰ê°€ í›„ë³´ ëª©ë¡ (thought claim í¬í•¨ JOIN)
                ê° dict êµ¬ì¡°:
                {
                    "id": int,
                    "thought_a_id": int,
                    "thought_b_id": int,
                    "thought_a_claim": str,
                    "thought_b_claim": str,
                    "similarity": float,
                    "raw_note_id_a": str,
                    "raw_note_id_b": str
                }

        Performance:
            - <100ms (ì¸ë±ìŠ¤ í™œìš©)
            - FIFO ë°©ì‹ (created_at ASC)

        Note:
            - llm_status='pending' AND llm_attempts < 3
            - thought_unitsì™€ 2ë²ˆ JOIN (claim í•„ìš”)
        """
        await self._ensure_initialized()

        min_sim, max_sim = similarity_range

        try:
            # pair_candidatesì—ì„œ pendingì¸ ê²ƒë§Œ ì¡°íšŒ (í•„í„°ë§ ë¨¼ì €)
            response = await (
                self.client.table("pair_candidates")
                .select("*")
                .eq("llm_status", "pending")
                .lt("llm_attempts", 3)
                .gte("similarity", min_sim)
                .lte("similarity", max_sim)
                .order("created_at", desc=False)  # FIFO
                .limit(limit)
                .execute()
            )

            candidates = response.data

            if not candidates:
                logger.info("No pending candidates found")
                return []

            # thought_units ì¡°íšŒë¥¼ ìœ„í•œ ID ìˆ˜ì§‘
            thought_ids = set()
            for c in candidates:
                thought_ids.add(c["thought_a_id"])
                thought_ids.add(c["thought_b_id"])

            # thought_units í•œ ë²ˆì— ì¡°íšŒ (N+1 ì¿¼ë¦¬ ë°©ì§€)
            thoughts_response = await (
                self.client.table("thought_units")
                .select("id, claim")
                .in_("id", list(thought_ids))
                .execute()
            )

            # thought_id â†’ claim ë§¤í•‘
            thought_map = {t["id"]: t["claim"] for t in thoughts_response.data}

            # claim ì¶”ê°€
            result = []
            for c in candidates:
                thought_a_claim = thought_map.get(c["thought_a_id"])
                thought_b_claim = thought_map.get(c["thought_b_id"])

                # claimì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë°ì´í„° ì •í•©ì„± ë¬¸ì œ)
                if not thought_a_claim or not thought_b_claim:
                    logger.warning(
                        f"Missing claim for candidate {c['id']}: "
                        f"thought_a={c['thought_a_id']}, thought_b={c['thought_b_id']}"
                    )
                    continue

                result.append({
                    "id": c["id"],
                    "thought_a_id": c["thought_a_id"],
                    "thought_b_id": c["thought_b_id"],
                    "thought_a_claim": thought_a_claim,
                    "thought_b_claim": thought_b_claim,
                    "similarity": c["similarity"],
                    "raw_note_id_a": c["raw_note_id_a"],
                    "raw_note_id_b": c["raw_note_id_b"]
                })

            logger.info(
                f"Retrieved {len(result)} pending candidates "
                f"(similarity: {min_sim:.2f}-{max_sim:.2f}, limit: {limit})"
            )
            return result

        except Exception as e:
            logger.error(f"Failed to get pending candidates: {e}")
            raise

    async def update_candidate_score(
        self,
        candidate_id: int,
        llm_score: int,
        connection_reason: str
    ) -> dict:
        """
        Claude í‰ê°€ ê²°ê³¼ë¥¼ pair_candidatesì— ì—…ë°ì´íŠ¸.

        Args:
            candidate_id: í›„ë³´ ID
            llm_score: Claude í‰ê°€ ì ìˆ˜ (0-100)
            connection_reason: Claudeê°€ ìƒì„±í•œ ì—°ê²° ì´ìœ 

        Returns:
            dict: ì—…ë°ì´íŠ¸ëœ candidate row

        Raises:
            Exception: DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # llm_attempts ì¦ê°€ë¥¼ ìœ„í•´ ë¨¼ì € ì¡°íšŒ
            get_response = await (
                self.client.table("pair_candidates")
                .select("llm_attempts")
                .eq("id", candidate_id)
                .single()
                .execute()
            )

            if not get_response.data:
                raise Exception(f"Candidate {candidate_id} not found")

            current_attempts = get_response.data["llm_attempts"]

            # ì—…ë°ì´íŠ¸
            update_data = {
                "llm_score": llm_score,
                "llm_status": "completed",
                "llm_attempts": current_attempts + 1,
                "last_evaluated_at": datetime.now(timezone.utc).isoformat(),
                "connection_reason": connection_reason,
                "evaluation_error": None  # ì„±ê³µ ì‹œ ì—ëŸ¬ ì´ˆê¸°í™”
            }

            response = await (
                self.client.table("pair_candidates")
                .update(update_data)
                .eq("id", candidate_id)
                .execute()
            )

            if not response.data:
                raise Exception(f"Update returned no data for candidate {candidate_id}")

            updated = response.data[0]

            logger.info(
                f"Updated candidate {candidate_id}: score={llm_score}, "
                f"attempts={updated['llm_attempts']}"
            )

            return updated

        except Exception as e:
            logger.error(f"Failed to update candidate {candidate_id} score: {e}")
            raise

    async def move_to_thought_pairs(
        self,
        candidate_ids: List[int],
        min_score: int = 65
    ) -> int:
        """
        ê³ ë“ì  í›„ë³´ë¥¼ pair_candidatesì—ì„œ thought_pairsë¡œ ì´ë™.

        Args:
            candidate_ids: ì´ë™í•  í›„ë³´ ID ëª©ë¡
            min_score: ìµœì†Œ ì ìˆ˜ (ê¸°ë³¸ 65, standard tier)

        Returns:
            int: ì‹¤ì œ ì´ë™ëœ í˜ì–´ ê°œìˆ˜

        Logic:
            1. pair_candidatesì—ì„œ ì¡°íšŒ (score >= min_score)
            2. quality_tier ê³„ì‚° (standard/premium/excellent)
            3. ThoughtPairCreateExtended ìƒì„±
            4. insert_thought_pairs_batch() í˜¸ì¶œ (UPSERT)

        Quality Tiers:
            - standard: 65-84
            - premium: 85-94
            - excellent: 95-100

        Raises:
            Exception: DB ì¡°íšŒ ë˜ëŠ” ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        if not candidate_ids:
            logger.warning("move_to_thought_pairs called with empty candidate_ids")
            return 0

        try:
            # Step 1: pair_candidatesì—ì„œ ì¡°íšŒ (score í•„í„°ë§)
            response = await (
                self.client.table("pair_candidates")
                .select("*")
                .in_("id", candidate_ids)
                .gte("llm_score", min_score)
                .eq("llm_status", "completed")
                .execute()
            )

            candidates = response.data

            if not candidates:
                logger.info(
                    f"No candidates with score >= {min_score} found "
                    f"(checked {len(candidate_ids)} IDs)"
                )
                return 0

            # Step 2: quality_tier ê³„ì‚° ë° ThoughtPairCreateExtended ìƒì„±
            pairs_to_insert = []

            for c in candidates:
                llm_score = c["llm_score"]

                # quality_tier ê³„ì‚°
                if llm_score >= 95:
                    quality_tier = "excellent"
                elif llm_score >= 85:
                    quality_tier = "premium"
                else:
                    quality_tier = "standard"

                pair = ThoughtPairCreateExtended(
                    thought_a_id=c["thought_a_id"],
                    thought_b_id=c["thought_b_id"],
                    similarity_score=c["similarity"],
                    connection_reason=c.get("connection_reason", ""),
                    claude_score=llm_score,
                    quality_tier=quality_tier,
                    essay_content=None  # UI í”„ë¦¬ë·°ëŠ” ë³„ë„ ìƒì„±
                )

                pairs_to_insert.append(pair)

            # Step 3: thought_pairsì— ë°°ì¹˜ ì €ì¥ (UPSERT)
            # Note: insert_thought_pairs_batch()ëŠ” ê¸°ì¡´ ë©”ì„œë“œ ì¬ì‚¬ìš© ë¶ˆê°€
            # (claude_score, quality_tier í•„ë“œ ì—†ìŒ)
            # ì§ì ‘ upsert ìˆ˜í–‰

            if not pairs_to_insert:
                logger.warning("No pairs to insert after quality_tier calculation")
                return 0

            # JSON ì§ë ¬í™”
            data = [pair.model_dump(mode='json') for pair in pairs_to_insert]

            # ë°°ì¹˜ UPSERT (ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸)
            upsert_response = await (
                self.client.table("thought_pairs")
                .upsert(data, on_conflict="thought_a_id,thought_b_id")
                .execute()
            )

            if not upsert_response.data:
                raise Exception("Batch upsert returned no data")

            moved_count = len(upsert_response.data)

            logger.info(
                f"Moved {moved_count} pairs to thought_pairs "
                f"(min_score={min_score}, quality tiers: "
                f"excellent={sum(1 for p in pairs_to_insert if p.quality_tier == 'excellent')}, "
                f"premium={sum(1 for p in pairs_to_insert if p.quality_tier == 'premium')}, "
                f"standard={sum(1 for p in pairs_to_insert if p.quality_tier == 'standard')})"
            )

            return moved_count

        except Exception as e:
            logger.error(
                f"Failed to move candidates to thought_pairs "
                f"({len(candidate_ids)} candidates): {e}"
            )
            raise

    # ============================================================
    # Distribution Cache (ìƒëŒ€ì  ì„ê³„ê°’ ì „ëµ)
    # ============================================================

    async def get_similarity_distribution_cache(self) -> Optional[Dict[str, Any]]:
        """
        ìœ ì‚¬ë„ ë¶„í¬ ìºì‹œ ì¡°íšŒ.

        Returns:
            {
                "thought_count": 1921,
                "total_pairs": 38420,
                "percentiles": {"p0": 0.26, "p10": 0.30, ...},
                "mean": 0.38,
                "stddev": 0.05,
                "calculated_at": "2026-01-26T10:00:00",
                "duration_ms": 5432
            }
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("similarity_distribution_cache")
                .select("*")
                .eq("id", 1)
                .maybe_single()
                .execute()
            )

            if not response.data:
                return None

            data = response.data

            # ë°±ë¶„ìœ„ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            percentiles = {
                "p0": data["p0"],
                "p10": data["p10"],
                "p20": data["p20"],
                "p30": data["p30"],
                "p40": data["p40"],
                "p50": data["p50"],
                "p60": data["p60"],
                "p70": data["p70"],
                "p80": data["p80"],
                "p90": data["p90"],
                "p100": data["p100"],
            }

            return {
                "thought_count": data["thought_unit_count"],
                "total_pairs": data["total_pair_count"],
                "percentiles": percentiles,
                "mean": data["mean"],
                "stddev": data["stddev"],
                "calculated_at": data["calculated_at"],
                "duration_ms": data.get("calculation_duration_ms"),
            }

        except Exception as e:
            logger.error(f"Failed to get distribution cache: {e}")
            raise

    async def calculate_similarity_distribution(self) -> Dict[str, Any]:
        """
        ìœ ì‚¬ë„ ë¶„í¬ ê³„ì‚° RPC í˜¸ì¶œ.

        NOTE: ì´ ë©”ì„œë“œëŠ” DEPRECATED. Distance Tableì—ì„œ ì§ì ‘ ê³„ì‚°í•˜ëŠ”
        calculate_distribution_from_distance_table()ì„ ì‚¬ìš©í•˜ì„¸ìš”.

        Returns:
            {
                "success": true,
                "thought_count": 1921,
                "total_pairs": 38420,
                "percentiles": {"p0": 0.26, ...},
                "mean": 0.38,
                "stddev": 0.05,
                "duration_ms": 5432
            }
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "calculate_similarity_distribution"
            ).execute()

            if not response.data:
                raise Exception("RPC returned no data")

            result = response.data

            logger.info(
                f"Distribution calculated: {result.get('thought_count')} thoughts, "
                f"{result.get('total_pairs')} pairs, "
                f"{result.get('duration_ms')}ms"
            )

            return result

        except Exception as e:
            logger.error(f"Failed to calculate distribution: {e}")
            raise

    async def calculate_distribution_from_distance_table(self) -> Dict[str, Any]:
        """
        Distance Table ê¸°ë°˜ ìœ ì‚¬ë„ ë¶„í¬ ê³„ì‚° (ë¹ ë¦„).

        ê¸°ì¡´ calculate_similarity_distribution: thought_units CROSS JOIN â†’ 60ì´ˆ+ íƒ€ì„ì•„ì›ƒ
        ì‹ ê·œ: thought_pair_distances ì§‘ê³„ â†’ 1ì´ˆ ë¯¸ë§Œ

        Returns:
            {
                "success": true,
                "total_pairs": 1821186,
                "percentiles": {
                    "total_pairs": 1821186,
                    "p0": 0.001, "p10": 0.057, ..., "p100": 0.987,
                    "mean": 0.342, "stddev": 0.15
                },
                "duration_ms": 850,
                "cached": true
            }
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "calculate_distribution_from_distance_table"
            ).execute()

            if not response.data:
                raise Exception("RPC returned no data")

            result = response.data

            if not result.get("success"):
                raise Exception(result.get("error", "Unknown error"))

            logger.info(
                f"Distribution calculated from Distance Table: "
                f"{result.get('total_pairs'):,} pairs, "
                f"{result.get('duration_ms')}ms"
            )

            return result

        except Exception as e:
            logger.error(f"Failed to calculate distribution from distance table: {e}")
            raise

    async def count_thought_units(self) -> int:
        """
        ì„ë² ë”©ì´ ìˆëŠ” thought_units ê°œìˆ˜ ì¡°íšŒ.

        Returns:
            thought_units ê°œìˆ˜
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_units")
                .select("id", count="exact")
                .not_.is_("embedding", "null")
                .execute()
            )

            count = response.count if response.count is not None else 0

            logger.debug(f"Thought units count: {count}")

            return count

        except Exception as e:
            logger.error(f"Failed to count thought units: {e}")
            raise

    # ============================================================
    # ìƒ˜í”Œë§ ê¸°ë°˜ ë§ˆì´ë‹ RPC (ì‹ ê·œ)
    # ============================================================

    async def mine_candidate_pairs(
        self,
        p_last_src_id: int = 0,
        p_src_batch: int = 30,
        p_dst_sample: int = 1200,
        p_k: int = 15,
        p_lo: float = 0.10,
        p_hi: float = 0.35,
        p_seed: int = 42,
        p_max_rounds: int = 3
    ) -> Dict[str, Any]:
        """
        ìƒ˜í”Œë§ ê¸°ë°˜ í›„ë³´ í˜ì–´ ë§ˆì´ë‹ RPC í˜¸ì¶œ

        Args:
            p_last_src_id: ë§ˆì§€ë§‰ ì²˜ë¦¬í•œ src ID (í‚¤ì…‹ í˜ì´ì§•)
            p_src_batch: ë°°ì¹˜ë‹¹ src ìˆ˜ (ê¸°ë³¸ 30)
            p_dst_sample: dst ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 1200)
            p_k: srcë‹¹ í›„ë³´ ìˆ˜ (ê¸°ë³¸ 15)
            p_lo: í•˜ìœ„ ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ 0.10)
            p_hi: ìƒìœ„ ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ 0.35)
            p_seed: ê²°ì •ë¡ ì  ìƒ˜í”Œë§ìš© ì‹œë“œ (ê¸°ë³¸ 42)
            p_max_rounds: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 3)

        Returns:
            {
                "success": bool,
                "new_last_src_id": int,
                "inserted_count": int,
                "src_processed_count": int,
                "rounds_used": int,
                "band_lo": float,
                "band_hi": float,
                "avg_candidates_per_src": float,
                "duration_ms": int
            }
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "mine_candidate_pairs",
                {
                    "p_last_src_id": p_last_src_id,
                    "p_src_batch": p_src_batch,
                    "p_dst_sample": p_dst_sample,
                    "p_k": p_k,
                    "p_lo": p_lo,
                    "p_hi": p_hi,
                    "p_seed": p_seed,
                    "p_max_rounds": p_max_rounds
                }
            ).execute()

            if not response.data:
                raise Exception("RPC returned no data")

            result = response.data

            if result.get("success"):
                logger.info(
                    f"mine_candidate_pairs: "
                    f"{result.get('inserted_count')} pairs, "
                    f"{result.get('src_processed_count')} sources, "
                    f"{result.get('duration_ms')}ms"
                )
            else:
                logger.error(f"mine_candidate_pairs failed: {result.get('error')}")

            return result

        except Exception as e:
            logger.error(f"mine_candidate_pairs exception: {e}")
            return {
                "success": False,
                "error": str(e),
                "new_last_src_id": p_last_src_id
            }

    async def build_distribution_sketch(
        self,
        p_seed: int = 42,
        p_src_sample: int = 200,
        p_dst_sample: int = 500,
        p_rounds: int = 1,
        p_exclude_same_memo: bool = True,
        p_policy: str = "random_pairs"
    ) -> Dict[str, Any]:
        """
        ì „ì—­ ë¶„í¬ ìŠ¤ì¼€ì¹˜ìš© ìƒ˜í”Œ ìˆ˜ì§‘ RPC í˜¸ì¶œ

        Args:
            p_seed: ê²°ì •ë¡ ì  ìƒ˜í”Œë§ìš© ì‹œë“œ (ê¸°ë³¸ 42)
            p_src_sample: src ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 200)
            p_dst_sample: dst ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 500)
            p_rounds: ìƒ˜í”Œë§ ë¼ìš´ë“œ ìˆ˜ (ê¸°ë³¸ 1)
            p_exclude_same_memo: ê°™ì€ ë©”ëª¨ ì œì™¸ ì—¬ë¶€ (ê¸°ë³¸ TRUE)
            p_policy: ìƒ˜í”Œë§ ì •ì±…ëª… (ê¸°ë³¸ random_pairs)

        Returns:
            {
                "success": bool,
                "run_id": str,
                "inserted_samples": int,
                "total_thoughts": int,
                "coverage_estimate": float,
                "duration_ms": int
            }
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "build_distribution_sketch",
                {
                    "p_seed": p_seed,
                    "p_src_sample": p_src_sample,
                    "p_dst_sample": p_dst_sample,
                    "p_rounds": p_rounds,
                    "p_exclude_same_memo": p_exclude_same_memo,
                    "p_policy": p_policy
                }
            ).execute()

            if not response.data:
                raise Exception("RPC returned no data")

            result = response.data

            if result.get("success"):
                logger.info(
                    f"build_distribution_sketch: "
                    f"{result.get('inserted_samples')} samples, "
                    f"run_id={result.get('run_id')}, "
                    f"{result.get('duration_ms')}ms"
                )
            else:
                logger.error(f"build_distribution_sketch failed: {result.get('error')}")

            return result

        except Exception as e:
            logger.error(f"build_distribution_sketch exception: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def calculate_distribution_from_sketch(
        self,
        p_run_id: str = None,
        p_sample_limit: int = 100000
    ) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ ê¸°ë°˜ ì „ì—­ ë¶„í¬ ê³„ì‚° RPC í˜¸ì¶œ

        Args:
            p_run_id: íŠ¹ì • runì˜ ìƒ˜í”Œ ì‚¬ìš© (NULLì´ë©´ ìµœì‹ )
            p_sample_limit: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 100,000)

        Returns:
            {
                "success": bool,
                "distribution": {
                    "p0": float, "p10": float, ..., "p100": float,
                    "mean": float, "stddev": float
                },
                "cached": bool,
                "is_approximate": bool,
                "sample_count": int,
                "duration_ms": int
            }
        """
        await self._ensure_initialized()

        try:
            params = {"p_sample_limit": p_sample_limit}
            if p_run_id:
                params["p_run_id"] = p_run_id

            response = await self.client.rpc(
                "calculate_distribution_from_sketch",
                params
            ).execute()

            if not response.data:
                raise Exception("RPC returned no data")

            result = response.data

            if result.get("success"):
                logger.info(
                    f"calculate_distribution_from_sketch: "
                    f"{result.get('sample_count')} samples, "
                    f"cached={result.get('cached')}, "
                    f"{result.get('duration_ms')}ms"
                )
            else:
                logger.error(f"calculate_distribution_from_sketch failed: {result.get('error')}")

            return result

        except Exception as e:
            logger.error(f"calculate_distribution_from_sketch exception: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    # ============================================================
    # ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ CRUD
    # ============================================================

    async def create_mining_progress(
        self,
        src_batch: int,
        dst_sample: int,
        k_per_src: int,
        p_lo: float,
        p_hi: float,
        max_rounds: int,
        seed: int
    ) -> Dict[str, Any]:
        """ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ ë ˆì½”ë“œ ìƒì„±"""
        await self._ensure_initialized()

        try:
            data = {
                "status": "in_progress",
                "started_at": datetime.now(timezone.utc).isoformat(),
                "src_batch": src_batch,
                "dst_sample": dst_sample,
                "k_per_src": k_per_src,
                "p_lo": p_lo,
                "p_hi": p_hi,
                "max_rounds": max_rounds,
                "seed": seed
            }

            response = await (
                self.client.table("pair_mining_progress")
                .insert(data)
                .execute()
            )

            if response.data:
                logger.info(f"Created mining progress: id={response.data[0]['id']}")
                return response.data[0]
            else:
                raise Exception("Insert returned no data")

        except Exception as e:
            logger.error(f"Failed to create mining progress: {e}")
            raise

    async def update_mining_progress(
        self,
        progress_id: int,
        status: str,
        last_src_id: int = None,
        total_src_processed: int = None,
        total_pairs_inserted: int = None,
        avg_candidates_per_src: float = None,
        error_message: str = None
    ) -> Dict[str, Any]:
        """ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        await self._ensure_initialized()

        try:
            data = {
                "status": status,
                "updated_at": datetime.now(timezone.utc).isoformat()
            }

            if last_src_id is not None:
                data["last_src_id"] = last_src_id
            if total_src_processed is not None:
                data["total_src_processed"] = total_src_processed
            if total_pairs_inserted is not None:
                data["total_pairs_inserted"] = total_pairs_inserted
            if avg_candidates_per_src is not None:
                data["avg_candidates_per_src"] = avg_candidates_per_src
            if error_message is not None:
                data["error_message"] = error_message

            if status == "completed":
                data["completed_at"] = datetime.now(timezone.utc).isoformat()

            response = await (
                self.client.table("pair_mining_progress")
                .update(data)
                .eq("id", progress_id)
                .execute()
            )

            if response.data:
                logger.debug(f"Updated mining progress: id={progress_id}, status={status}")
                return response.data[0]
            else:
                raise Exception(f"Mining progress {progress_id} not found")

        except Exception as e:
            logger.error(f"Failed to update mining progress: {e}")
            raise

    async def get_mining_progress(self) -> Optional[Dict[str, Any]]:
        """ìµœì‹  ë§ˆì´ë‹ ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("pair_mining_progress")
                .select("*")
                .order("updated_at", desc=True)
                .limit(1)
                .maybe_single()
                .execute()
            )

            return response.data

        except Exception as e:
            logger.error(f"Failed to get mining progress: {e}")
            return None

    # ============================================================
    # Distance Table ì¡°íšŒ (ë ˆê±°ì‹œ - í–¥í›„ ì‚­ì œ ì˜ˆì •)
    # ============================================================

    async def get_candidates_from_distance_table(
        self,
        min_similarity: float,
        max_similarity: float
    ) -> List[dict]:
        """
        Distance Tableì—ì„œ ìœ ì‚¬ë„ ë²”ìœ„ ë‚´ í›„ë³´ ì¡°íšŒ (ì´ˆê³ ì†).

        Performance: <0.1ì´ˆ (vs v4 60ì´ˆ+)

        Security: 80% ë²”ìœ„ ê²€ì¦ìœ¼ë¡œ ë¹„ì •ìƒ ìš”ì²­ ì°¨ë‹¨
        - Normal: p10_p40 (30% ë²”ìœ„) â†’ ~48,000ê°œ ìˆ˜ì§‘
        - Blocked: p0_p100 (100% ë²”ìœ„) â†’ ValueError ë°œìƒ

        êµ¬í˜„ ì „ëµ:
        0. ë²”ìœ„ ê²€ì¦ (80% ì„ê³„ê°’)
        1. thought_pair_distancesì—ì„œ ìœ ì‚¬ë„ ë²”ìœ„ ì¡°íšŒ (ì¸ë±ìŠ¤ í™œìš©, ~0.05ì´ˆ, ë¬´ì œí•œ)
        2. thought_unitsì—ì„œ claim, raw_note_id JOIN (~0.05ì´ˆ)
        3. ê²°ê³¼ ì¡°í•© (Python ë©”ëª¨ë¦¬ ì—°ì‚°)

        Args:
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ [0, 1] (ì˜ˆ: 0.057)
            max_similarity: ìµœëŒ€ ìœ ì‚¬ë„ [0, 1] (ì˜ˆ: 0.093)

        Returns:
            List[dict]: [
                {
                    "thought_a_id": int,
                    "thought_b_id": int,
                    "thought_a_claim": str,
                    "thought_b_claim": str,
                    "similarity": float,
                    "raw_note_id_a": str,
                    "raw_note_id_b": str
                }
            ]

        Raises:
            ValueError: ë²”ìœ„ê°€ 80%ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
            Exception: DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        # Step 0: ë²”ìœ„ ê²€ì¦ (80% ì„ê³„ê°’)
        similarity_range = max_similarity - min_similarity
        if similarity_range > 0.8:
            error_msg = (
                f"Similarity range too wide: {similarity_range:.1%} > 80%. "
                f"Range [{min_similarity:.3f}, {max_similarity:.3f}] is likely an error. "
                f"Normal strategies use 30-40% range (e.g., p10_p40, p30_p60)."
            )
            logger.error(f"Range validation failed: {error_msg}")
            raise ValueError(error_msg)

        logger.info(
            f"Querying distance table: "
            f"range=[{min_similarity:.3f}, {max_similarity:.3f}] "
            f"({similarity_range:.1%}), no limit"
        )

        try:
            start_time = time.time()

            # Step 1: ìœ ì‚¬ë„ ë²”ìœ„ ì¡°íšŒ (í˜ì´ì§• ì²˜ë¦¬)
            # Supabase REST APIëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 1,000ê°œë§Œ ë°˜í™˜í•˜ë¯€ë¡œ í˜ì´ì§• í•„ìš”
            # ì•ˆì „ ìƒí•œì„ : 100,000ê°œ (80% ë²”ìœ„ ê²€ì¦ìœ¼ë¡œ ëŒ€ë¶€ë¶„ ì°¨ë‹¨ë¨)
            pairs = []
            page_size = 1000  # Supabase ê¸°ë³¸ limit
            max_total = 100000  # ì•ˆì „ ìƒí•œì„ 
            offset = 0

            while len(pairs) < max_total:
                page_response = await (
                    self.client.table("thought_pair_distances")
                    .select("thought_a_id, thought_b_id, similarity")
                    .gte("similarity", min_similarity)
                    .lte("similarity", max_similarity)
                    .order("similarity", desc=False)  # ë‚®ì€ ìœ ì‚¬ë„ë¶€í„°
                    .range(offset, offset + page_size - 1)  # í˜ì´ì§•
                    .execute()
                )

                page_data = page_response.data
                if not page_data:
                    # ë” ì´ìƒ ë°ì´í„° ì—†ìŒ
                    break

                pairs.extend(page_data)

                # ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ ê²½ìš° ì¢…ë£Œ
                if len(page_data) < page_size:
                    break

                offset += page_size

                # ë¡œê·¸ (2í˜ì´ì§€ ì´ìƒì¼ ë•Œë§Œ)
                if offset > page_size:
                    logger.info(f"  Fetched {len(pairs)} pairs so far (offset: {offset})...")

            step1_duration = time.time() - start_time

            if not pairs:
                logger.info(
                    f"No pairs found in similarity range "
                    f"[{min_similarity:.3f}, {max_similarity:.3f}]"
                )
                return []

            logger.info(
                f"Step 1: Found {len(pairs)} pairs in {step1_duration:.2f}s "
                f"({len(pairs)//page_size + 1} pages)"
            )

            # Step 2: thought_unitsì—ì„œ claim, raw_note_id JOIN
            step2_start = time.time()

            # ëª¨ë“  thought ID ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
            thought_ids = set()
            for p in pairs:
                thought_ids.add(p["thought_a_id"])
                thought_ids.add(p["thought_b_id"])

            # ë°°ì¹˜ ì¡°íšŒ (IN ì—°ì‚°)
            thoughts_response = await (
                self.client.table("thought_units")
                .select("id, claim, raw_note_id")
                .in_("id", list(thought_ids))
                .execute()
            )

            # thought_id â†’ {claim, raw_note_id} ë§¤í•‘
            thought_map = {
                t["id"]: {
                    "claim": t["claim"],
                    "raw_note_id": t["raw_note_id"]
                }
                for t in thoughts_response.data
            }

            step2_duration = time.time() - step2_start

            logger.info(
                f"Step 2: Retrieved {len(thought_map)} thought details in {step2_duration:.2f}s"
            )

            # Step 3: ê²°ê³¼ ì¡°í•© (Python ë©”ëª¨ë¦¬ ì—°ì‚°)
            step3_start = time.time()

            result = []
            for p in pairs:
                a_id = p["thought_a_id"]
                b_id = p["thought_b_id"]

                # thought_mapì— ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ (ë°ì´í„° ì •í•©ì„± ë¬¸ì œ)
                if a_id not in thought_map or b_id not in thought_map:
                    logger.warning(
                        f"Missing thought data: thought_a_id={a_id}, thought_b_id={b_id}"
                    )
                    continue

                result.append({
                    "thought_a_id": a_id,
                    "thought_b_id": b_id,
                    "thought_a_claim": thought_map[a_id]["claim"],
                    "thought_b_claim": thought_map[b_id]["claim"],
                    "similarity": p["similarity"],
                    "raw_note_id_a": thought_map[a_id]["raw_note_id"],
                    "raw_note_id_b": thought_map[b_id]["raw_note_id"]
                })

            step3_duration = time.time() - step3_start
            total_duration = time.time() - start_time

            logger.info(
                f"Step 3: Combined {len(result)} pairs in {step3_duration:.2f}s. "
                f"Total duration: {total_duration:.2f}s"
            )

            return result

        except Exception as e:
            logger.error(f"Failed to get candidates from distance table: {e}")
            raise


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_supabase_service: Optional[SupabaseService] = None


def get_supabase_service() -> SupabaseService:
    """
    Supabase ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜.

    FastAPI Dependsì—ì„œ ì‚¬ìš©.
    """
    global _supabase_service
    if _supabase_service is None:
        _supabase_service = SupabaseService()
    return _supabase_service
