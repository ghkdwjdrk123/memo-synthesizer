"""
Supabase ì„œë¹„ìŠ¤.

PostgreSQL + pgvectorë¥¼ ì‚¬ìš©í•œ ë°ì´í„° CRUD.
"""

import logging
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from uuid import UUID

from supabase import create_async_client, AsyncClient

from config import settings
from schemas.raw import RawNote, RawNoteCreate
from schemas.normalized import ThoughtUnitCreate
from schemas.zk import ThoughtPairCreate

logger = logging.getLogger(__name__)


class SupabaseService:
    """Supabase CRUD + ì—°ê²° í’€ë§."""

    def __init__(self):
        """
        Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”.

        HTTP ì—°ê²° í’€ë§ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”.
        """
        # Supabase async í´ë¼ì´ì–¸íŠ¸
        self.client: AsyncClient = None
        self._initialized = False

    async def _ensure_initialized(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìµœì´ˆ í˜¸ì¶œ ì‹œ)"""
        if not self._initialized:
            self.client = await create_async_client(
                settings.supabase_url, settings.supabase_key
            )
            self._initialized = True

    async def close(self):
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ."""
        # Supabase ë‚´ì¥ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì‹œ ë³„ë„ ì¢…ë£Œ ë¶ˆí•„ìš”
        pass

    # ============================================================
    # RAW Notes CRUD
    # ============================================================

    async def upsert_raw_note(self, note: RawNoteCreate) -> dict:
        """
        RAW note ì €ì¥ (notion_page_id ê¸°ì¤€ upsert).

        Args:
            note: ì €ì¥í•  RAW note

        Returns:
            ì €ì¥ëœ note ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (datetime â†’ ISO string)
            data = note.model_dump(mode='json')

            # Supabase upsert (conflict on notion_page_id)
            response = await (
                self.client.table("raw_notes")
                .upsert(data, on_conflict="notion_page_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Raw note upserted: {note.notion_page_id} (title: {note.title[:50] if note.title else 'N/A'}, content: {len(note.content) if note.content else 0} chars)"
                )
                return response.data[0]
            else:
                raise Exception("Upsert returned no data")

        except Exception as e:
            logger.error(f"Failed to upsert raw note {note.notion_page_id}: {e}")
            raise

    async def get_raw_note_ids(self) -> List[str]:
        """
        ëª¨ë“  í™œì„± RAW noteì˜ ID ëª©ë¡ ì¡°íšŒ (ë©”ëª¨ë¦¬ ì ˆì•½).

        Returns:
            UUID ëª©ë¡ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("id")
                .eq("is_deleted", False)
                .execute()
            )

            ids = [row["id"] for row in response.data]
            logger.info(f"Retrieved {len(ids)} active raw note IDs")
            return ids

        except Exception as e:
            logger.error(f"Failed to get raw note IDs: {e}")
            raise

    async def get_raw_notes_by_ids(self, note_ids: List[str]) -> List[dict]:
        """
        ID ëª©ë¡ìœ¼ë¡œ í™œì„± RAW notes ì¡°íšŒ (ë°°ì¹˜ë³„ full content ë¡œë“œ).

        Args:
            note_ids: UUID ëª©ë¡

        Returns:
            RAW note ë°ì´í„° ëª©ë¡ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("*")
                .in_("id", note_ids)
                .eq("is_deleted", False)
                .execute()
            )

            logger.info(f"Retrieved {len(response.data)} active raw notes by IDs")
            return response.data

        except Exception as e:
            logger.error(f"Failed to get raw notes by IDs: {e}")
            raise

    async def get_raw_note_count(self) -> int:
        """í™œì„± RAW notes ì´ ê°œìˆ˜ ì¡°íšŒ (ì‚­ì œëœ í˜ì´ì§€ ì œì™¸)."""
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("raw_notes")
                .select("id", count="exact")
                .eq("is_deleted", False)
                .execute()
            )

            count = response.count if response.count else 0
            logger.info(f"Total active raw notes: {count}")
            return count

        except Exception as e:
            logger.error(f"Failed to get raw note count: {e}")
            return 0

    # ============================================================
    # Thought Units CRUD
    # ============================================================

    async def insert_thought_unit(self, thought: ThoughtUnitCreate) -> dict:
        """
        Thought unit í•œ ê°œ ì €ì¥.

        Args:
            thought: ì €ì¥í•  ì‚¬ê³  ë‹¨ìœ„ (ì„ë² ë”© í¬í•¨)

        Returns:
            ì €ì¥ëœ thought unit ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            data = thought.model_dump(mode='json')

            # pgvectorëŠ” list[float]ë¥¼ ìë™ìœ¼ë¡œ vector íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            response = await (
                self.client.table("thought_units")
                .insert(data)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Thought unit inserted: ID={response.data[0]['id']}, "
                    f"raw_note_id={thought.raw_note_id}, "
                    f"claim={thought.claim[:50]}..."
                )
                return response.data[0]
            else:
                raise Exception("Insert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to insert thought unit for raw_note {thought.raw_note_id}: {e}"
            )
            raise

    async def insert_thought_units_batch(
        self, thoughts: List[ThoughtUnitCreate]
    ) -> List[dict]:
        """
        ì—¬ëŸ¬ thought units ë°°ì¹˜ ì €ì¥.

        Args:
            thoughts: ì €ì¥í•  ì‚¬ê³  ë‹¨ìœ„ ëª©ë¡

        Returns:
            ì €ì¥ëœ thought units ë°ì´í„° ëª©ë¡

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        if not thoughts:
            logger.warning("Batch insert called with empty list")
            return []

        try:
            # ëª¨ë“  thoughtë¥¼ JSON ì§ë ¬í™”
            data = [thought.model_dump(mode='json') for thought in thoughts]

            # ë°°ì¹˜ insert
            response = await (
                self.client.table("thought_units")
                .insert(data)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Batch inserted {len(response.data)} thought units "
                    f"(raw_note_ids: {set(t.raw_note_id for t in thoughts)})"
                )
                return response.data
            else:
                raise Exception("Batch insert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to batch insert {len(thoughts)} thought units: {e}"
            )
            raise

    async def get_thought_units_by_raw_note(self, raw_note_id: str) -> List[dict]:
        """
        íŠ¹ì • raw_noteì˜ ëª¨ë“  thought units ì¡°íšŒ.

        Args:
            raw_note_id: ì›ë³¸ ë©”ëª¨ UUID

        Returns:
            Thought units ëª©ë¡
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_units")
                .select("*")
                .eq("raw_note_id", raw_note_id)
                .order("id")
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} thought units for raw_note {raw_note_id}"
            )
            return response.data

        except Exception as e:
            logger.error(
                f"Failed to get thought units for raw_note {raw_note_id}: {e}"
            )
            raise

    async def get_all_thought_units_with_embeddings(self) -> List[dict]:
        """
        ì„ë² ë”©ì´ ìˆëŠ” ëª¨ë“  thought units ì¡°íšŒ (ìœ ì‚¬ë„ ê²€ìƒ‰ìš©).

        Returns:
            Thought units ëª©ë¡ (ì„ë² ë”© í¬í•¨)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_units")
                .select("id, raw_note_id, claim, context, embedding, embedding_model, extracted_at")
                .not_.is_("embedding", "null")
                .order("id")
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} thought units with embeddings"
            )
            return response.data

        except Exception as e:
            logger.error(f"Failed to get thought units with embeddings: {e}")
            raise

    # ============================================================
    # Thought Pairs CRUD (Step 3: ZK ë ˆì´ì–´)
    # ============================================================

    async def find_candidate_pairs(
        self,
        min_similarity: float = 0.05,
        max_similarity: float = 0.35,
        limit: int = 20
    ) -> List[dict]:
        """
        Stored Procedureë¥¼ í˜¸ì¶œí•˜ì—¬ í›„ë³´ í˜ì–´ ì¡°íšŒ (ë‚®ì€ ìœ ì‚¬ë„ = ì•½í•œ ì—°ê²°).

        Args:
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ (ê¸°ë³¸ 0.05, ë‚®ì€ ìœ ì‚¬ë„ = ì„œë¡œ ë‹¤ë¥¸ ì•„ì´ë””ì–´)
            max_similarity: ìµœëŒ€ ìœ ì‚¬ë„ (ê¸°ë³¸ 0.35, ì•½í•œ ì—°ê²° = ì°½ì˜ì  í™•ì¥ ê°€ëŠ¥)
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 20)

        Returns:
            í›„ë³´ ìŒ ëª©ë¡ (thought_a_id, thought_b_id, similarity_score, thought_a_claim, thought_b_claim)

        Raises:
            Exception: Stored Procedure í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await self.client.rpc(
                "find_similar_pairs",
                {
                    "min_sim": min_similarity,
                    "max_sim": max_similarity,
                    "lim": limit
                }
            ).execute()

            logger.info(
                f"Found {len(response.data)} candidate pairs with weak connections "
                f"(similarity: {min_similarity:.2f}-{max_similarity:.2f})"
            )
            return response.data

        except Exception as e:
            error_msg = str(e)
            if "function" in error_msg.lower() and "does not exist" in error_msg.lower():
                logger.error(
                    "Stored procedure 'find_similar_pairs' not found. "
                    "Please run docs/supabase_setup.sql first"
                )
                raise Exception(
                    "Stored procedure 'find_similar_pairs' not found. "
                    "Please run docs/supabase_setup.sql first"
                )
            logger.error(f"Failed to find candidate pairs: {e}")
            raise

    async def insert_thought_pair(self, pair: ThoughtPairCreate) -> dict:
        """
        Thought pair í•œ ê°œ ì €ì¥ (UPSERT).

        Args:
            pair: ì €ì¥í•  í˜ì–´ ë°ì´í„°

        Returns:
            ì €ì¥ëœ thought pair ë°ì´í„°

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            data = pair.model_dump(mode='json')

            # UPSERT (thought_a_id, thought_b_id ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€)
            # is_used_in_essayëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
            response = await (
                self.client.table("thought_pairs")
                .upsert(data, on_conflict="thought_a_id,thought_b_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Thought pair upserted: ID={response.data[0]['id']}, "
                    f"thoughts=({pair.thought_a_id}, {pair.thought_b_id}), "
                    f"similarity={pair.similarity_score:.3f}"
                )
                return response.data[0]
            else:
                raise Exception("Upsert returned no data")

        except Exception as e:
            logger.error(
                f"Failed to upsert thought pair ({pair.thought_a_id}, {pair.thought_b_id}): {e}"
            )
            raise

    async def insert_thought_pairs_batch(
        self, pairs: List[ThoughtPairCreate]
    ) -> List[dict]:
        """
        ì—¬ëŸ¬ thought pairs ë°°ì¹˜ ì €ì¥ (UPSERT).

        Args:
            pairs: ì €ì¥í•  í˜ì–´ ëª©ë¡

        Returns:
            ì €ì¥ëœ thought pairs ë°ì´í„° ëª©ë¡

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        if not pairs:
            logger.warning("Batch insert called with empty pairs list")
            return []

        try:
            # ëª¨ë“  pairë¥¼ JSON ì§ë ¬í™”
            data = [pair.model_dump(mode='json') for pair in pairs]

            # ë°°ì¹˜ UPSERT
            response = await (
                self.client.table("thought_pairs")
                .upsert(data, on_conflict="thought_a_id,thought_b_id")
                .execute()
            )

            if response.data:
                logger.info(
                    f"Batch upserted {len(response.data)} thought pairs "
                    f"(avg similarity: {sum(p.similarity_score for p in pairs) / len(pairs):.3f})"
                )
                return response.data
            else:
                raise Exception("Batch upsert returned no data")

        except Exception as e:
            logger.error(f"Failed to batch upsert {len(pairs)} thought pairs: {e}")
            raise

    async def get_unused_thought_pairs(self, limit: int = 10) -> List[dict]:
        """
        ë¯¸ì‚¬ìš© thought pairs ì¡°íšŒ (ì—ì„¸ì´ ìƒì„±ìš©).

        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 10)

        Returns:
            ë¯¸ì‚¬ìš© thought pairs ëª©ë¡ (similarity_score ASC ì •ë ¬ - ë‚®ì€ ìœ ì‚¬ë„ë¶€í„°)
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_pairs")
                .select("*")
                .eq("is_used_in_essay", False)
                .order("similarity_score", desc=False)  # ë‚®ì€ ìœ ì‚¬ë„ë¶€í„° ì„ íƒ (ì°½ì˜ì  ì¡°í•©)
                .limit(limit)
                .execute()
            )

            logger.info(
                f"Retrieved {len(response.data)} unused thought pairs (limit: {limit})"
            )
            return response.data

        except Exception as e:
            logger.error(f"Failed to get unused thought pairs: {e}")
            raise

    async def update_pair_used_status(
        self, pair_id: int, is_used: bool = True
    ) -> dict:
        """
        Thought pair ì‚¬ìš© ìƒíƒœ ì—…ë°ì´íŠ¸.

        Args:
            pair_id: í˜ì–´ ID
            is_used: ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)

        Returns:
            ì—…ë°ì´íŠ¸ëœ thought pair ë°ì´í„°

        Raises:
            Exception: DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await (
                self.client.table("thought_pairs")
                .update({"is_used_in_essay": is_used})
                .eq("id", pair_id)
                .execute()
            )

            if response.data:
                logger.info(
                    f"Updated thought pair {pair_id} used status: {is_used}"
                )
                return response.data[0]
            else:
                raise Exception(f"Thought pair {pair_id} not found")

        except Exception as e:
            logger.error(f"Failed to update pair {pair_id} used status: {e}")
            raise

    async def get_pair_with_thoughts(self, pair_id: int) -> dict:
        """
        Thought pair + ì‚¬ê³  ë‹¨ìœ„ + ì›ë³¸ ë©”ëª¨ ì •ë³´ JOIN ì¡°íšŒ.

        Args:
            pair_id: í˜ì–´ ID

        Returns:
            í˜ì–´ ì •ë³´ + ì–‘ìª½ thoughtì˜ claim/context + ì›ë³¸ ë©”ëª¨ title/url

        Raises:
            Exception: DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # Step 1: í˜ì–´ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
            pair_response = await (
                self.client.table("thought_pairs")
                .select("*")
                .eq("id", pair_id)
                .single()
                .execute()
            )

            if not pair_response.data:
                raise Exception(f"Thought pair {pair_id} not found")

            pair_data = pair_response.data

            # Step 2: ë‘ ê°œì˜ thought units ì¡°íšŒ
            thought_a_response = await (
                self.client.table("thought_units")
                .select("id, claim, context, raw_note_id")
                .eq("id", pair_data["thought_a_id"])
                .single()
                .execute()
            )

            thought_b_response = await (
                self.client.table("thought_units")
                .select("id, claim, context, raw_note_id")
                .eq("id", pair_data["thought_b_id"])
                .single()
                .execute()
            )

            thought_a = thought_a_response.data
            thought_b = thought_b_response.data

            # Step 3: ë‘ ê°œì˜ raw notes ì¡°íšŒ
            raw_note_a_response = await (
                self.client.table("raw_notes")
                .select("id, title, notion_url")
                .eq("id", thought_a["raw_note_id"])
                .single()
                .execute()
            )

            raw_note_b_response = await (
                self.client.table("raw_notes")
                .select("id, title, notion_url")
                .eq("id", thought_b["raw_note_id"])
                .single()
                .execute()
            )

            raw_note_a = raw_note_a_response.data
            raw_note_b = raw_note_b_response.data

            # Step 4: ê²°ê³¼ ì¡°í•©
            result = {
                "pair_id": pair_data["id"],
                "similarity_score": pair_data["similarity_score"],
                "connection_reason": pair_data["connection_reason"],
                "is_used_in_essay": pair_data["is_used_in_essay"],
                "selected_at": pair_data["selected_at"],
                "thought_a": {
                    "id": thought_a["id"],
                    "claim": thought_a["claim"],
                    "context": thought_a["context"],
                    "source_title": raw_note_a["title"],
                    "source_url": raw_note_a["notion_url"]
                },
                "thought_b": {
                    "id": thought_b["id"],
                    "claim": thought_b["claim"],
                    "context": thought_b["context"],
                    "source_title": raw_note_b["title"],
                    "source_url": raw_note_b["notion_url"]
                }
            }

            logger.info(
                f"Retrieved pair {pair_id} with full thought details "
                f"(thoughts: {thought_a['id']}, {thought_b['id']})"
            )
            return result

        except Exception as e:
            logger.error(f"Failed to get pair {pair_id} with thoughts: {e}")
            raise

    # ========================
    # Essay CRUD Methods (Step 4)
    # ========================

    async def insert_essay(self, essay: "EssayCreate") -> dict:
        """
        essays í…Œì´ë¸”ì— ë‹¨ì¼ ì—ì„¸ì´ ì €ì¥.

        Args:
            essay: EssayCreate ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            {
                "id": int,
                "type": str,
                "title": str,
                "outline": list[str],
                "used_thoughts_json": list[dict],
                "reason": str,
                "pair_id": int,
                "generated_at": str (ISO format)
            }

        Raises:
            Exception: DB ì €ì¥ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            # JSONB í•„ë“œ ì§ë ¬í™”
            essay_dict = {
                "type": essay.type,
                "title": essay.title,
                "outline": essay.outline,  # list â†’ JSONB (ìë™)
                "used_thoughts_json": [t.model_dump() for t in essay.used_thoughts],  # JSONB
                "reason": essay.reason,
                "pair_id": essay.pair_id
            }

            response = await self.client.table("essays")\
                .insert(essay_dict)\
                .execute()

            inserted = response.data[0]
            logger.info(f"Inserted essay ID {inserted['id']} for pair {essay.pair_id}")
            return inserted

        except Exception as e:
            logger.error(f"Failed to insert essay: {e}")
            logger.error(f"Essay data: {essay_dict}")
            raise

    async def insert_essays_batch(self, essays: List["EssayCreate"]) -> List[dict]:
        """
        ì—¬ëŸ¬ ì—ì„¸ì´ ë°°ì¹˜ ì €ì¥.

        Args:
            essays: EssayCreate ëª¨ë¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ëœ ì—ì„¸ì´ ë¦¬ìŠ¤íŠ¸

        Note:
            - UPSERTëŠ” í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€ëŠ” pair_id ì™¸ë˜í‚¤ë¡œ ë³´ì¥)
            - ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¡¤ë°±
        """
        await self._ensure_initialized()

        if not essays:
            return []

        try:
            essays_dict = [
                {
                    "type": e.type,
                    "title": e.title,
                    "outline": e.outline,
                    "used_thoughts_json": [t.model_dump() for t in e.used_thoughts],
                    "reason": e.reason,
                    "pair_id": e.pair_id
                }
                for e in essays
            ]

            response = await self.client.table("essays")\
                .insert(essays_dict)\
                .execute()

            inserted = response.data
            logger.info(f"Batch inserted {len(inserted)} essays")
            return inserted

        except Exception as e:
            logger.error(f"Failed to batch insert essays: {e}")
            raise

    async def get_essays(
        self,
        limit: int = 10,
        offset: int = 0
    ) -> List[dict]:
        """
        essays í…Œì´ë¸” ì¡°íšŒ (ìµœì‹ ìˆœ).

        Args:
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜ (ê¸°ë³¸ 10)
            offset: ê±´ë„ˆë›¸ ê°œìˆ˜ (í˜ì´ì§€ë„¤ì´ì…˜)

        Returns:
            ì—ì„¸ì´ ë¦¬ìŠ¤íŠ¸ (JSONB í•„ë“œ ìë™ íŒŒì‹±ë¨)
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("essays")\
                .select("*")\
                .order("generated_at", desc=True)\
                .limit(limit)\
                .offset(offset)\
                .execute()

            essays = response.data
            logger.info(f"Retrieved {len(essays)} essays")
            return essays

        except Exception as e:
            logger.error(f"Failed to get essays: {e}")
            raise

    async def get_essay_by_id(self, essay_id: int) -> dict:
        """
        ë‹¨ì¼ ì—ì„¸ì´ ì¡°íšŒ.

        Args:
            essay_id: ì—ì„¸ì´ ID

        Returns:
            ì—ì„¸ì´ ë°ì´í„°

        Raises:
            Exception: ì—ì„¸ì´ê°€ ì—†ê±°ë‚˜ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("essays")\
                .select("*")\
                .eq("id", essay_id)\
                .single()\
                .execute()

            essay = response.data
            logger.info(f"Retrieved essay ID {essay_id}")
            return essay

        except Exception as e:
            logger.error(f"Failed to get essay {essay_id}: {e}")
            raise

    # ============================================================
    # Import Jobs CRUD (Background Task Tracking)
    # ============================================================

    async def create_import_job(self, job: "ImportJobCreate") -> dict:
        """
        Create new import job record.

        Args:
            job: Import job creation data

        Returns:
            dict: Created job record with UUID

        Raises:
            Exception: Job creation failed
        """
        await self._ensure_initialized()

        try:
            data = {
                "status": "pending",
                "mode": job.mode,
                "config_json": job.config_json
            }
            response = await self.client.table("import_jobs").insert(data).execute()
            created = response.data[0]
            logger.info(f"Created import job {created['id']} (mode: {job.mode})")
            return created
        except Exception as e:
            logger.error(f"Failed to create import job: {e}")
            raise

    async def update_import_job(self, job_id: str, updates: "ImportJobUpdate") -> dict:
        """
        Update import job progress.

        Args:
            job_id: UUID of import job
            updates: Fields to update

        Returns:
            dict: Updated job record

        Raises:
            Exception: Job not found or update failed
        """
        await self._ensure_initialized()

        try:
            data = updates.model_dump(exclude_none=True, mode='json')
            response = await self.client.table("import_jobs")\
                .update(data).eq("id", job_id).execute()

            if not response.data:
                raise Exception(f"Import job {job_id} not found")

            return response.data[0]
        except Exception as e:
            logger.error(f"Failed to update import job {job_id}: {e}")
            raise

    async def get_import_job(self, job_id: str) -> dict:
        """
        Retrieve import job by ID.

        Args:
            job_id: UUID of import job

        Returns:
            dict: Job record

        Raises:
            Exception: Job not found
        """
        await self._ensure_initialized()

        try:
            response = await self.client.table("import_jobs")\
                .select("*").eq("id", job_id).single().execute()

            if not response.data:
                raise Exception(f"Import job {job_id} not found")

            return response.data
        except Exception as e:
            logger.error(f"Failed to get import job {job_id}: {e}")
            raise

    async def get_pages_to_fetch(
        self,
        notion_pages: List[Dict[str, Any]]
    ) -> tuple[List[str], List[str], List[str]]:
        """
        Compare Notion pages with DB using server-side RPC.

        Uses PostgreSQL function for efficient change detection.
        Falls back to full table scan if RPC fails.

        Args:
            notion_pages: List of page metadata from Notion API
                Each page must have: id, last_edited_time

        Returns:
            Tuple of (new_page_ids, updated_page_ids, deleted_page_ids)

        Performance:
            - RPC mode: ~150ms (constant time, scales to 100k pages)
            - Fallback mode: ~110ms (current size)
            - Network: Only changed pages (0.5KB vs 60KB)

        Example:
            >>> pages = [{"id": "abc", "last_edited_time": "2024-01-15T14:30:00.000Z"}]
            >>> new, updated, deleted = await service.get_pages_to_fetch(pages)
            >>> print(f"New: {len(new)}, Updated: {len(updated)}, Deleted: {len(deleted)}")
        """
        await self._ensure_initialized()

        # Prepare data for RPC
        pages_json = []
        force_new_ids = []  # Pages with invalid timestamps â†’ treat as new

        for p in notion_pages:
            page_id = p.get("id")
            last_edited = p.get("last_edited_time")

            if not page_id:
                logger.warning("Page missing 'id' field, skipping")
                continue

            if not last_edited:
                logger.warning(f"Page {page_id} missing 'last_edited_time', treating as new")
                force_new_ids.append(page_id)
                continue

            try:
                # Parse ISO 8601 timestamp
                notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))

                # Truncate to seconds (match SQL function behavior)
                notion_time = notion_time.replace(microsecond=0)

                pages_json.append({
                    "id": page_id,
                    "last_edited": notion_time.isoformat()
                })
            except (ValueError, AttributeError, TypeError) as e:
                logger.warning(f"Invalid timestamp for {page_id}: {e}, treating as new")
                force_new_ids.append(page_id)

        if not pages_json and not force_new_ids:
            logger.warning("No valid pages to check")
            return [], []

        logger.info(f"Change detection: checking {len(pages_json)} pages via RPC (sample: {[p['id'] for p in pages_json[:3]]})")

        # Try RPC change detection (Solution 3)
        try:
            import time
            start_time = time.time()

            response = await self.client.rpc('get_changed_pages', {
                'pages_data': pages_json
            }).execute()

            elapsed = time.time() - start_time

            # Validate response structure
            if not response.data or not isinstance(response.data, dict):
                raise ValueError("Invalid RPC response format: expected dict")

            result = response.data

            # Check for SQL function error
            if 'error' in result:
                raise ValueError(f"SQL function error: {result['error']} (SQLSTATE: {result.get('error_detail', 'unknown')})")

            # Extract results
            new_page_ids = result.get('new_page_ids', [])
            updated_page_ids = result.get('updated_page_ids', [])
            deleted_page_ids = result.get('deleted_page_ids', [])

            # Validate types
            if not isinstance(new_page_ids, list):
                raise ValueError(f"Invalid type for new_page_ids: {type(new_page_ids)}")
            if not isinstance(updated_page_ids, list):
                raise ValueError(f"Invalid type for updated_page_ids: {type(updated_page_ids)}")
            if not isinstance(deleted_page_ids, list):
                raise ValueError(f"Invalid type for deleted_page_ids: {type(deleted_page_ids)}")

            # Add force_new pages
            new_page_ids.extend(force_new_ids)

            # Validate UUIDs
            import re
            UUID_PATTERN = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)

            for page_id in new_page_ids + updated_page_ids + deleted_page_ids:
                if not UUID_PATTERN.match(page_id):
                    raise ValueError(f"Invalid UUID format: {page_id}")

            logger.info(
                f"âœ… RPC change detection completed in {elapsed:.2f}s: "
                f"{len(new_page_ids)} new, {len(updated_page_ids)} updated, "
                f"{len(deleted_page_ids)} deleted, "
                f"{result.get('unchanged_count', len(pages_json) - len(new_page_ids) - len(updated_page_ids))} unchanged"
            )

            return new_page_ids, updated_page_ids, deleted_page_ids

        except Exception as rpc_error:
            logger.error(f"âŒ RPC change detection failed: {rpc_error}, falling back to full table scan")

            # Fallback: Full table scan (ë°©ì‹ A)
            try:
                logger.info("Using fallback: full table scan")

                response = await (
                    self.client.table("raw_notes")
                    .select("notion_page_id, notion_last_edited_time")
                    .execute()
                )

                # Build existing_map from DB
                existing_map = {}
                for row in response.data:
                    db_page_id = row["notion_page_id"]
                    db_time = row["notion_last_edited_time"]

                    # Parse timestamp
                    if isinstance(db_time, str):
                        db_time = datetime.fromisoformat(db_time.replace("Z", "+00:00"))

                    # Ensure timezone-aware
                    if db_time.tzinfo is None:
                        db_time = db_time.replace(tzinfo=timezone.utc)

                    # Truncate to seconds
                    db_time = db_time.replace(microsecond=0)
                    existing_map[db_page_id] = db_time

                # Build page_map from Notion pages
                page_map = {}
                for p_json in pages_json:
                    page_id = p_json["id"]
                    notion_time = datetime.fromisoformat(p_json["last_edited"])
                    page_map[page_id] = notion_time

                # Compare
                new_ids = []
                updated_ids = []

                for page_id, notion_time in page_map.items():
                    if page_id not in existing_map:
                        new_ids.append(page_id)
                    elif notion_time > existing_map[page_id]:
                        updated_ids.append(page_id)

                # Add force_new pages
                new_ids.extend(force_new_ids)

                # Detect deleted pages (in DB but not in Notion)
                all_notion_ids = set(page_map.keys())
                deleted_ids = [
                    db_id for db_id in existing_map.keys()
                    if db_id not in all_notion_ids
                ]

                logger.info(
                    f"âœ… Fallback completed: {len(new_ids)} new, {len(updated_ids)} updated, "
                    f"{len(deleted_ids)} deleted, "
                    f"{len(page_map) - len(new_ids) - len(updated_ids)} unchanged"
                )

                return new_ids, updated_ids, deleted_ids

            except Exception as fallback_error:
                logger.error(f"âŒ Fallback also failed: {fallback_error}, treating all as new (last resort)")

                # Last resort: treat all as new
                all_ids = [p["id"] for p in pages_json] + force_new_ids
                return all_ids, [], []

    async def validate_rpc_function_exists(self) -> bool:
        """
        Check if RPC function is deployed in Supabase.

        Returns:
            bool: True if function exists and works, False otherwise
        """
        try:
            await self._ensure_initialized()

            # Test with empty array
            response = await self.client.rpc('get_changed_pages', {
                'pages_data': []
            }).execute()

            # Validate response
            if not response.data or not isinstance(response.data, dict):
                logger.warning("âš ï¸  RPC function returned unexpected format")
                return False

            logger.info("âœ… RPC function 'get_changed_pages' is available and working")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  RPC function 'get_changed_pages' not available: {e}")
            logger.warning("   Import will use fallback mode (full table scan)")
            return False

    async def soft_delete_raw_note(self, notion_page_id: str) -> None:
        """
        Mark a page as deleted without removing from DB (soft delete).

        This preserves the page and all downstream data (thought_units, essays)
        while marking it as deleted in Notion.

        Args:
            notion_page_id: Notion page ID to soft delete

        Raises:
            No exceptions - failures are logged only
        """
        await self._ensure_initialized()

        try:
            await self.client.table("raw_notes").update({
                "is_deleted": True,
                "deleted_at": datetime.now(timezone.utc).isoformat()
            }).eq("notion_page_id", notion_page_id).execute()

            logger.warning(f"ğŸ—‘ï¸  Soft deleted page: {notion_page_id} (essays preserved)")

        except Exception as e:
            logger.error(f"Failed to soft delete page {notion_page_id}: {e}")

    async def increment_job_progress(
        self,
        job_id: str,
        imported: bool = False,
        skipped: bool = False,
        failed_page: Optional[Dict[str, str]] = None
    ) -> None:
        """
        Atomically increment job progress counters.

        This method NEVER raises exceptions - failures are logged only.
        This ensures import continues even if progress tracking fails.

        Args:
            job_id: UUID of import job
            imported: True if page was successfully imported
            skipped: True if page was skipped
            failed_page: Dict with page_id and error_message if page failed
        """
        await self._ensure_initialized()

        try:
            job = await self.get_import_job(job_id)
            updates = {"processed_pages": job["processed_pages"] + 1}

            if imported:
                updates["imported_pages"] = job["imported_pages"] + 1
            if skipped:
                updates["skipped_pages"] = job["skipped_pages"] + 1
            if failed_page:
                current_failed = job.get("failed_pages", [])
                current_failed.append(failed_page)
                updates["failed_pages"] = current_failed

            await self.client.table("import_jobs").update(updates).eq("id", job_id).execute()
        except Exception as e:
            # âœ… CRITICAL: Don't raise - just log
            # Import continues even if progress tracking fails
            logger.error(f"Failed to increment job {job_id} progress: {e}")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_supabase_service: Optional[SupabaseService] = None


def get_supabase_service() -> SupabaseService:
    """
    Supabase ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜.

    FastAPI Dependsì—ì„œ ì‚¬ìš©.
    """
    global _supabase_service
    if _supabase_service is None:
        _supabase_service = SupabaseService()
    return _supabase_service
