# ë‹¤ìŒ ìŠ¤í…: MVP í™•ì¥ ê³„íš

## í˜„ì¬ ìƒíƒœ (MVP ì™„ì„±)

âœ… **ì™„ë£Œëœ í•­ëª©:**
1. ë°±ì—”ë“œ íŒŒì´í”„ë¼ì¸ 4ë‹¨ê³„ (RAW â†’ NORMALIZED â†’ ZK â†’ Essay)
2. í”„ë¡ íŠ¸ì—”ë“œ ë¸ŒëŸ°ì¹˜ ìŠ¤íƒ€ì¼ ë·°ì–´ (ëª©ë¡ + ìƒì„¸)
3. ìœ ì‚¬ë„ ë²„ê·¸ ìˆ˜ì • (ë‚®ì€ ìœ ì‚¬ë„ ìš°ì„  ì„ íƒ)
4. í…ŒìŠ¤íŠ¸ 44ê°œ í†µê³¼

## ì œì•ˆëœ í™•ì¥ ë°©í–¥ (2ê°€ì§€)

### ì˜µì…˜ 1: Notion ëŒ€ëŸ‰ í˜ì´ì§€ ìˆ˜ì§‘ í™•ì¥
**ëª©í‘œ:** ìˆ˜ë°± ê°œì˜ í•˜ìœ„ í˜ì´ì§€ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì™€ raw_notes í…Œì´ë¸”ì— ì €ì¥

### ì˜µì…˜ 2: ë‹¤ì¤‘ Pair ZK ì•Œê³ ë¦¬ì¦˜
**ëª©í‘œ:** í˜„ì¬ 2-pair ê¸°ë°˜ â†’ N-pair (3ê°œ ì´ìƒ thought ì¡°í•©)ë¡œ í™•ì¥

---

## ì˜µì…˜ 1: Notion ëŒ€ëŸ‰ í˜ì´ì§€ ìˆ˜ì§‘ (Bulk Import)

### í˜„ì¬ ìƒíƒœ ìƒì„¸ ë¶„ì„

#### ì½”ë“œ ë ˆë²¨ ì œì•½ ì‚¬í•­ (ë¼ì¸ë³„ ë¶„ì„)

**1. `backend/services/notion_service.py`**

**í˜„ì¬ êµ¬í˜„ (Lines 42-84: `query_database()`):**
```python
Line 53-56: response = self.client.databases.query(
               database_id=self.database_id,
               page_size=page_size
            )  # ë‹¨ 1íšŒ í˜¸ì¶œ, cursor íŒŒë¼ë¯¸í„° ì—†ìŒ

Line 76:    "has_more": response.get("has_more", False)  # ìº¡ì²˜ë§Œ í•˜ê³  ì‚¬ìš© ì•ˆ í•¨
Line 77:    return {...}  # ì—¬ê¸°ì„œ ì¢…ë£Œ, ë£¨í”„ ì—†ìŒ
```

**ë¬¸ì œì :**
- `has_more=True`ì—¬ë„ ì¶”ê°€ í˜ì´ì§€ ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ
- `next_cursor` í•„ë“œë¥¼ ì•„ì˜ˆ ìº¡ì²˜í•˜ì§€ ì•ŠìŒ
- `start_cursor` íŒŒë¼ë¯¸í„°ë¥¼ query()ì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ

**2. `backend/routers/pipeline.py`**

**í˜„ì¬ import ë¡œì§ (Lines 26-126: `import_from_notion()`):**
```python
Line 28:  page_size: int = Query(default=100, le=100)  # 100ê°œ í•˜ë“œ ë¦¬ë¯¸íŠ¸
Line 54:  pages_data = await notion_service.query_database(page_size=page_size)
Line 70-99: for page in pages_data.get("pages", []):
    Line 91:    content=None  # í•˜ë“œì½”ë”©, ë¸”ë¡ ì½˜í…ì¸  ë¯¸ìˆ˜ì§‘
    Line 102:   await supabase_service.upsert_raw_note(raw_note)
                # í˜ì´ì§€ë‹¹ 1ë²ˆ DB í˜¸ì¶œ (ë°°ì¹˜ ì—†ìŒ)
```

**ë¬¸ì œì :**
- 100ê°œ ì œí•œ (Line 28)
- `content` í•„ë“œê°€ í•­ìƒ `None` (Line 91)
- í˜ì´ì§€ë³„ ìˆœì°¨ upsert (Në²ˆ DB í˜¸ì¶œ)
- `blocks.children.list()` API í˜¸ì¶œ ì—†ìŒ

**3. Rate Limiting ë¯¸êµ¬í˜„**

**í˜„ì¬ ìƒíƒœ:**
- `backend/config.py`ì— `RATE_LIMIT_NOTION` ì„¤ì • ì—†ìŒ
- `rate_limiter.py` ì¡´ì¬í•˜ì§€ ì•ŠìŒ
- Notion APIëŠ” 3 req/sec ì œí•œ ìˆìœ¼ë‚˜ ì½”ë“œì— throttling ë¡œì§ ì—†ìŒ
- 429 ì—ëŸ¬ ë°œìƒ ì‹œ exponential backoff ì—†ìŒ

**4. ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ ë¯¸êµ¬í˜„**

**í•„ìš”í•œ API:**
```python
# í˜„ì¬ ì½”ë“œì— ì—†ìŒ
client.blocks.children.list(page_id, page_size=100)
```

**Notion ë¸”ë¡ êµ¬ì¡°:**
```json
{
  "results": [
    {"type": "paragraph", "paragraph": {"rich_text": [...]}},
    {"type": "heading_1", "heading_1": {"rich_text": [...]}},
    {"type": "bulleted_list_item", ...}
  ],
  "has_more": true,
  "next_cursor": "..."
}
```

**ì²˜ë¦¬ í•„ìš” ë¸”ë¡ íƒ€ì…:**
- paragraph, heading_1/2/3, bulleted_list_item, numbered_list_item
- quote, callout, toggle (ì ‘íŒ ì½˜í…ì¸ )
- code (ì½”ë“œ ë¸”ë¡), equation (ìˆ˜ì‹)

**5. ì¬ê·€ íƒìƒ‰ ë¯¸êµ¬í˜„**

**í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤:**
- Database â†’ Page â†’ Child Database â†’ Child Pages
- Page â†’ Child Page â†’ Grandchild Page

**í˜„ì¬ ìƒíƒœ:**
- ë‹¨ì¼ depthë§Œ ì§€ì› (root databaseì˜ ì§ì ‘ childrenë§Œ)
- `child_page`, `child_database` ë¸”ë¡ íƒ€ì… ì²˜ë¦¬ ì—†ìŒ

### êµ¬í˜„ ê³„íš (Phaseë³„ ìƒì„¸ ì„¤ê³„)

---

## Phase 1: í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„ êµ¬í˜„ (í•„ìˆ˜, 2-3ì‹œê°„)

### ëª©í‘œ
- 100ê°œ ì œí•œ ì œê±°
- `has_more=True`ì¼ ë•Œ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
- ìˆ˜ë°±~ìˆ˜ì²œ ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ê°€ëŠ¥

### êµ¬í˜„ ìƒì„¸

#### 1-1. `backend/services/notion_service.py` ìˆ˜ì •

**í˜„ì¬ ì½”ë“œ (Lines 42-84):**
```python
async def query_database(self, page_size: int = 10) -> dict:
    response = self.client.databases.query(
        database_id=self.database_id,
        page_size=page_size
    )
    # ... ë‹¨ì¼ ë°°ì¹˜ë§Œ ì²˜ë¦¬
    return {"pages": pages, "has_more": response.get("has_more", False)}
```

**ìƒˆë¡œìš´ êµ¬í˜„:**
```python
async def fetch_all_database_pages(
    self,
    database_id: str | None = None,
    page_size: int = 100
) -> list[dict]:
    """
    ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°

    Args:
        database_id: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ID (Noneì´ë©´ self.database_id ì‚¬ìš©)
        page_size: ë°°ì¹˜ë‹¹ í˜ì´ì§€ ìˆ˜ (ìµœëŒ€ 100)

    Returns:
        List of page objects (Notion API response format)

    Raises:
        NotionAPIError: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    target_db_id = database_id or self.database_id
    all_pages = []
    start_cursor = None
    batch_count = 0

    logger.info(f"Starting pagination for database {target_db_id}")

    while True:
        try:
            # Rate limiting ì ìš© (Phase 2ì—ì„œ ì¶”ê°€ ì˜ˆì •)
            # await self.rate_limiter.acquire()

            # Notion API í˜¸ì¶œ
            response = self.client.databases.query(
                database_id=target_db_id,
                page_size=page_size,
                **({"start_cursor": start_cursor} if start_cursor else {})
            )

            # í˜„ì¬ ë°°ì¹˜ í˜ì´ì§€ ì¶”ê°€
            batch_pages = response.get("results", [])
            all_pages.extend(batch_pages)
            batch_count += 1

            logger.info(
                f"Batch {batch_count}: Fetched {len(batch_pages)} pages "
                f"(Total: {len(all_pages)})"
            )

            # ë‹¤ìŒ ë°°ì¹˜ í™•ì¸
            has_more = response.get("has_more", False)
            if not has_more:
                logger.info(f"Pagination complete: {len(all_pages)} total pages")
                break

            # ë‹¤ìŒ cursor ì„¤ì •
            start_cursor = response.get("next_cursor")
            if not start_cursor:
                logger.warning("has_more=True but no next_cursor, stopping")
                break

        except Exception as e:
            logger.error(
                f"Pagination error at batch {batch_count} "
                f"(cursor: {start_cursor}): {e}"
            )
            # Phase 3ì—ì„œ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ ì˜ˆì •
            raise

    return all_pages
```

**ë³€ê²½ ì‚¬í•­:**
- **Line 42-84 ì „ì²´ êµì²´** â†’ ìƒˆë¡œìš´ `fetch_all_database_pages()` ë©”ì„œë“œ
- ê¸°ì¡´ `query_database()` â†’ ë‚´ë¶€ì ìœ¼ë¡œ `fetch_all_database_pages()` í˜¸ì¶œí•˜ë„ë¡ ë˜í¼ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
- `start_cursor` íŒŒë¼ë¯¸í„° ì¶”ê°€
- `while True` ë£¨í”„ë¡œ `has_more=False`ê¹Œì§€ ë°˜ë³µ

#### 1-2. `backend/routers/pipeline.py` ìˆ˜ì •

**í˜„ì¬ ì½”ë“œ (Lines 26-60):**
```python
@router.post("/import-from-notion")
async def import_from_notion(
    page_size: int = Query(default=100, le=100),  # â† ì œí•œ
    notion_credentials: NotionCredentials = Depends(get_notion_credentials),
    supabase_service: SupabaseService = Depends(get_supabase_service),
):
    notion_service = NotionService(...)
    pages_data = await notion_service.query_database(page_size=page_size)
    pages = pages_data.get("pages", [])
    # ...
```

**ìƒˆë¡œìš´ êµ¬í˜„:**
```python
@router.post("/import-from-notion")
async def import_from_notion(
    # page_size íŒŒë¼ë¯¸í„° ì œê±° (í•­ìƒ 100 ì‚¬ìš©)
    fetch_all: bool = Query(
        default=True,
        description="True: ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°, False: ì²« 100ê°œë§Œ"
    ),
    notion_credentials: NotionCredentials = Depends(get_notion_credentials),
    supabase_service: SupabaseService = Depends(get_supabase_service),
):
    logger.info(f"Starting Notion import (fetch_all={fetch_all})")

    notion_service = NotionService(
        api_key=notion_credentials.api_key,
        database_id=notion_credentials.database_id,
    )

    # ìƒˆë¡œìš´ ë©”ì„œë“œ í˜¸ì¶œ
    if fetch_all:
        pages = await notion_service.fetch_all_database_pages(page_size=100)
    else:
        # í•˜ìœ„ í˜¸í™˜: ë‹¨ì¼ ë°°ì¹˜ë§Œ (í…ŒìŠ¤íŠ¸ìš©)
        pages_data = await notion_service.query_database(page_size=100)
        pages = pages_data.get("pages", [])

    logger.info(f"Fetched {len(pages)} pages from Notion")

    # ë‚˜ë¨¸ì§€ ë¡œì§ ë™ì¼ (Lines 70-126)
    # ...
```

**ë³€ê²½ ì‚¬í•­:**
- **Line 28:** `page_size` íŒŒë¼ë¯¸í„° ì œê±°, `fetch_all` í”Œë˜ê·¸ ì¶”ê°€
- **Line 54:** `fetch_all_database_pages()` í˜¸ì¶œ
- **Line 50-60:** ì¡°ê±´ë¶€ë¡œ ë‹¨ì¼ ë°°ì¹˜ ì§€ì› (í•˜ìœ„ í˜¸í™˜)

#### 1-3. í…ŒìŠ¤íŠ¸ ì „ëµ

**í…ŒìŠ¤íŠ¸ ë°ì´í„°:**
- 100ê°œ ë¯¸ë§Œ í˜ì´ì§€: ë‹¨ì¼ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
- 150ê°œ í˜ì´ì§€: 2 ë°°ì¹˜ í…ŒìŠ¤íŠ¸
- 300ê°œ í˜ì´ì§€: 3 ë°°ì¹˜ í…ŒìŠ¤íŠ¸

**ê²€ì¦ í•­ëª©:**
- `start_cursor`ê°€ ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬ë˜ëŠ”ì§€
- `has_more=False`ê¹Œì§€ ë£¨í”„ê°€ ë„ëŠ”ì§€
- ì¤‘ë³µ í˜ì´ì§€ ì—†ëŠ”ì§€ (notion_page_id ê¸°ì¤€)
- ë¡œê·¸ ì¶œë ¥ì´ ë°°ì¹˜ë³„ë¡œ í‘œì‹œë˜ëŠ”ì§€

**Phase 1 ì™„ë£Œ ê¸°ì¤€:**
âœ… 100ê°œ ì´ìƒ í˜ì´ì§€ë¥¼ í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ëª¨ë‘ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
âœ… `has_more`, `next_cursor` ë¡œì§ì´ ì •ìƒ ë™ì‘
âœ… ê¸°ì¡´ í…ŒìŠ¤íŠ¸ 44ê°œê°€ ì—¬ì „íˆ í†µê³¼

---

## Phase 2: ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ (ì¤‘ìš”, 3-4ì‹œê°„)

### ëª©í‘œ
- `content` í•„ë“œë¥¼ `None`ì´ ì•„ë‹Œ ì‹¤ì œ í˜ì´ì§€ ë³¸ë¬¸ìœ¼ë¡œ ì±„ìš°ê¸°
- Notion ë¸”ë¡ API (`blocks.children.list()`) í˜¸ì¶œ
- ë‹¤ì–‘í•œ ë¸”ë¡ íƒ€ì… ì²˜ë¦¬ (paragraph, heading, list, quote ë“±)

### êµ¬í˜„ ìƒì„¸

#### 2-1. `backend/services/notion_service.py` - ë¸”ë¡ ì¶”ì¶œ ë©”ì„œë“œ ì¶”ê°€

**ìƒˆë¡œìš´ ë©”ì„œë“œ 1: Rich Text ì¶”ì¶œ í—¬í¼**
```python
def _extract_rich_text(self, rich_text_array: list[dict]) -> str:
    """
    Notion rich_text ë°°ì—´ì—ì„œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        rich_text_array: Notion rich_text ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        ê²°í•©ëœ plain text ë¬¸ìì—´

    Example:
        Input: [{"plain_text": "Hello "}, {"plain_text": "World"}]
        Output: "Hello World"
    """
    if not rich_text_array:
        return ""

    texts = [item.get("plain_text", "") for item in rich_text_array]
    return "".join(texts)
```

**ìƒˆë¡œìš´ ë©”ì„œë“œ 2: ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ (í•µì‹¬)**
```python
async def fetch_page_blocks(
    self,
    page_id: str,
    max_depth: int = 2
) -> str:
    """
    í˜ì´ì§€ì˜ ëª¨ë“  ë¸”ë¡ì„ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        page_id: íƒ€ê²Ÿ í˜ì´ì§€ ID
        max_depth: ì¤‘ì²© ë¸”ë¡ íƒìƒ‰ ê¹Šì´ (toggle, column ë“±)

    Returns:
        í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼)

    Raises:
        NotionAPIError: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    content_parts = []
    start_cursor = None

    logger.debug(f"Fetching blocks for page {page_id}")

    while True:
        try:
            # Rate limiting ì ìš© (Phase 3ì—ì„œ êµ¬í˜„)
            # await self.rate_limiter.acquire()

            # ë¸”ë¡ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)
            response = self.client.blocks.children.list(
                block_id=page_id,
                page_size=100,
                **({"start_cursor": start_cursor} if start_cursor else {})
            )

            blocks = response.get("results", [])

            # ê° ë¸”ë¡ ì²˜ë¦¬
            for block in blocks:
                block_type = block.get("type")
                block_data = block.get(block_type, {})

                # ë¸”ë¡ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if block_type == "paragraph":
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    if text.strip():
                        content_parts.append(text)

                elif block_type in ["heading_1", "heading_2", "heading_3"]:
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    if text.strip():
                        # ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ í—¤ë”
                        level = int(block_type[-1])
                        content_parts.append(f"{'#' * level} {text}")

                elif block_type in ["bulleted_list_item", "numbered_list_item"]:
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    if text.strip():
                        prefix = "-" if block_type == "bulleted_list_item" else "1."
                        content_parts.append(f"{prefix} {text}")

                elif block_type == "quote":
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    if text.strip():
                        content_parts.append(f"> {text}")

                elif block_type == "callout":
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    emoji = block_data.get("icon", {}).get("emoji", "ğŸ’¡")
                    if text.strip():
                        content_parts.append(f"{emoji} {text}")

                elif block_type == "code":
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    language = block_data.get("language", "")
                    if text.strip():
                        content_parts.append(f"```{language}\n{text}\n```")

                elif block_type == "toggle":
                    # í† ê¸€ ì œëª©ë§Œ ì¶”ì¶œ (ì¤‘ì²© ë¸”ë¡ì€ max_depth ì œì–´)
                    text = self._extract_rich_text(block_data.get("rich_text", []))
                    if text.strip():
                        content_parts.append(f"â–¶ {text}")

                # TODO: ì¶”ê°€ ë¸”ë¡ íƒ€ì… (table, image ë“±) Phase 2.5ì—ì„œ êµ¬í˜„

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²´í¬
            has_more = response.get("has_more", False)
            if not has_more:
                break

            start_cursor = response.get("next_cursor")

        except Exception as e:
            logger.error(f"Error fetching blocks for page {page_id}: {e}")
            # ë¸”ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨í•´ë„ ë¶€ë¶„ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì½˜í…ì¸  ë°˜í™˜
            break

    # í…ìŠ¤íŠ¸ ê²°í•© (ê° ë¸”ë¡ ì‚¬ì´ ë¹ˆ ì¤„)
    full_content = "\n\n".join(content_parts)
    logger.debug(f"Extracted {len(full_content)} characters from page {page_id}")

    return full_content
```

**ë³€ê²½ ì‚¬í•­:**
- **ìƒˆë¡œìš´ ë©”ì„œë“œ ì¶”ê°€** (ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ìŒ)
- `_extract_rich_text()` - Line 125 ì´í›„ ì¶”ê°€
- `fetch_page_blocks()` - Line 200 ì´í›„ ì¶”ê°€
- 9ê°€ì§€ ë¸”ë¡ íƒ€ì… ì§€ì› (paragraph, heading 1/2/3, list, quote, callout, code, toggle)

#### 2-2. `backend/routers/pipeline.py` - content í•„ë“œ ì±„ìš°ê¸°

**í˜„ì¬ ì½”ë“œ (Lines 70-99):**
```python
for page in pages:
    try:
        page_id = page.get("id")
        # ...
        content = None  # â† ì—¬ê¸°ê°€ ë¬¸ì œ
```

**ìƒˆë¡œìš´ êµ¬í˜„:**
```python
for page in pages:
    try:
        page_id = page.get("id")

        # ì œëª© ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
        properties = page.get("properties", {})
        title = None
        for key in ["ì œëª©", "Name", "ì´ë¦„", "title"]:
            if key in properties:
                title_data = properties[key]
                if title_data.get("type") == "title":
                    title_array = title_data.get("title", [])
                    if title_array:
                        title = title_array[0].get("plain_text", "")
                        break

        # âœ¨ ìƒˆë¡œìš´ ë¶€ë¶„: ë¸”ë¡ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
        try:
            content = await notion_service.fetch_page_blocks(page_id)

            # ì½˜í…ì¸  ì—†ëŠ” ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬
            if not content or len(content.strip()) < 10:
                logger.warning(f"Page {page_id} has no meaningful content")
                content = None  # thought ì¶”ì¶œ ì‹œ ìŠ¤í‚µë¨

        except Exception as e:
            logger.warning(f"Failed to fetch blocks for page {page_id}: {e}")
            content = None  # ë¸”ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨í•´ë„ í˜ì´ì§€ëŠ” ì €ì¥

        # ë‚˜ë¨¸ì§€ ë¡œì§ ë™ì¼
        notion_url = f"https://www.notion.so/{page_id.replace('-', '')}"
        # ...
```

**ë³€ê²½ ì‚¬í•­:**
- **Line 85-95** ì˜ì—­ì— ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ ë¡œì§ ì¶”ê°€
- `content=None` â†’ `content=await notion_service.fetch_page_blocks(page_id)`
- Try-exceptë¡œ ë¸”ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œì—ë„ í˜ì´ì§€ëŠ” ì €ì¥ (contentë§Œ None)
- 10ì ë¯¸ë§Œ ì§§ì€ ì½˜í…ì¸ ëŠ” `None` ì²˜ë¦¬ (thought ì¶”ì¶œ ë‹¨ê³„ì—ì„œ í•„í„°ë§ë¨)

#### 2-3. ì„±ëŠ¥ ìµœì í™” ê³ ë ¤ ì‚¬í•­

**ë¬¸ì œ:** í˜ì´ì§€ 300ê°œ Ã— ë¸”ë¡ API í˜¸ì¶œ 1íšŒ = 300ë²ˆ ì¶”ê°€ API í˜¸ì¶œ

**ìµœì í™” ì „ëµ (Phase 2.5ì—ì„œ ì„ íƒì  êµ¬í˜„):**

**ì˜µì…˜ A: ë°°ì¹˜ ì²˜ë¦¬**
```python
# 10ê°œì”© ë°°ì¹˜ë¡œ ë¸”ë¡ ê°€ì ¸ì˜¤ê¸°
async def fetch_blocks_batch(page_ids: list[str]) -> dict[str, str]:
    tasks = [fetch_page_blocks(page_id) for page_id in page_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return dict(zip(page_ids, results))
```

**ì˜µì…˜ B: ì½˜í…ì¸  ìˆ˜ì§‘ ë¶„ë¦¬ (ì¶”ì²œ)**
```python
# Step 1: ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € import (ë¹ ë¦„)
# Step 2: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì½˜í…ì¸  ì±„ìš°ê¸° (ë³„ë„ ì—”ë“œí¬ì¸íŠ¸)
POST /pipeline/fetch-content?page_ids=...
```

**Phase 2ì—ì„œëŠ” ìˆœì°¨ ì²˜ë¦¬ë¡œ êµ¬í˜„, Phase 2.5ì—ì„œ ë°°ì¹˜ ìµœì í™” ê²€í† **

#### 2-4. í…ŒìŠ¤íŠ¸ ì „ëµ

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤:**
1. **ë¹ˆ í˜ì´ì§€** - ë¸”ë¡ 0ê°œ â†’ `content=None`
2. **ë‹¨ìˆœ í˜ì´ì§€** - paragraph 5ê°œ â†’ ì •ìƒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
3. **ë³µí•© í˜ì´ì§€** - heading + list + quote â†’ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í™•ì¸
4. **ëŒ€ëŸ‰ ë¸”ë¡** - 100ê°œ ì´ìƒ ë¸”ë¡ â†’ í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
5. **ì—ëŸ¬ í˜ì´ì§€** - ë¸”ë¡ API ì‹¤íŒ¨ â†’ `content=None`, í˜ì´ì§€ëŠ” ì €ì¥ë¨

**ê²€ì¦ í•­ëª©:**
- `raw_notes.content` í•„ë“œê°€ `NULL`ì´ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ì±„ì›Œì§
- Step 2 (thought ì¶”ì¶œ)ì—ì„œ `content`ë¥¼ ì œëŒ€ë¡œ ì½ì–´ì˜´
- ë¸”ë¡ íƒ€ì…ë³„ë¡œ ì˜¬ë°”ë¥´ê²Œ í¬ë§·íŒ…ë¨

**Phase 2 ì™„ë£Œ ê¸°ì¤€:**
âœ… `content` í•„ë“œê°€ ì‹¤ì œ í˜ì´ì§€ ë³¸ë¬¸ìœ¼ë¡œ ì±„ì›Œì§
âœ… 9ê°€ì§€ ì£¼ìš” ë¸”ë¡ íƒ€ì… ì²˜ë¦¬ ê°€ëŠ¥
âœ… ë¸”ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨í•´ë„ import í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰

## Phase 3: Rate Limiting êµ¬í˜„ (í•„ìˆ˜, 2ì‹œê°„)

### ëª©í‘œ
- Notion API 3 req/sec ì œí•œ ì¤€ìˆ˜
- 429 Too Many Requests ì—ëŸ¬ ë°©ì§€
- Exponential backoffë¡œ ì¬ì‹œë„

### êµ¬í˜„ ìƒì„¸

#### 3-1. `backend/services/rate_limiter.py` ìƒì„± (ì‹ ê·œ íŒŒì¼)

```python
"""
Rate Limiter for API calls using Token Bucket algorithm
"""
import time
import asyncio
from typing import Optional


class RateLimiter:
    """
    Token Bucket ê¸°ë°˜ Rate Limiter

    Usage:
        limiter = RateLimiter(rate=3.0)  # 3 req/sec
        await limiter.acquire()
    """

    def __init__(self, rate: float = 3.0):
        """
        Args:
            rate: ì´ˆë‹¹ í—ˆìš© ìš”ì²­ ìˆ˜ (ì˜ˆ: 3.0 = 3 req/sec)
        """
        self.rate = rate
        self.tokens = rate
        self.max_tokens = rate
        self.last_update = time.monotonic()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """
        í† í°ì„ ì†Œë¹„í•˜ê³  ìš”ì²­ ì‹¤í–‰ ê¶Œí•œ íšë“
        í† í°ì´ ë¶€ì¡±í•˜ë©´ ëŒ€ê¸°
        """
        async with self.lock:
            while self.tokens < 1:
                # í† í° ë³´ì¶©
                now = time.monotonic()
                elapsed = now - self.last_update
                self.tokens = min(
                    self.max_tokens,
                    self.tokens + elapsed * self.rate
                )
                self.last_update = now

                if self.tokens < 1:
                    # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                    sleep_time = (1 - self.tokens) / self.rate
                    await asyncio.sleep(sleep_time)

            # í† í° 1ê°œ ì†Œë¹„
            self.tokens -= 1


class ExponentialBackoff:
    """
    Exponential Backoff ì¬ì‹œë„ ì „ëµ

    Usage:
        backoff = ExponentialBackoff()
        for attempt in range(max_retries):
            try:
                result = await api_call()
                break
            except Exception as e:
                await backoff.sleep(attempt)
    """

    def __init__(
        self,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        multiplier: float = 2.0
    ):
        """
        Args:
            base_delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_delay: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            multiplier: ì§€ìˆ˜ ë°°ìœ¨
        """
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.multiplier = multiplier

    async def sleep(self, attempt: int):
        """
        ì¬ì‹œë„ attemptì— ë”°ë¼ ëŒ€ê¸°

        Args:
            attempt: ì¬ì‹œë„ íšŸìˆ˜ (0ë¶€í„° ì‹œì‘)
        """
        delay = min(
            self.base_delay * (self.multiplier ** attempt),
            self.max_delay
        )
        await asyncio.sleep(delay)
```

#### 3-2. `backend/config.py` ìˆ˜ì •

**ì¶”ê°€í•  ì„¤ì •:**
```python
# Line 40 ì´í›„ ì¶”ê°€

# Rate Limiting
RATE_LIMIT_NOTION: int = Field(default=3, env="RATE_LIMIT_NOTION")
RATE_LIMIT_OPENAI: int = Field(default=10, env="RATE_LIMIT_OPENAI")
RATE_LIMIT_ANTHROPIC: int = Field(default=5, env="RATE_LIMIT_ANTHROPIC")

# Retry Configuration
MAX_RETRIES: int = Field(default=3, env="MAX_RETRIES")
RETRY_BASE_DELAY: float = Field(default=1.0, env="RETRY_BASE_DELAY")
RETRY_MAX_DELAY: float = Field(default=60.0, env="RETRY_MAX_DELAY")
```

#### 3-3. `backend/services/notion_service.py` ìˆ˜ì •

**__init__ ë©”ì„œë“œ ìˆ˜ì • (Lines 13-16):**
```python
from .rate_limiter import RateLimiter, ExponentialBackoff
from ..config import settings

def __init__(self, api_key: str, database_id: str):
    self.client = Client(auth=api_key)
    self.database_id = database_id

    # Rate Limiter ì´ˆê¸°í™” (NEW)
    self.rate_limiter = RateLimiter(rate=float(settings.RATE_LIMIT_NOTION))
    self.backoff = ExponentialBackoff(
        base_delay=settings.RETRY_BASE_DELAY,
        max_delay=settings.RETRY_MAX_DELAY
    )
```

**API í˜¸ì¶œ ë¶€ë¶„ ìˆ˜ì • (Lines 160-170, 345-355):**
```python
# fetch_all_database_pages() ë‚´ë¶€ (Line 162 ì£¼ì„ í•´ì œ)
await self.rate_limiter.acquire()

response = self.client.databases.query(...)

# fetch_page_blocks() ë‚´ë¶€ (Line 347 ì£¼ì„ í•´ì œ)
await self.rate_limiter.acquire()

response = self.client.blocks.children.list(...)
```

**429 ì—ëŸ¬ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€:**
```python
# fetch_all_database_pages() try-except ìˆ˜ì • (Lines 160-200)
for retry in range(settings.MAX_RETRIES):
    try:
        await self.rate_limiter.acquire()

        response = self.client.databases.query(...)
        break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ

    except APIResponseError as e:
        if e.code == 429:  # Too Many Requests
            logger.warning(f"Rate limited, retrying ({retry+1}/{settings.MAX_RETRIES})")
            await self.backoff.sleep(retry)
            continue
        else:
            raise  # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ raise

    except Exception as e:
        logger.error(f"API error: {e}")
        if retry < settings.MAX_RETRIES - 1:
            await self.backoff.sleep(retry)
            continue
        raise
```

#### 3-4. í…ŒìŠ¤íŠ¸ ì „ëµ

**Rate Limiting ê²€ì¦:**
1. **ìˆ˜ë™ í…ŒìŠ¤íŠ¸** - ë¡œê·¸ë¡œ ìš”ì²­ ê°„ê²© í™•ì¸
   ```
   [INFO] Request 1 at 0.00s
   [INFO] Request 2 at 0.33s  # â† 1/3ì´ˆ ê°„ê²©
   [INFO] Request 3 at 0.66s
   ```

2. **429 ì—ëŸ¬ ì‹œë®¬ë ˆì´ì…˜** - Mock APIë¡œ 429 ë°˜í™˜ â†’ ì¬ì‹œë„ í™•ì¸

**Phase 3 ì™„ë£Œ ê¸°ì¤€:**
âœ… Notion API í˜¸ì¶œì´ 3 req/sec ì†ë„ë¡œ throttleë¨
âœ… 429 ì—ëŸ¬ ë°œìƒ ì‹œ exponential backoffë¡œ ì¬ì‹œë„
âœ… ë¡œê·¸ì—ì„œ rate limiting ë™ì‘ í™•ì¸ ê°€ëŠ¥

---

## Phase 4: í•˜ìœ„ í˜ì´ì§€ ì¬ê·€ íƒìƒ‰ (ì„ íƒ, 3-4ì‹œê°„)

### ëª©í‘œ
- Database ë‚´ child_page, child_database ë¸”ë¡ íƒìƒ‰
- ìµœëŒ€ depth ì œì–´ (ë¬´í•œ ì¬ê·€ ë°©ì§€)
- í˜ì´ì§€ ê³„ì¸µ êµ¬ì¡° flatí•˜ê²Œ ì €ì¥

### êµ¬í˜„ ìƒì„¸

#### 4-1. `backend/services/notion_service.py` - ì¬ê·€ ë©”ì„œë“œ ì¶”ê°€

```python
async def fetch_child_pages_recursive(
    self,
    parent_id: str,
    max_depth: int = 3,
    current_depth: int = 0
) -> list[dict]:
    """
    ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ í˜ì´ì§€ íƒìƒ‰

    Args:
        parent_id: ë¶€ëª¨ í˜ì´ì§€/ë°ì´í„°ë² ì´ìŠ¤ ID
        max_depth: ìµœëŒ€ íƒìƒ‰ ê¹Šì´
        current_depth: í˜„ì¬ ê¹Šì´ (ë‚´ë¶€ ì‚¬ìš©)

    Returns:
        List of page objects (flat list, ê³„ì¸µ êµ¬ì¡° ìœ ì§€ ì•ˆ í•¨)

    Note:
        child_databaseëŠ” databaseë¡œ query
        child_pageëŠ” pageë¡œ retrieve
    """
    if current_depth >= max_depth:
        logger.debug(f"Max depth {max_depth} reached for {parent_id}")
        return []

    child_pages = []

    try:
        await self.rate_limiter.acquire()

        # ë¸”ë¡ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        blocks_response = self.client.blocks.children.list(
            block_id=parent_id,
            page_size=100
        )

        for block in blocks_response.get("results", []):
            block_type = block.get("type")
            block_id = block.get("id")

            if block_type == "child_page":
                # Child pageëŠ” pages.retrieve()ë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                await self.rate_limiter.acquire()
                page_data = self.client.pages.retrieve(page_id=block_id)
                child_pages.append(page_data)

                # ì¬ê·€ í˜¸ì¶œ
                grandchildren = await self.fetch_child_pages_recursive(
                    block_id,
                    max_depth,
                    current_depth + 1
                )
                child_pages.extend(grandchildren)

            elif block_type == "child_database":
                # Child databaseëŠ” databases.query()ë¡œ í˜ì´ì§€ë“¤ ê°€ì ¸ì˜¤ê¸°
                db_pages = await self.fetch_all_database_pages(
                    database_id=block_id,
                    page_size=100
                )
                child_pages.extend(db_pages)

                # ê° í˜ì´ì§€ì˜ í•˜ìœ„ í˜ì´ì§€ë„ íƒìƒ‰
                for db_page in db_pages:
                    grandchildren = await self.fetch_child_pages_recursive(
                        db_page.get("id"),
                        max_depth,
                        current_depth + 1
                    )
                    child_pages.extend(grandchildren)

    except Exception as e:
        logger.error(f"Error fetching children of {parent_id}: {e}")
        # ì—ëŸ¬ ë°œìƒí•´ë„ ì´ë¯¸ ìˆ˜ì§‘í•œ í˜ì´ì§€ë“¤ì€ ë°˜í™˜

    return child_pages
```

#### 4-2. `backend/routers/pipeline.py` ìˆ˜ì •

**íŒŒë¼ë¯¸í„° ì¶”ê°€:**
```python
@router.post("/import-from-notion")
async def import_from_notion(
    fetch_all: bool = Query(default=True),
    include_children: bool = Query(
        default=False,
        description="True: í•˜ìœ„ í˜ì´ì§€ë„ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"
    ),
    max_depth: int = Query(
        default=3,
        ge=1,
        le=5,
        description="í•˜ìœ„ í˜ì´ì§€ íƒìƒ‰ ìµœëŒ€ ê¹Šì´ (1-5)"
    ),
    # ...
):
    # ...

    # ë£¨íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    root_pages = await notion_service.fetch_all_database_pages(page_size=100)

    all_pages = root_pages.copy()

    # í•˜ìœ„ í˜ì´ì§€ ì¬ê·€ íƒìƒ‰ (ì˜µì…˜)
    if include_children:
        logger.info(f"Fetching child pages (max_depth={max_depth})")

        for root_page in root_pages:
            child_pages = await notion_service.fetch_child_pages_recursive(
                parent_id=root_page.get("id"),
                max_depth=max_depth
            )
            all_pages.extend(child_pages)

        logger.info(
            f"Collected {len(root_pages)} root pages + "
            f"{len(all_pages) - len(root_pages)} child pages"
        )

    # ë‚˜ë¨¸ì§€ import ë¡œì§ ë™ì¼ (Lines 70-126)
    # all_pagesë¥¼ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
```

#### 4-3. ì¤‘ë³µ ì œê±° ë¡œì§

**ë¬¸ì œ:** ì¬ê·€ íƒìƒ‰ ì‹œ ê°™ì€ í˜ì´ì§€ê°€ ì—¬ëŸ¬ ê²½ë¡œë¡œ ì°¸ì¡°ë  ìˆ˜ ìˆìŒ

**í•´ê²°:**
```python
# pipeline.py ë‚´ë¶€ (Line 70 ì´ì „)
# ì¤‘ë³µ ì œê±° (notion_page_id ê¸°ì¤€)
unique_pages = {}
for page in all_pages:
    page_id = page.get("id")
    if page_id not in unique_pages:
        unique_pages[page_id] = page

all_pages = list(unique_pages.values())
logger.info(f"After deduplication: {len(all_pages)} unique pages")
```

#### 4-4. í…ŒìŠ¤íŠ¸ ì „ëµ

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**
1. **ë‹¨ì¼ depth** - Root í˜ì´ì§€ë§Œ (max_depth=0 equivalent)
2. **2-depth** - Root â†’ Child pages
3. **3-depth** - Root â†’ Child â†’ Grandchild
4. **ìˆœí™˜ ì°¸ì¡°** - Page A â†’ Page B â†’ Page A (ë¬´í•œ ë£¨í”„ ë°©ì§€ í™•ì¸)
5. **ëŒ€ëŸ‰ í•˜ìœ„ í˜ì´ì§€** - ê° í˜ì´ì§€ë§ˆë‹¤ 10ê°œ child â†’ 100í˜ì´ì§€ ìˆ˜ì§‘ í™•ì¸

**ê²€ì¦ í•­ëª©:**
- `max_depth` ì œí•œì´ ì •í™•íˆ ë™ì‘í•˜ëŠ”ì§€
- ì¤‘ë³µ í˜ì´ì§€ê°€ ì—†ëŠ”ì§€
- ìˆœí™˜ ì°¸ì¡°ë¡œ ì¸í•œ ë¬´í•œ ë£¨í”„ê°€ ì—†ëŠ”ì§€

**Phase 4 ì™„ë£Œ ê¸°ì¤€:**
âœ… `include_children=true`ì¼ ë•Œ í•˜ìœ„ í˜ì´ì§€ ì¬ê·€ íƒìƒ‰
âœ… `max_depth` íŒŒë¼ë¯¸í„°ë¡œ ê¹Šì´ ì œì–´
âœ… ì¤‘ë³µ ì œê±° ë¡œì§ ë™ì‘

**Note:** Phase 4ëŠ” ì„ íƒ ì‚¬í•­ì´ë¯€ë¡œ Phase 1-3 ì™„ë£Œ í›„ í•„ìš”ì„± ì¬ê²€í† 

---

## Phase 5: ë°°ì¹˜ ì²˜ë¦¬ & ìµœì í™” (ì„ íƒ, 2ì‹œê°„)

### ëª©í‘œ
- Supabase upsertë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (Në²ˆ â†’ 1ë²ˆ DB í˜¸ì¶œ)
- ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ì„ ë³‘ë ¬í™” (asyncio.gather)

### êµ¬í˜„ ìƒì„¸

#### 5-1. `backend/services/supabase_service.py` - ë°°ì¹˜ upsert ì¶”ê°€

```python
async def upsert_raw_notes_batch(
    self,
    notes: list[RawNoteCreate]
) -> dict:
    """
    ì—¬ëŸ¬ raw_notesë¥¼ í•œ ë²ˆì— upsert

    Args:
        notes: RawNoteCreate ë¦¬ìŠ¤íŠ¸

    Returns:
        {"inserted": int, "updated": int}
    """
    if not notes:
        return {"inserted": 0, "updated": 0}

    # Pydantic ëª¨ë¸ â†’ dict ë³€í™˜
    notes_data = [note.model_dump() for note in notes]

    try:
        response = (
            self.client.table("raw_notes")
            .upsert(
                notes_data,
                on_conflict="notion_page_id",  # ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸
                count="exact"
            )
            .execute()
        )

        count = response.count or len(notes)
        logger.info(f"Batch upserted {count} raw_notes")

        return {"inserted": count, "updated": 0}  # SupabaseëŠ” êµ¬ë¶„ ì•ˆ ë¨

    except Exception as e:
        logger.error(f"Batch upsert failed: {e}")
        raise
```

#### 5-2. `backend/routers/pipeline.py` - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€ê²½

**í˜„ì¬ (Lines 70-110):**
```python
for page in pages:
    # ... í˜ì´ì§€ ì²˜ë¦¬
    await supabase_service.upsert_raw_note(raw_note)  # Në²ˆ í˜¸ì¶œ
```

**ë³€ê²½ í›„:**
```python
# ëª¨ë“  í˜ì´ì§€ë¥¼ ë¨¼ì € ì²˜ë¦¬í•˜ê³  ëª¨ì•„ë‘ê¸°
raw_notes_batch = []

for page in pages:
    try:
        # ... í˜ì´ì§€ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        raw_note = RawNoteCreate(...)
        raw_notes_batch.append(raw_note)

    except Exception as e:
        logger.warning(f"Failed to process page {page.get('id')}: {e}")
        skipped_count += 1
        continue

# ë°°ì¹˜ë¡œ í•œ ë²ˆì— upsert
if raw_notes_batch:
    await supabase_service.upsert_raw_notes_batch(raw_notes_batch)
    imported_count = len(raw_notes_batch)
```

**ì„±ëŠ¥ ê°œì„ :**
- Before: 300 í˜ì´ì§€ â†’ 300ë²ˆ DB í˜¸ì¶œ â†’ ~30ì´ˆ
- After: 300 í˜ì´ì§€ â†’ 1ë²ˆ DB í˜¸ì¶œ â†’ ~3ì´ˆ

#### 5-3. ë¸”ë¡ ì½˜í…ì¸  ë³‘ë ¬ ìˆ˜ì§‘

**í˜„ì¬ (ìˆœì°¨ ì²˜ë¦¬):**
```python
for page in pages:
    content = await notion_service.fetch_page_blocks(page_id)  # ìˆœì°¨
```

**ë³€ê²½ í›„ (ë³‘ë ¬ ì²˜ë¦¬):**
```python
# 10ê°œì”© ë°°ì¹˜ë¡œ ë³‘ë ¬ ìˆ˜ì§‘
BATCH_SIZE = 10

for i in range(0, len(pages), BATCH_SIZE):
    batch = pages[i:i+BATCH_SIZE]

    # asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰
    tasks = [
        notion_service.fetch_page_blocks(page.get("id"))
        for page in batch
    ]

    contents = await asyncio.gather(*tasks, return_exceptions=True)

    # ê²°ê³¼ ë§¤í•‘
    for page, content in zip(batch, contents):
        if isinstance(content, Exception):
            logger.warning(f"Failed to fetch blocks: {content}")
            content = None

        # contentë¥¼ í˜ì´ì§€ì— ì—°ê²°
        page["_content"] = content
```

**ì„±ëŠ¥ ê°œì„ :**
- Before: 300 í˜ì´ì§€ Ã— 0.5ì´ˆ/ë¸”ë¡ = 150ì´ˆ (2.5ë¶„)
- After: 300 í˜ì´ì§€ / 10 ë°°ì¹˜ Ã— 0.5ì´ˆ = 15ì´ˆ

**Trade-off:** Rate limitingê³¼ ì¶©ëŒ ê°€ëŠ¥ â†’ ë°°ì¹˜ í¬ê¸° ì¡°ì • í•„ìš”

#### 5-4. í…ŒìŠ¤íŠ¸ ì „ëµ

**ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:**
- 100 í˜ì´ì§€ import ì‹œê°„ ì¸¡ì • (Before / After)
- DB í˜¸ì¶œ íšŸìˆ˜ ì¹´ìš´íŠ¸ (ë¡œê·¸ ë¶„ì„)

**Phase 5 ì™„ë£Œ ê¸°ì¤€:**
âœ… Supabase upsertë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
âœ… ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ì„ ë³‘ë ¬í™” (ì„ íƒ)
âœ… ì „ì²´ import ì‹œê°„ 50% ì´ìƒ ë‹¨ì¶•

**Note:** Phase 5ëŠ” ì„ íƒ ì‚¬í•­ì´ë¯€ë¡œ Phase 1-3 ì•ˆì •í™” í›„ ê²€í† 

## êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ì†Œìš” ì‹œê°„

### ê¶Œì¥ ìˆœì„œ

**Phase 1 (í•„ìˆ˜, 2-3ì‹œê°„):**
- í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„ êµ¬í˜„
- ê¸°ë³¸ ê¸°ëŠ¥ ì™„ì„± (100ê°œ ì´ìƒ í˜ì´ì§€ ìˆ˜ì§‘)

**Phase 2 (í•„ìˆ˜, 3-4ì‹œê°„):**
- ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘
- `content` í•„ë“œ ì±„ìš°ê¸°

**Phase 3 (í•„ìˆ˜, 2ì‹œê°„):**
- Rate Limiting êµ¬í˜„
- 429 ì—ëŸ¬ ëŒ€ì‘

**Phase 4 (ì„ íƒ, 3-4ì‹œê°„):**
- í•˜ìœ„ í˜ì´ì§€ ì¬ê·€ íƒìƒ‰
- Phase 1-3 ì•ˆì •í™” í›„ ê²€í† 

**Phase 5 (ì„ íƒ, 2ì‹œê°„):**
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ì„±ëŠ¥ ê°œì„ 

**ì´ ì†Œìš” ì‹œê°„ ì˜ˆìƒ:**
- í•„ìˆ˜ Phase (1-3): 7-9ì‹œê°„
- ì„ íƒ Phase (4-5): +5-6ì‹œê°„
- **í•©ê³„: 12-15ì‹œê°„** (2-3ì¼)

### ìˆ˜ì •/ìƒì„± íŒŒì¼ ìš”ì•½

**ìˆ˜ì • í•„ìš”:**
1. `backend/services/notion_service.py`
   - `fetch_all_database_pages()` ì¶”ê°€ (Phase 1)
   - `fetch_page_blocks()` ì¶”ê°€ (Phase 2)
   - `fetch_child_pages_recursive()` ì¶”ê°€ (Phase 4)
   - Rate Limiter í†µí•© (Phase 3)

2. `backend/routers/pipeline.py`
   - `fetch_all` íŒŒë¼ë¯¸í„° ì¶”ê°€ (Phase 1)
   - ë¸”ë¡ ì½˜í…ì¸  ìˆ˜ì§‘ ë¡œì§ (Phase 2)
   - `include_children` íŒŒë¼ë¯¸í„° (Phase 4)
   - ë°°ì¹˜ upsertë¡œ ë³€ê²½ (Phase 5)

3. `backend/config.py`
   - Rate limiting ì„¤ì • ì¶”ê°€ (Phase 3)

4. `backend/services/supabase_service.py`
   - `upsert_raw_notes_batch()` ì¶”ê°€ (Phase 5)

**ì‹ ê·œ ìƒì„±:**
1. `backend/services/rate_limiter.py` (Phase 3)
   - RateLimiter í´ë˜ìŠ¤
   - ExponentialBackoff í´ë˜ìŠ¤

---

## ì¶”ê°€ ê³ ë ¤ ì‚¬í•­ ë° ë°©ì–´ ì „ëµ

### 1. ì¤‘ê°„ ì—ëŸ¬ ì¬ì‹œë„ ëŒ€ì±…

**ë¬¸ì œ:** 300ê°œ í˜ì´ì§€ import ì¤‘ 150ë²ˆì§¸ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ìŒë¶€í„° ì¬ì‹œì‘?

**í•´ê²° ë°©ì•ˆ (Phase 2.5, ì„ íƒ):**

ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ê°œ:
```python
# ë°°ì¹˜ë§ˆë‹¤ cursor ì €ì¥
checkpoint = {"last_cursor": next_cursor, "processed": page_count}
save_checkpoint(checkpoint)  # JSON íŒŒì¼ë¡œ ì €ì¥

# ì¬ì‹œì‘ ì‹œ ë¡œë“œ
checkpoint = load_checkpoint()
start_cursor = checkpoint.get("last_cursor")
```

**Phase 1-3ì—ì„œëŠ”:**
- ë‹¨ìˆœ êµ¬í˜„ (ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ)
- ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¬ì‹œë„
- 3-5ë¶„ ë‚´ ì™„ë£Œ ê°€ëŠ¥í•˜ë¯€ë¡œ í—ˆìš© ê°€ëŠ¥

---

### 2. ì¤‘ë³µ ë©”ëª¨ ë°©ì–´ ì „ëµ

#### ì´ë¯¸ êµ¬í˜„ëœ ë³´í˜¸ (DB ë ˆë²¨)

```sql
-- raw_notes í…Œì´ë¸”
notion_page_id TEXT UNIQUE NOT NULL  -- â† UNIQUE ì œì•½
```

```python
# supabase_service.py
.upsert(note_data, on_conflict="notion_page_id")  # â† ì¤‘ë³µ ì‹œ UPDATE
```

**ë™ì‘:**
- ê°™ì€ í˜ì´ì§€ ì¬import â†’ ì—ëŸ¬ ì—†ì´ **ì—…ë°ì´íŠ¸** (ë®ì–´ì“°ê¸°)
- `notion_last_edited_time` ìµœì‹ í™”ë¨

#### ì¶”ê°€ ë°©ì–´ (Phase 1ì— ì¶”ê°€)

**ë©”ëª¨ë¦¬ ë‚´ ì¤‘ë³µ ì œê±°:**
```python
# pipeline.py - import_from_notion()
all_pages = await notion_service.fetch_all_database_pages()

# ì¤‘ë³µ ì œê±° (ê°™ì€ ì„¸ì…˜ ë‚´)
unique_pages = {}
for page in all_pages:
    page_id = page.get("id")
    if page_id not in unique_pages:
        unique_pages[page_id] = page

all_pages = list(unique_pages.values())
logger.info(f"{len(all_pages)} unique pages after deduplication")
```

**Phase 4 ì¬ê·€ íƒìƒ‰ ì‹œ:**
- ê°™ì€ í˜ì´ì§€ê°€ ì—¬ëŸ¬ ê²½ë¡œë¡œ ë°œê²¬ë  ìˆ˜ ìˆìŒ
- ìœ„ ë¡œì§ìœ¼ë¡œ ìë™ ì œê±°

---

### 3. raw_notes í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„±

#### í˜„ì¬ ìŠ¤í‚¤ë§ˆ (supabase_setup.sql)
```sql
notion_page_id TEXT UNIQUE NOT NULL,
notion_url TEXT NOT NULL,
title TEXT,                              -- nullable
content TEXT,                            -- nullable
properties_json JSONB DEFAULT '{}'::jsonb,
notion_created_time TIMESTAMPTZ NOT NULL,
notion_last_edited_time TIMESTAMPTZ NOT NULL,
imported_at TIMESTAMPTZ DEFAULT NOW()
```

#### Notion API â†’ DB ë§¤í•‘

| í•„ë“œ | Notion API | í˜„ì¬ ë¡œì§ | Phase ë³€ê²½ |
|------|-----------|----------|-----------|
| `notion_page_id` | `page["id"]` | âœ… ë§¤í•‘ë¨ | ë³€ê²½ ì—†ìŒ |
| `notion_url` | N/A | âœ… ìˆ˜ë™ ìƒì„± | ë³€ê²½ ì—†ìŒ |
| `title` | `properties["ì œëª©/Name"]` | âœ… ì¶”ì¶œ | ë³€ê²½ ì—†ìŒ |
| `content` | N/A | âŒ `None` | **Phase 2: ë¸”ë¡ ìˆ˜ì§‘** |
| `properties_json` | `page["properties"]` | âœ… ë§¤í•‘ë¨ | ë³€ê²½ ì—†ìŒ |
| íƒ€ì„ìŠ¤íƒ¬í”„ | `created_time` | âœ… ISO íŒŒì‹± | ë³€ê²½ ì—†ìŒ |

**í˜¸í™˜ì„±:**
- âœ… ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì™€ ì™„ì „ í˜¸í™˜
- âœ… Phase 2ì—ì„œ `content`ë§Œ `None â†’ í…ìŠ¤íŠ¸`ë¡œ ë³€ê²½
- âœ… nullableì´ë¯€ë¡œ Phase 1ì—ì„œë„ ë™ì‘

---

### 4. ê¸°íƒ€ ê°„ê³¼í•œ ì§€ì 

#### 4-1. ë¹ˆ title ì²˜ë¦¬

**í˜„ì¬:** `title = None` (nullable) âœ…

**ì¶”ê°€ ë°©ì–´ (ì„ íƒ):**
```python
if not title:
    title = f"Untitled ({datetime.now().strftime('%Y-%m-%d')})"
```

#### 4-2. ëŒ€ìš©ëŸ‰ content ì²˜ë¦¬

**DB ì œì•½:** `TEXT` íƒ€ì… = ìµœëŒ€ 1GB âœ…

**ë°©ì–´ (Phase 2):**
```python
MAX_CONTENT_LENGTH = 1_000_000  # 1MB

if len(full_content) > MAX_CONTENT_LENGTH:
    logger.warning(f"Content truncated: {len(full_content)} bytes")
    full_content = full_content[:MAX_CONTENT_LENGTH] + "\n[...truncated]"
```

#### 4-3. ë¯¸ì§€ì› ë¸”ë¡ íƒ€ì…

**Phase 2 ì§€ì›:** paragraph, heading, list, quote, callout, code, toggle

**ë¯¸ì§€ì›:** table, image, video, file, bookmark

**ì²˜ë¦¬:**
```python
elif block_type in ["table", "image", "video"]:
    content_parts.append(f"[{block_type.upper()}]")  # íƒ€ì…ë§Œ í‘œì‹œ
```

#### 4-4. Notion API íƒ€ì„ì¡´

**í˜„ì¬ ì½”ë“œ:**
```python
datetime.fromisoformat(page.get("created_time").replace("Z", "+00:00"))
```
âœ… ì˜¬ë°”ë¦„ (Notionì€ UTCë¡œ ë°˜í™˜)

#### 4-5. Rate Limiting ì •í™•ë„

**ê²€ì¦ í•„ìš” (Phase 3 í…ŒìŠ¤íŠ¸):**
```python
# 10ë²ˆ ìš”ì²­ ì‹œ 3.33ì´ˆ ì†Œìš” í™•ì¸
for i in range(10):
    await limiter.acquire()
    # 0.33ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
```

---

## ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 ì™„ë£Œ ê¸°ì¤€
âœ… 100ê°œ ì´ìƒ í˜ì´ì§€ ìˆ˜ì§‘
âœ… `has_more`, `next_cursor` ì •ìƒ ë™ì‘
âœ… ì¤‘ë³µ í˜ì´ì§€ ìë™ ì œê±°
âœ… ê¸°ì¡´ í…ŒìŠ¤íŠ¸ 44ê°œ í†µê³¼

### Phase 2 ì™„ë£Œ ê¸°ì¤€
âœ… `content` í•„ë“œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ì±„ì›Œì§
âœ… 9ê°€ì§€ ë¸”ë¡ íƒ€ì… ì²˜ë¦¬
âœ… ë¹ˆ í˜ì´ì§€ â†’ `content=None`
âœ… ë¸”ë¡ í˜ì´ì§€ë„¤ì´ì…˜ ë™ì‘

### Phase 3 ì™„ë£Œ ê¸°ì¤€
âœ… Rate limiting 3 req/sec ì¤€ìˆ˜
âœ… 429 ì—ëŸ¬ ì¬ì‹œë„
âœ… ë¡œê·¸ì—ì„œ throttling í™•ì¸

### í†µí•© í…ŒìŠ¤íŠ¸
- **50ê°œ í˜ì´ì§€:** 5ë¶„ ë‚´ ì™„ë£Œ, content ì±„ì›Œì§
- **300ê°œ í˜ì´ì§€:** 20ë¶„ ë‚´ ì™„ë£Œ, ì¤‘ë³µ 0ê°œ
- **ì¬import:** ê¸°ì¡´ í˜ì´ì§€ ì—…ë°ì´íŠ¸ (ì—ëŸ¬ ì—†ìŒ)

---

## ì˜µì…˜ 2: ë‹¤ì¤‘ Pair ZK ì•Œê³ ë¦¬ì¦˜ (N-Pair)

### í˜„ì¬ ì œì•½ ì‚¬í•­

**ë¶„ì„ ê²°ê³¼ ìš”ì•½:**
1. **DB ìŠ¤í‚¤ë§ˆ:** `thought_pairs` í…Œì´ë¸”ì´ 2ê°œ ì»¬ëŸ¼ë§Œ ì§€ì›
2. **ì¡°í•© ì•Œê³ ë¦¬ì¦˜:** C(n,2) í•˜ë“œì½”ë”© (Stored Procedure JOIN)
3. **LLM í”„ë¡¬í”„íŠ¸:** "ë‘ ì•„ì´ë””ì–´" ë¹„êµ ì „ì œ
4. **Essay ìƒì„±:** ì •í™•íˆ 2ê°œ thoughtë§Œ ì²˜ë¦¬
5. **ë³µì¡ë„ í­ë°œ:** C(100,3) = 161,700 (32ë°° ì¦ê°€)

### í•„ìš”í•œ êµ¬í˜„ ì‚¬í•­

#### 1. DB ìŠ¤í‚¤ë§ˆ ì¬ì„¤ê³„ (í•„ìˆ˜)

**Option A: Junction í…Œì´ë¸” (ê¶Œì¥)**
```sql
-- Nê°œ thought ì¡°í•©ì„ ì €ì¥
CREATE TABLE thought_clusters (
    id SERIAL PRIMARY KEY,
    avg_similarity FLOAT NOT NULL,
    connection_reason TEXT,
    selected_at TIMESTAMPTZ DEFAULT NOW(),
    is_used_in_essay BOOLEAN DEFAULT FALSE
);

CREATE TABLE cluster_thoughts (
    cluster_id INT REFERENCES thought_clusters(id) ON DELETE CASCADE,
    thought_id INT REFERENCES thought_units(id) ON DELETE CASCADE,
    position INT NOT NULL,  -- ìˆœì„œ ë³´ì¡´
    PRIMARY KEY (cluster_id, thought_id)
);

-- essays í…Œì´ë¸” ìˆ˜ì •
ALTER TABLE essays
    DROP COLUMN pair_id,
    ADD COLUMN cluster_id INT REFERENCES thought_clusters(id);
```

**Option B: JSONB ë°°ì—´ (ê°„ë‹¨í•˜ì§€ë§Œ ëœ ìœ ì—°)**
```sql
CREATE TABLE thought_clusters (
    id SERIAL PRIMARY KEY,
    thought_ids INT[] NOT NULL,  -- [1, 3, 7]
    similarity_matrix JSONB,  -- {"1-3": 0.12, "1-7": 0.25}
    connection_reason TEXT,
    CHECK (array_length(thought_ids, 1) >= 2)
);
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸:**
```sql
-- ê¸°ì¡´ thought_pairs ë°ì´í„°ë¥¼ thought_clustersë¡œ ë³€í™˜
INSERT INTO thought_clusters (id, avg_similarity, connection_reason, selected_at, is_used_in_essay)
SELECT id, similarity_score, connection_reason, selected_at, is_used_in_essay
FROM thought_pairs;

INSERT INTO cluster_thoughts (cluster_id, thought_id, position)
SELECT id, thought_a_id, 1 FROM thought_pairs
UNION ALL
SELECT id, thought_b_id, 2 FROM thought_pairs;

-- ê¸°ì¡´ í…Œì´ë¸”ì€ ë°±ì—… í›„ ì‚­ì œ
ALTER TABLE thought_pairs RENAME TO thought_pairs_backup;
```

**ìˆ˜ì • íŒŒì¼:**
- `/backend/docs/supabase_setup.sql` (ìƒˆ ìŠ¤í‚¤ë§ˆ)
- `/backend/schemas/zk.py` (ThoughtCluster ëª¨ë¸)
- Migration script ìƒì„±

---

#### 2. ì¡°í•© ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (í•„ìˆ˜)

**Pythonìœ¼ë¡œ ì´ë™ (Stored Procedure ëŒ€ì²´)**
```python
# backend/services/supabase_service.py
from itertools import combinations

async def find_candidate_clusters(
    cluster_size: int = 3,
    min_similarity: float = 0.05,
    max_similarity: float = 0.35,
    max_candidates: int = 1000  # ë³µì¡ë„ ì œí•œ
) -> List[dict]:
    """Nê°œ thought ì¡°í•© ìƒì„± (C(n,k))"""

    # 1. ëª¨ë“  thought_units ì¡°íšŒ
    thoughts = await self.get_all_thoughts()

    # 2. C(n, k) ì¡°í•© ìƒì„±
    all_combos = combinations(thoughts, cluster_size)

    # 3. ê° ì¡°í•©ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    candidates = []
    for combo in all_combos:
        avg_sim = calculate_avg_pairwise_similarity(combo)

        if min_similarity <= avg_sim <= max_similarity:
            candidates.append({
                "thought_ids": [t["id"] for t in combo],
                "thoughts": combo,
                "avg_similarity": avg_sim
            })

        if len(candidates) >= max_candidates:
            break  # ì¡°ê¸° ì¢…ë£Œ

    return candidates
```

**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ (ë³µì¡ë„ ì™„í™”, ê¶Œì¥):**
```python
async def find_clusters_hybrid(cluster_size: int = 3):
    """2-pair ê¸°ë°˜ ì ì§„ì  í™•ì¥"""

    # Step 1: ê¸°ì¡´ 2-pair ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¢‹ì€ í˜ì–´ ì„ íƒ
    top_pairs = await find_candidate_pairs(top_n=50, min_score=75)

    # Step 2: ê° í˜ì–´ì— 3ë²ˆì§¸ thought ì¶”ê°€
    clusters = []
    for pair in top_pairs:
        # í˜ì–´ì™€ ì•½í•œ ì—°ê²°ëœ 3ë²ˆì§¸ thought ì°¾ê¸°
        third = await find_complementary_thought(
            pair.thought_a_id,
            pair.thought_b_id,
            min_sim=0.05,
            max_sim=0.35
        )

        if third:
            clusters.append({
                "thought_ids": [pair.thought_a_id, pair.thought_b_id, third["id"]],
                "avg_similarity": (pair.similarity + third["sim_to_pair"]) / 2
            })

    return clusters
```

**ìˆ˜ì • íŒŒì¼:**
- `/backend/services/supabase_service.py` - ì¡°í•© ì•Œê³ ë¦¬ì¦˜
- `/backend/routers/pipeline.py` - Step 3 ì—”ë“œí¬ì¸íŠ¸ (cluster_size íŒŒë¼ë¯¸í„° ì¶”ê°€)

---

#### 3. LLM í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„± (í•„ìˆ˜)

**í˜„ì¬ (2-pair):**
```
ë‘ ì•„ì´ë””ì–´ì˜ ì°½ì˜ì  ì—°ê²° ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ì„¸ìš”.

claim_a: ...
claim_b: ...

ì ìˆ˜ (0-100):
```

**N-pair í”„ë¡¬í”„íŠ¸:**
```python
# backend/services/ai_service.py
async def score_clusters(clusters: List[dict]) -> List[dict]:
    """Nê°œ thought ì¡°í•© í‰ê°€"""

    prompt = f"""
ë‹¤ìŒ {cluster_size}ê°œ ì•„ì´ë””ì–´ì˜ ì°½ì˜ì  ì¡°í•© ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ì„¸ìš”.

{format_thoughts_list(cluster["thoughts"])}

í‰ê°€ ê¸°ì¤€:
1. ëª¨ë“  ì•„ì´ë””ì–´ê°€ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ê°€? (30ì )
2. 2ê°œ ì¡°í•©ë³´ë‹¤ {cluster_size}ê°œ ì¡°í•©ì´ ë” í’ë¶€í•œ í†µì°°ì„ ì£¼ëŠ”ê°€? (40ì )
3. ì„œë¡œ ë‹¤ë¥¸ ë§¥ë½ì˜ ì•„ì´ë””ì–´ê°€ êµì°¨í•˜ëŠ”ê°€? (30ì )

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{
  "score": 0-100,
  "reason": "í‰ê°€ ì´ìœ  (í•œê¸€, 100ì ì´ë‚´)"
}}
"""
```

**Essay ìƒì„± í”„ë¡¬í”„íŠ¸:**
```python
async def generate_essay_from_cluster(cluster: dict) -> dict:
    """Nê°œ thoughtë¡œ Essay ìƒì„±"""

    prompt = f"""
ë‹¤ìŒ {len(cluster['thoughts'])}ê°œ ì‚¬ê³  ë‹¨ìœ„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸€ê°ì„ ìƒì„±í•˜ì„¸ìš”.

{format_thoughts_detailed(cluster['thoughts'])}

ìš”êµ¬ì‚¬í•­:
- ì œëª©: 5-100ì
- outline: {len(cluster['thoughts'])}ê°œ í•­ëª© (ê° thoughtë‹¹ 1ê°œ)
  - 1ë‹¨: ì²« ë²ˆì§¸ + ë‘ ë²ˆì§¸ ì•„ì´ë””ì–´ ë„ì…
  - 2ë‹¨: ì„¸ ë²ˆì§¸ ì•„ì´ë””ì–´ë¡œ ë³µì¡ë„ í™•ì¥
  - 3ë‹¨: ëª¨ë“  ì•„ì´ë””ì–´ë¥¼ í†µí•©í•œ ìƒˆë¡œìš´ í†µì°°
- reason: ì™œ ì´ {len(cluster['thoughts'])}ê°œ ì¡°í•©ì´ ì¢‹ì€ ê¸€ê°ì¸ì§€ (300ì ì´ë‚´)
"""
```

**ìˆ˜ì • íŒŒì¼:**
- `/backend/services/ai_service.py` - score_pairs â†’ score_clusters
- `/backend/services/ai_service.py` - generate_essay í”„ë¡¬í”„íŠ¸ ë™ì í™”

---

#### 4. ë³µì¡ë„ ì™„í™” ì „ëµ (ì¤‘ìš”)

**ë¬¸ì œ:**
```
C(100, 2) = 4,950
C(100, 3) = 161,700 (32ë°°)
C(100, 4) = 3,921,225 (790ë°°)
```

**í•´ê²°ì±…:**

**ì „ëµ 1: Pre-filtering (ìœ ì‚¬ë„ ë²”ìœ„)**
```python
# Step 1: ìœ ì‚¬ë„ ë²”ìœ„ë¡œ ë¨¼ì € í•„í„°ë§
thoughts_in_range = await filter_by_similarity(min=0.05, max=0.35)
# 100ê°œ â†’ 50ê°œë¡œ ì¶•ì†Œ
# C(50, 3) = 19,600 (manageable)
```

**ì „ëµ 2: Greedy í™•ì¥ (í•˜ì´ë¸Œë¦¬ë“œ)**
```python
# Step 1: ì¢‹ì€ 2-pair ì„ íƒ (50ê°œ)
pairs = await select_pairs(top_n=50)

# Step 2: ê° í˜ì–´ì— 3ë²ˆì§¸ë§Œ ì¶”ê°€
# 50 pairs * 50 candidates = 2,500 ì¡°í•© (vs 161,700)
```

**ì „ëµ 3: K-means Clustering + Sampling**
```python
# Step 1: ì„ë² ë”© ê¸°ë°˜ K-means (k=10)
clusters = kmeans(embeddings, n_clusters=10)

# Step 2: ê° í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œë§Œ ì¡°í•©
# C(10, 3) * 10 clusters = 1,200
```

**ìˆ˜ì • íŒŒì¼:**
- `/backend/services/supabase_service.py` - Pre-filtering ë¡œì§
- `/backend/routers/pipeline.py` - max_candidates íŒŒë¼ë¯¸í„°

---

### êµ¬í˜„ ìš°ì„ ìˆœìœ„

**Phase 1 (í•µì‹¬):**
1. âœ… DB ìŠ¤í‚¤ë§ˆ ë³€ê²½ + ë§ˆì´ê·¸ë ˆì´ì…˜ (2-3ì‹œê°„)
2. âœ… í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (2-3ì‹œê°„)
3. âœ… LLM í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„± (1-2ì‹œê°„)

**Phase 2 (ìµœì í™”):**
4. ë³µì¡ë„ ì™„í™” (Pre-filtering) (1ì‹œê°„)
5. í”„ë¡ íŠ¸ì—”ë“œ N-pair í‘œì‹œ (1ì‹œê°„)

**ì˜ˆìƒ ì´ ì†Œìš” ì‹œê°„:** 5-8ì‹œê°„ (Phase 1), +2ì‹œê°„ (Phase 2)

---

### Critical Files

**ìˆ˜ì • í•„ìš”:**
- `/backend/docs/supabase_setup.sql` - ìŠ¤í‚¤ë§ˆ ì¬ì„¤ê³„
- `/backend/schemas/zk.py` - ThoughtCluster ëª¨ë¸
- `/backend/services/supabase_service.py` - ì¡°í•© ì•Œê³ ë¦¬ì¦˜
- `/backend/services/ai_service.py` - í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„±
- `/backend/routers/pipeline.py` - Step 3 ì—”ë“œí¬ì¸íŠ¸

**ì‹ ê·œ ìƒì„±:**
- `/backend/migrations/001_pair_to_cluster.sql` - ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

---

## ê¶Œì¥ ìˆœì„œ

### ì‹œë‚˜ë¦¬ì˜¤ A: ë¹ ë¥¸ ê°€ì¹˜ ì°½ì¶œ
1. **ì˜µì…˜ 1 (Notion ëŒ€ëŸ‰ ìˆ˜ì§‘) - Phase 1** (4-6ì‹œê°„)
   - ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥, ë°ì´í„° í’ë¶€í™”
   - ê¸°ì¡´ ì•„í‚¤í…ì²˜ ìœ ì§€
2. **ì˜µì…˜ 2 (N-pair) - Phase 1** (5-8ì‹œê°„)
   - ì°½ì˜ì„± ì¦ê°€
   - ì•„í‚¤í…ì²˜ ë³€ê²½ í•„ìš”

**ì´ ì†Œìš” ì‹œê°„:** 9-14ì‹œê°„ (ì•½ 2ì¼)

### ì‹œë‚˜ë¦¬ì˜¤ B: ë‹¨ê³„ì  ì ‘ê·¼
1. **ì˜µì…˜ 1 Phase 1** (4-6ì‹œê°„) â†’ ë°°í¬ & í”¼ë“œë°±
2. **ì˜µì…˜ 2 Phase 1** (5-8ì‹œê°„) â†’ ë°°í¬ & í”¼ë“œë°±
3. Phase 2 ìµœì í™”ë“¤

**ì´ ì†Œìš” ì‹œê°„:** 1ì£¼ì¼ (ì—¬ìœ  ìˆëŠ” ì¼ì •)

---

## ê²€ì¦ ê³„íš

### ì˜µì…˜ 1 ê²€ì¦:
```bash
# 1. í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸
POST /pipeline/import-from-notion?page_size=100
â†’ 100ê°œ ì´ìƒ ìˆ˜ì§‘ í™•ì¸

# 2. ë³¸ë¬¸ ìˆ˜ì§‘ í™•ì¸
SELECT content FROM raw_notes WHERE content IS NOT NULL;

# 3. Rate limiting í™•ì¸
â†’ ë¡œê·¸ì—ì„œ 429 ì—ëŸ¬ ì—†ìŒ í™•ì¸
```

### ì˜µì…˜ 2 ê²€ì¦:
```bash
# 1. 3-pair ìƒì„± í…ŒìŠ¤íŠ¸
POST /pipeline/select-pairs?cluster_size=3

# 2. DB í™•ì¸
SELECT * FROM thought_clusters;
SELECT * FROM cluster_thoughts;

# 3. Essay ìƒì„± (3ê°œ thought)
POST /pipeline/generate-essays

# 4. í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œ í™•ì¸
â†’ used_thoughts_jsonì— 3ê°œ í•­ëª© í‘œì‹œ
```

---

## ë³µì¡ë„ ìƒì„¸ ë¶„ì„: 2-pair â†’ N-pair

### ìˆ˜í•™ì  ê¸°ì´ˆ: ì¡°í•© ê³µì‹

**C(n, k) = n! / (k! Ã— (n-k)!)**

- n = ì „ì²´ thought ê°œìˆ˜
- k = ì„ íƒí•  thought ê°œìˆ˜ (pair size)

---

### ì‹¤ì œ ë°ì´í„° ê·œëª¨ë³„ ë³µì¡ë„

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ì†Œê·œëª¨ (n=10, í˜„ì¬ ìƒíƒœ)

| Pair Size | ì¡°í•© ìˆ˜ | ì¦ê°€ìœ¨ | LLM ë¹„ìš© | ì†Œìš” ì‹œê°„ |
|-----------|---------|--------|----------|-----------|
| 2-pair    | 45      | -      | $0.14    | 1ë¶„       |
| 3-pair    | 120     | 2.67x  | $0.36    | 2ë¶„       |
| 4-pair    | 210     | 1.75x  | $0.63    | 3ë¶„       |
| 5-pair    | 252     | 1.20x  | $0.76    | 4ë¶„       |

**ê²°ë¡ :** n=10ì¼ ë•ŒëŠ” ë¬¸ì œ ì—†ìŒ

---

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ê·œëª¨ (n=50)

| Pair Size | ì¡°í•© ìˆ˜    | ì¦ê°€ìœ¨ | LLM ë¹„ìš© | ì†Œìš” ì‹œê°„ |
|-----------|-----------|--------|----------|-----------|
| 2-pair    | 1,225     | -      | $3.68    | 5ë¶„       |
| 3-pair    | 19,600    | 16x    | $58.80   | 1ì‹œê°„     |
| 4-pair    | 230,300   | 11.8x  | $690.90  | 12ì‹œê°„    |
| 5-pair    | 2,118,760 | 9.2x   | $6,356   | 5ì¼       |

**ê²°ë¡ :** 3-pairê¹Œì§€ ì‹¤ìš© ê°€ëŠ¥, 4-pairë¶€í„° ë¹„í˜„ì‹¤ì 

---

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ê·œëª¨ (n=100, ëª©í‘œ ê·œëª¨)

| Pair Size | ì¡°í•© ìˆ˜      | ì¦ê°€ìœ¨ | LLM ë¹„ìš©  | ì†Œìš” ì‹œê°„ |
|-----------|-------------|--------|-----------|-----------|
| 2-pair    | 4,950       | -      | $14.85    | 10ë¶„      |
| 3-pair    | 161,700     | 32.7x  | $485.10   | 5ì‹œê°„     |
| 4-pair    | 3,921,225   | 24.3x  | $11,763   | 5ì¼       |
| 5-pair    | 75,287,520  | 19.2x  | $225,863  | 3ê°œì›”     |

**ê²°ë¡ :**
- 2-pair: âœ… ì‹¤ìš©ì  (í˜„ì¬ êµ¬í˜„)
- 3-pair: âš ï¸ í•œê³„ì„  (í•„í„°ë§ í•„ìˆ˜)
- 4-pair ì´ìƒ: âŒ ë¶ˆê°€ëŠ¥

**ë¹„ìš© ê³„ì‚° ê°€ì •:**
- Claude 3.5 Sonnet: $3/M input, $15/M output
- í‰ê°€ 1íšŒ = 500 tokens input + 100 tokens output
- í‰ê°€ 1íšŒë‹¹ ë¹„ìš© = $0.003

---

### í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ (Hybrid Strategy) ìƒì„¸ ì„¤ëª…

#### ê°œë…

**ìˆœìˆ˜ ì¡°í•© ë°©ì‹ (Naive):**
```
ëª¨ë“  C(n, k) ì¡°í•©ì„ ìƒì„± â†’ LLM í‰ê°€ â†’ ìƒìœ„ Nê°œ ì„ íƒ
ë¬¸ì œ: k=3, n=100ì¼ ë•Œ 161,700ê°œ í‰ê°€ í•„ìš”
```

**í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (Hybrid):**
```
Step 1: ê¸°ì¡´ 2-pair ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¢‹ì€ í˜ì–´ ì„ íƒ (ê²€ì¦ëœ ë¡œì§)
Step 2: ê° í˜ì–´ì— 3ë²ˆì§¸ thoughtë§Œ ì¶”ê°€ (ì ì§„ì  í™•ì¥)
ì´ì : ë³µì¡ë„ O(nÂ³) â†’ O(nÂ²)
```

---

#### ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

**Phase 1: 2-pair ì„ íƒ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)**
```python
# 1. pgvectorë¡œ ìœ ì‚¬ë„ ë²”ìœ„ í•„í„°ë§ (0.05-0.35)
candidates = find_candidate_pairs(min_sim=0.05, max_sim=0.35)
# ê²°ê³¼: ì•½ 500-1000 í˜ì–´

# 2. LLMìœ¼ë¡œ ì°½ì˜ì„± í‰ê°€
scored_pairs = score_pairs(candidates)
# í”„ë¡¬í”„íŠ¸: "ë‘ ì•„ì´ë””ì–´ì˜ ì°½ì˜ì  ì—°ê²° ê°€ëŠ¥ì„±"

# 3. ìƒìœ„ 50ê°œ ì„ íƒ (threshold >= 75ì )
top_pairs = select_top(scored_pairs, top_n=50, min_score=75)
```

**Phase 2: 3ë²ˆì§¸ thought ì¶”ê°€ (ìƒˆë¡œìš´ ë¡œì§)**
```python
triplets = []

for pair in top_pairs:  # 50ë²ˆ ë°˜ë³µ
    # í˜ì–´ì™€ "ì•½í•œ ì—°ê²°"ëœ 3ë²ˆì§¸ thought ì°¾ê¸°
    third_candidates = find_complementary_thought(
        pair.thought_a_id,
        pair.thought_b_id,
        min_sim=0.05,  # í˜ì–´ì™€ ë‚®ì€ ìœ ì‚¬ë„
        max_sim=0.35,
        limit=30       # ìƒìœ„ 30ê°œë§Œ (ë³µì¡ë„ ì œí•œ)
    )

    # 50 pairs Ã— 30 candidates = 1,500 triplets

    # LLMìœ¼ë¡œ 3ê°œ ì¡°í•© í‰ê°€
    for third in third_candidates:
        triplet = {
            "thought_ids": [pair.a_id, pair.b_id, third.id],
            "thoughts": [pair.a, pair.b, third],
            "avg_similarity": calculate_avg_sim([pair.a, pair.b, third])
        }

        score = score_triplet(triplet)
        # í”„ë¡¬í”„íŠ¸: "ì„¸ ì•„ì´ë””ì–´ì˜ ì°½ì˜ì  ì¡°í•© ê°€ëŠ¥ì„±"

        if score >= 70:
            triplets.append(triplet)

# ê²°ê³¼: ì•½ 100-200ê°œ ê³ í’ˆì§ˆ 3-pair
```

**Phase 3: ìµœì¢… ì„ íƒ**
```python
# LLM í‰ê°€ ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ Nê°œ
final_clusters = select_top(triplets, top_n=10)
```

---

#### ë³µì¡ë„ ë¹„êµ

**ìˆœìˆ˜ ì¡°í•© (Naive):**
```
C(100, 3) = 161,700
LLM í˜¸ì¶œ: 161,700íšŒ
ë¹„ìš©: $485
ì‹œê°„: 5ì‹œê°„
```

**í•˜ì´ë¸Œë¦¬ë“œ (Hybrid):**
```
Phase 1: C(100, 2) = 4,950 (ê¸°ì¡´)
Phase 2: 50 pairs Ã— 30 candidates = 1,500

ì´ LLM í˜¸ì¶œ: 4,950 + 1,500 = 6,450
ë¹„ìš©: $19.35
ì‹œê°„: 30ë¶„

ë³µì¡ë„ ê°ì†Œ: 96% â†“ (161,700 â†’ 6,450)
```

---

#### í•˜ì´ë¸Œë¦¬ë“œ ì „ëµì˜ ì¥ì 

**1. ë³µì¡ë„ ì œì–´**
- O(nÂ³) â†’ O(nÂ² + kn) where k=50 (ìƒìˆ˜)
- n=200ê¹Œì§€ í™•ì¥ ê°€ëŠ¥

**2. í’ˆì§ˆ ë³´ì¥**
- Phase 1ì—ì„œ ì´ë¯¸ ê²€ì¦ëœ ì¢‹ì€ í˜ì–´ ê¸°ë°˜
- "ì¢‹ì€ 2ê°œ + ë³´ì™„ì  1ê°œ" êµ¬ì¡°ë¡œ ì°½ì˜ì„± ê·¹ëŒ€í™”

**3. ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©**
- 2-pair ì•Œê³ ë¦¬ì¦˜ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²€ì¦ë¨)
- ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½

**4. ì ì§„ì  í™•ì¥ ê°€ëŠ¥**
- 3-pair â†’ 4-pair í™•ì¥ ì‹œì—ë„ ë™ì¼ íŒ¨í„´ ì ìš©
- 4-pair: 50 pairs Ã— 20 thirds Ã— 10 fourths = 10,000 (vs 3.9M)

---

#### ì™œ "í•˜ì´ë¸Œë¦¬ë“œ"ì¸ê°€?

**"Naive Combination" + "Greedy Extension"ì˜ í•˜ì´ë¸Œë¦¬ë“œ**

1. **Naive ë¶€ë¶„ (Phase 1):**
   - ëª¨ë“  C(n,2) ì¡°í•© ìƒì„± (ì™„ì „ íƒìƒ‰)
   - í’ˆì§ˆ ë³´ì¥

2. **Greedy ë¶€ë¶„ (Phase 2):**
   - ì¢‹ì€ í˜ì–´ì—ë§Œ 3ë²ˆì§¸ ì¶”ê°€ (íƒìš•ì  ì„ íƒ)
   - ë³µì¡ë„ ì ˆê°

**ê²°ê³¼:** í’ˆì§ˆì€ ìœ ì§€í•˜ë©´ì„œ ë³µì¡ë„ë§Œ ê°ì†Œ

---

### ë³µì¡ë„ ì™„í™” ì „ëµ ë¹„êµ

| ì „ëµ | ë³µì¡ë„ ê°ì†Œ | í’ˆì§ˆ | êµ¬í˜„ ë‚œì´ë„ | ì¶”ì²œë„ |
|------|-------------|------|-------------|--------|
| **í•˜ì´ë¸Œë¦¬ë“œ** | 96% â†“ | â­â­â­â­â­ | ì¤‘ê°„ | â­â­â­â­â­ |
| Pre-filtering | 97% â†“ | â­â­â­â­ | ì‰¬ì›€ | â­â­â­â­ |
| Random Sampling | 97% â†“ | â­â­ | ì‰¬ì›€ | â­â­ |
| K-means Clustering | 99% â†“ | â­â­â­ | ì–´ë ¤ì›€ | â­â­â­ |

**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµì´ ìµœì„ ì¸ ì´ìœ :**
- í’ˆì§ˆê³¼ ë³µì¡ë„ ëª¨ë‘ ìš°ìˆ˜
- ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- ì ì§„ì  í™•ì¥ ê°€ëŠ¥

---

### N-pair í™•ì¥ í•œê³„

**ì¼ë°˜ ê³µì‹ (Naive):**
```
k=2: C(n,2) = nÂ²/2
k=3: C(n,3) = nÂ³/6
k=4: C(n,4) = nâ´/24
k=5: C(n,5) = nâµ/120
```

**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ (ì‹¤ì œ ë³µì¡ë„):**
```
k=2: O(nÂ²)           â†’ 4,950 (n=100)
k=3: O(nÂ²)           â†’ 6,450 (n=100)
k=4: O(nÂ² + n)       â†’ 10,000 ì¶”ì •
k=5: O(nÂ² + nÂ²)      â†’ 25,000 ì¶”ì •
```

**ê¶Œì¥ ì‚¬í•­:**
- âœ… **3-pairê¹Œì§€ë§Œ êµ¬í˜„** (ì¶©ë¶„íˆ ì°½ì˜ì )
- âš ï¸ 4-pairëŠ” ì‹ ì¤‘íˆ ê²€í†  (í•„ìš”ì„± ì˜ë¬¸)
- âŒ 5-pair ì´ìƒì€ ë¹„ì¶”ì²œ (LLMë„ í˜¼ë€)

---

### ì‹¤ì „ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

**ëª©í‘œ: Notion ë©”ëª¨ 100ê°œ â†’ Essay ìƒì„±**

```
Step 1: Notion Import (ì˜µì…˜ 1)
â†’ 100ê°œ í˜ì´ì§€ ìˆ˜ì§‘

Step 2: Extract Thoughts
â†’ 100 pages Ã— í‰ê·  2 thoughts = 200 thoughts

Step 3: Select 3-pair Clusters (ì˜µì…˜ 2 í•˜ì´ë¸Œë¦¬ë“œ)
â†’ Phase 1: C(200,2) = 19,900 (2-pair í‰ê°€)
â†’ Phase 2: 50 pairs Ã— 30 = 1,500 (3-pair í‰ê°€)
â†’ ì´ ë¹„ìš©: $64
â†’ ì†Œìš” ì‹œê°„: 1ì‹œê°„

Step 4: Generate Essays
â†’ ìƒìœ„ 10ê°œ cluster ì„ íƒ
â†’ 10ê°œ Essay ìƒì„±
```

**ê²°ê³¼:**
- 10ê°œ ê³ í’ˆì§ˆ Essay (3ê°œ thought ì¡°í•©)
- ì´ ë¹„ìš©: $70 (Notion 200ê°œ ê¸°ì¤€)
- ì´ ì‹œê°„: 2ì‹œê°„

---

## ìµœì¢… ì¶”ì²œ

**ì¶”ì²œ ìˆœì„œ: ì˜µì…˜ 1 â†’ ì˜µì…˜ 2 (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)**

**ì´ìœ :**
1. **ì˜µì…˜ 1**ì€ ì¦‰ì‹œ ê°€ì¹˜ ì œê³µ (ë” ë§ì€ ë©”ëª¨ â†’ ë” ë§ì€ Essay)
2. **ì˜µì…˜ 2**ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìœ¼ë¡œ ë³µì¡ë„ ë¬¸ì œ í•´ê²°
3. ë°ì´í„°ê°€ ì¶©ë¶„í•´ì•¼ N-pair íš¨ê³¼ ê·¹ëŒ€í™”
4. ì˜µì…˜ 1ë¡œ ë°ì´í„° í™•ë³´ í›„ ì˜µì…˜ 2 ì‹¤í—˜ì´ ì•ˆì „

**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì±„íƒ ì´ìœ :**
- ë³µì¡ë„ 96% ê°ì†Œ (161K â†’ 6.5K)
- í’ˆì§ˆ ë³´ì¥ (ì¢‹ì€ 2-pair ê¸°ë°˜)
- ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš© (ì•ˆì •ì„±)
- 3-pair êµ¬í˜„ìœ¼ë¡œ ì¶©ë¶„í•œ ì°½ì˜ì„±

**ë‹¨, ë™ì‹œ ì§„í–‰ ê°€ëŠ¥:**
- ë‘ ì˜µì…˜ì´ ì„œë¡œ ë…ë¦½ì  (DB ì¶©ëŒ ì—†ìŒ)
- ë³‘ë ¬ ì‘ì—… ê°€ëŠ¥ (ë³„ë„ ë¸Œëœì¹˜)
