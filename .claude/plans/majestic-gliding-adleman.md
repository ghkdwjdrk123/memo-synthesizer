# Plan: Incremental Import - Fetch Only Changed Pages

## Problem Statement

### Current Implementation Issues
**í˜„ì¬ ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±:**
- ë§¤ë²ˆ **ì „ì²´ 724ê°œ í˜ì´ì§€ë¥¼ 9ë¶„ê°„ ì²˜ë¦¬**
- 731ë²ˆì§¸ í˜ì´ì§€ë¥¼ ì¶”ê°€í•˜ë©´ â†’ ì „ì²´ 725ê°œ ì¬ì²˜ë¦¬
- 500ë²ˆì§¸ í˜ì´ì§€ ìˆ˜ì •í•˜ë©´ â†’ ì „ì²´ 724ê°œ ì¬ì²˜ë¦¬
- **ë³€ê²½ ê°ì§€ ì—†ìŒ**: ìˆ˜ì • ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë¬´ì¡°ê±´ ì „ì²´ fetch

**ì„±ëŠ¥ ë¬¸ì œ:**
- Import ì‹œê°„: 9ë¶„ (724 pages Ã— 3 req/sec rate limit)
- API í˜¸ì¶œ ë‚­ë¹„: ëŒ€ë¶€ë¶„ì˜ í˜ì´ì§€ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŒ
- DB ì“°ê¸° ë¶€í•˜: ë¶ˆí•„ìš”í•œ upsert ë°˜ë³µ

### User's Proposed Solution (Incremental Update)

**ê°œì„  ì œì•ˆ í”„ë¡œì„¸ìŠ¤:**
```
1. ë¶€ëª¨ í˜ì´ì§€ì—ì„œ í•˜ìœ„ í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (lightweight)
   â†“
2. ê° í˜ì´ì§€ì˜ notion_last_edited_time í™•ì¸
   â†“
3. DBì˜ raw_notes í…Œì´ë¸”ê³¼ ë¹„êµ
   â†“
4. ë³€ê²½ ê°ì§€:
   - ì‹ ê·œ í˜ì´ì§€ (DBì— ì—†ìŒ) â†’ fetch ëŒ€ìƒ
   - ìˆ˜ì •ëœ í˜ì´ì§€ (last_edited_time ë‹¤ë¦„) â†’ fetch ëŒ€ìƒ
   - ë¯¸ìˆ˜ì • í˜ì´ì§€ (last_edited_time ê°™ìŒ) â†’ SKIP
   â†“
5. fetch ëŒ€ìƒ í˜ì´ì§€ë§Œ API í˜¸ì¶œí•˜ì—¬ content ê°€ì ¸ì˜¤ê¸°
   â†“
6. ëŒ€ìƒ í˜ì´ì§€ë§Œ upsert
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì´ˆê¸° import: 724ê°œ ì „ì²´ (9ë¶„) - ë¶ˆê°€í”¼
- 1ê°œ ì¶”ê°€ ì‹œ: 1ê°œë§Œ fetch (<1ì´ˆ)
- 10ê°œ ìˆ˜ì • ì‹œ: 10ê°œë§Œ fetch (~3ì´ˆ)
- 100% ì ˆê°: ë³€ê²½ ì—†ì„ ì‹œ 0ê°œ fetch (ì¦‰ì‹œ ì™„ë£Œ)

---

## Technical Analysis

### Phase 1: Notion API Metadata Fetching

**í˜„ì¬ ì½”ë“œ ë¶„ì„:**
```python
# backend/services/notion_service.py:324
async def fetch_child_pages_from_parent(...) -> List[Dict]:
    """Fetch all child pages from parent page."""
    pages = []
    has_more = True
    start_cursor = None

    while has_more:
        response = await self.client.blocks.children.list(
            block_id=parent_page_id,
            page_size=page_size,
            start_cursor=start_cursor
        )
        pages.extend(response.get("results", []))
        # ...

    return pages  # âœ… ì´ë¯¸ metadata í¬í•¨ (id, last_edited_time, properties)
```

**í˜„ì¬ ë°˜í™˜ ë°ì´í„° êµ¬ì¡°:**
```json
{
  "id": "abc-123",
  "created_time": "2024-01-01T12:00:00.000Z",
  "last_edited_time": "2024-01-15T14:30:00.000Z",  // âœ… ë¹„êµ í‚¤
  "properties": {
    "ì œëª©": "í˜ì´ì§€ ì œëª©",
    // ... ê¸°íƒ€ properties
  },
  "url": "https://notion.so/abc-123"
}
```

**âœ… ê²°ë¡ :**
- Notion APIëŠ” ì´ë¯¸ `last_edited_time`ì„ ë°˜í™˜
- ì¶”ê°€ API í˜¸ì¶œ ë¶ˆí•„ìš”
- ë©”íƒ€ë°ì´í„°ëŠ” lightweight (content ì—†ìŒ)

---

### Phase 2: Database Comparison Strategy

**DB ìŠ¤í‚¤ë§ˆ:**
```sql
CREATE TABLE raw_notes (
    id UUID PRIMARY KEY,
    notion_page_id TEXT UNIQUE NOT NULL,  -- âœ… ë¹„êµ í‚¤
    notion_last_edited_time TIMESTAMPTZ NOT NULL,  -- âœ… ë¹„êµ í‚¤
    content TEXT,
    properties_json JSONB,
    imported_at TIMESTAMPTZ DEFAULT NOW()
);
```

**ë¹„êµ ì¿¼ë¦¬ ì„¤ê³„:**

**Option A: Single Query with LEFT JOIN (ì¶”ì²œ)**
```python
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict]
) -> tuple[List[str], List[str]]:
    """
    Returns (new_page_ids, updated_page_ids).

    Args:
        notion_pages: List of page metadata from Notion API

    Returns:
        new_page_ids: Pages not in DB
        updated_page_ids: Pages in DB but last_edited_time differs
    """
    page_map = {
        p["id"]: datetime.fromisoformat(p["last_edited_time"].replace("Z", "+00:00"))
        for p in notion_pages
    }

    # Fetch existing pages from DB
    existing = await self.client.table("raw_notes").select(
        "notion_page_id, notion_last_edited_time"
    ).in_("notion_page_id", list(page_map.keys())).execute()

    existing_map = {
        row["notion_page_id"]: row["notion_last_edited_time"]
        for row in existing.data
    }

    new_page_ids = []
    updated_page_ids = []

    for page_id, notion_time in page_map.items():
        if page_id not in existing_map:
            new_page_ids.append(page_id)  # ì‹ ê·œ
        elif existing_map[page_id] != notion_time:
            updated_page_ids.append(page_id)  # ìˆ˜ì •ë¨
        # else: ë³€ê²½ ì—†ìŒ â†’ SKIP

    return new_page_ids, updated_page_ids
```

**ì„±ëŠ¥:**
- DB ì¿¼ë¦¬: 1íšŒ (WHERE IN ì‚¬ìš©)
- ë©”ëª¨ë¦¬: O(n) - 724ê°œ í˜ì´ì§€ ë©”íƒ€ë°ì´í„°ë§Œ
- ì‹œê°„ ë³µì¡ë„: O(n) - ë‹¨ìˆœ ë¹„êµ

**Option B: Batch Query (ëŒ€ì•ˆ)**
```sql
-- PostgreSQLì˜ ê²½ìš°
WITH notion_data(page_id, edited_time) AS (
    VALUES
        ('id1', '2024-01-15 14:30:00+00'::timestamptz),
        ('id2', '2024-01-15 15:00:00+00'::timestamptz),
        ...
)
SELECT
    nd.page_id,
    CASE
        WHEN rn.notion_page_id IS NULL THEN 'new'
        WHEN rn.notion_last_edited_time < nd.edited_time THEN 'updated'
        ELSE 'unchanged'
    END as status
FROM notion_data nd
LEFT JOIN raw_notes rn ON rn.notion_page_id = nd.page_id;
```

**âœ… ì¶”ì²œ:** Option A (Python ë ˆë²¨ ë¹„êµ)
- ì´ìœ : Supabase Python SDKì™€ í˜¸í™˜ì„± ì¢‹ìŒ
- SQLë³´ë‹¤ ë””ë²„ê¹… ì‰¬ì›€
- ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ (724ê°œ ì •ë„)

---

### Phase 3: Selective Content Fetching

**í˜„ì¬ ë¡œì§ (ì „ì²´ fetch):**
```python
# backend/routers/pipeline.py:119-157
for idx, page in enumerate(pages, 1):  # 724ë²ˆ ë°˜ë³µ
    page_id = page.get("id")
    fetched_content = await _fetch_page_with_retry(notion_service, page_id, max_retries=3)
    # ... upsert
```

**ê°œì„  ë¡œì§ (ì„ íƒì  fetch):**
```python
# 1. ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (lightweight)
all_pages = await notion_service.fetch_child_pages_from_parent(...)

# 2. ë³€ê²½ ê°ì§€
new_ids, updated_ids = await supabase_service.get_pages_to_fetch(all_pages)
fetch_targets = new_ids + updated_ids

logger.info(
    f"Change detection: {len(new_ids)} new, {len(updated_ids)} updated, "
    f"{len(all_pages) - len(fetch_targets)} unchanged (skipped)"
)

# 3. ëŒ€ìƒë§Œ fetch (ë³€ê²½ëœ í˜ì´ì§€ë§Œ)
for page in all_pages:
    page_id = page.get("id")

    if page_id not in fetch_targets:
        await supabase_service.increment_job_progress(job_id, skipped=True)
        continue  # âœ… SKIP

    # Fetch content only for changed pages
    fetched_content = await _fetch_page_with_retry(notion_service, page_id, max_retries=3)
    # ... upsert
    await supabase_service.increment_job_progress(job_id, imported=True)
```

**ì„±ëŠ¥ ë¹„êµ:**

| ì‹œë‚˜ë¦¬ì˜¤ | í˜„ì¬ ë°©ì‹ | ê°œì„  ë°©ì‹ | ì ˆê°ìœ¨ |
|---------|----------|----------|-------|
| ì´ˆê¸° import (724ê°œ ì „ì²´ ì‹ ê·œ) | 9ë¶„ | 9ë¶„ | 0% |
| 1ê°œ ì¶”ê°€ | 9ë¶„ | <1ì´ˆ | 99.8% |
| 10ê°œ ìˆ˜ì • | 9ë¶„ | ~3ì´ˆ | 99.4% |
| 100ê°œ ìˆ˜ì • | 9ë¶„ | ~30ì´ˆ | 94.4% |
| ë³€ê²½ ì—†ìŒ (ì¬ì‹¤í–‰) | 9ë¶„ | <1ì´ˆ | 99.9% |

---

## Implementation Plan

### Phase 1: Add Change Detection Method (30 min)

**File:** `backend/services/supabase_service.py` (ADD after line 880)

**New Method:**
```python
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict[str, Any]]
) -> tuple[List[str], List[str]]:
    """
    Compare Notion pages with DB to detect changes.

    Args:
        notion_pages: List of page metadata from Notion API
            Each page must have: id, last_edited_time

    Returns:
        Tuple of (new_page_ids, updated_page_ids)
        - new_page_ids: Pages not in raw_notes table
        - updated_page_ids: Pages with different last_edited_time

    Example:
        >>> pages = [{"id": "abc", "last_edited_time": "2024-01-15T14:30:00.000Z"}]
        >>> new, updated = await service.get_pages_to_fetch(pages)
        >>> print(f"New: {len(new)}, Updated: {len(updated)}")
    """
    await self._ensure_initialized()

    # Build map: page_id -> last_edited_time (from Notion)
    page_map = {}
    for p in notion_pages:
        page_id = p.get("id")
        last_edited = p.get("last_edited_time")

        if not page_id or not last_edited:
            logger.warning(f"Page missing id or last_edited_time: {p}")
            continue

        # Parse ISO 8601 timestamp
        notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
        page_map[page_id] = notion_time

    if not page_map:
        logger.warning("No valid pages to check")
        return [], []

    # Fetch existing pages from DB
    try:
        response = await (
            self.client.table("raw_notes")
            .select("notion_page_id, notion_last_edited_time")
            .in_("notion_page_id", list(page_map.keys()))
            .execute()
        )

        existing_map = {
            row["notion_page_id"]: row["notion_last_edited_time"]
            for row in response.data
        }

    except Exception as e:
        logger.error(f"Failed to fetch existing pages: {e}")
        # On error, treat all as new (safe fallback)
        return list(page_map.keys()), []

    # Compare timestamps
    new_page_ids = []
    updated_page_ids = []

    for page_id, notion_time in page_map.items():
        if page_id not in existing_map:
            new_page_ids.append(page_id)
        else:
            db_time = existing_map[page_id]

            # Parse DB timestamp (may be string or datetime)
            if isinstance(db_time, str):
                db_time = datetime.fromisoformat(db_time.replace("Z", "+00:00"))

            # Compare (notion_time > db_time means updated)
            if notion_time > db_time:
                updated_page_ids.append(page_id)
            # else: unchanged, skip

    logger.info(
        f"Change detection: {len(new_page_ids)} new, {len(updated_page_ids)} updated, "
        f"{len(page_map) - len(new_page_ids) - len(updated_page_ids)} unchanged"
    )

    return new_page_ids, updated_page_ids
```

**Estimated Lines:** ~70 lines

---

### Phase 2: Update Background Task Logic (45 min)

**File:** `backend/routers/pipeline.py` (MODIFY lines 118-157)

**Changes:**

**Before:**
```python
# Process each page with content fetching (RESTORED LOGIC)
for idx, page in enumerate(pages, 1):
    page_id = page.get("id")
    try:
        fetched_content = await _fetch_page_with_retry(notion_service, page_id, max_retries=3)
        # ... rest of logic
```

**After:**
```python
# Process each page with INCREMENTAL content fetching
# 1. Detect changes
new_page_ids, updated_page_ids = await supabase_service.get_pages_to_fetch(pages)
fetch_targets = set(new_page_ids + updated_page_ids)

logger.info(
    f"[Job {job_id}] Incremental import: "
    f"{len(new_page_ids)} new, {len(updated_page_ids)} updated, "
    f"{len(pages) - len(fetch_targets)} unchanged (will skip)"
)

# 2. Process only changed pages
for idx, page in enumerate(pages, 1):
    page_id = page.get("id")

    try:
        # Skip unchanged pages
        if page_id not in fetch_targets:
            logger.info(f"[Job {job_id}] [{idx}/{total_count}] â­ï¸  Skipped (unchanged): {page_id}")
            await supabase_service.increment_job_progress(job_id, skipped=True)
            continue

        # Fetch content only for new/updated pages
        fetched_content = None
        if mode == "parent_page":
            try:
                fetched_content = await _fetch_page_with_retry(notion_service, page_id, max_retries=3)
                # ... rest of existing logic (title fallback, etc.)
```

**Key Changes:**
1. Call `get_pages_to_fetch()` before loop
2. Convert to set for O(1) lookup
3. Add skip logic with logging
4. Update progress counters (skipped)

**Estimated Lines:** +15 lines, modified 5 lines

---

### Phase 3: Update Progress Tracking (15 min)

**File:** `backend/services/supabase_service.py` (MODIFY line 850)

**Current Signature:**
```python
async def increment_job_progress(
    self,
    job_id: str,
    imported: bool = False,
    failed_page: Optional[Dict[str, str]] = None
) -> None:
```

**Updated Signature:**
```python
async def increment_job_progress(
    self,
    job_id: str,
    imported: bool = False,
    skipped: bool = False,  # âœ… NEW
    failed_page: Optional[Dict[str, str]] = None
) -> None:
    """Increment job progress. Never raises exceptions."""
    try:
        job = await self.get_import_job(job_id)
        updates = {"processed_pages": job["processed_pages"] + 1}

        if imported:
            updates["imported_pages"] = job["imported_pages"] + 1
        if skipped:
            updates["skipped_pages"] = job["skipped_pages"] + 1  # âœ… NEW
        if failed_page:
            current_failed = job.get("failed_pages", [])
            current_failed.append(failed_page)
            updates["failed_pages"] = current_failed

        await self.client.table("import_jobs").update(updates).eq("id", job_id).execute()

    except Exception as e:
        logger.error(f"Failed to increment job {job_id} progress: {e}")
```

**Estimated Lines:** +3 lines modified

---

### Phase 4: Update Job Status Response (10 min)

**File:** `backend/routers/pipeline.py` (MODIFY get_import_status endpoint)

**Add to response:**
```python
return ImportJobStatus(
    job_id=job["id"],
    status=job["status"],
    # ... existing fields ...
    skipped_pages=job.get("skipped_pages", 0),  # âœ… Already exists
    # New computed fields:
    new_pages=job.get("new_pages", 0),  # Optional: track separately
    updated_pages=job.get("updated_pages", 0),  # Optional
)
```

**Note:** `skipped_pages` already exists in current schema, no changes needed.

---

## Testing Strategy

### Test Case 1: Initial Import (No Existing Data)
**Scenario:** First time importing all pages

**Expected Behavior:**
- All 724 pages marked as "new"
- All 724 pages fetched
- Time: ~9 minutes
- DB: 724 rows inserted

**Test Command:**
```bash
curl -X POST "http://localhost:8000/pipeline/import-from-notion?page_size=100"
# Wait for completion
# Verify: 724 imported, 0 skipped
```

---

### Test Case 2: Re-import Without Changes
**Scenario:** Run import again immediately (no edits in Notion)

**Expected Behavior:**
- 0 pages new
- 0 pages updated
- 724 pages skipped
- Time: <5 seconds (no API calls to fetch_page_blocks)
- DB: No writes

**Test Command:**
```bash
# Run import again
curl -X POST "http://localhost:8000/pipeline/import-from-notion?page_size=100"
# Verify: 0 imported, 724 skipped
```

**Verification:**
```python
# Check job status
response = requests.get(f"http://localhost:8000/pipeline/import-status/{job_id}")
assert response.json()["imported_pages"] == 0
assert response.json()["skipped_pages"] == 724
```

---

### Test Case 3: Single Page Added
**Scenario:** Add 1 new page in Notion, then import

**Setup:**
1. Manually create new page in Notion parent
2. Wait 1 minute (Notion indexing)
3. Run import

**Expected Behavior:**
- 1 page new
- 0 pages updated
- 724 pages skipped
- Time: <2 seconds (1 API call)
- DB: 1 row inserted

**Verification:**
```sql
SELECT COUNT(*) FROM raw_notes;  -- Should be 725
```

---

### Test Case 4: Multiple Pages Updated
**Scenario:** Edit 10 existing pages in Notion

**Setup:**
1. Manually edit content of 10 pages
2. Wait 1 minute
3. Run import

**Expected Behavior:**
- 0 pages new
- 10 pages updated
- 715 pages skipped (725 - 10)
- Time: ~3 seconds (10 API calls)
- DB: 10 rows updated

**Verification:**
```python
# Check last imported_at timestamps
response = supabase.table("raw_notes").select("notion_page_id, imported_at").order("imported_at", desc=True).limit(10).execute()
# Top 10 should have recent timestamps
```

---

### Test Case 5: Mixed Changes
**Scenario:** 50 new pages, 100 updated pages, 575 unchanged

**Expected Behavior:**
- 50 pages new
- 100 pages updated
- 575 pages skipped
- Time: ~50 seconds (150 API calls)
- DB: 50 inserts, 100 updates

---

## Performance Analysis

### API Call Reduction

**Scenario Analysis:**

| ì‹œë‚˜ë¦¬ì˜¤ | ë³€ê²½ í˜ì´ì§€ | í˜„ì¬ API í˜¸ì¶œ | ê°œì„  í›„ API í˜¸ì¶œ | ì ˆê° |
|---------|-----------|------------|---------------|-----|
| ì´ˆê¸° import | 724 | 724 | 724 | 0% |
| 1ê°œ ì¶”ê°€ | 1 | 724 | 1 | 99.86% |
| 10ê°œ ìˆ˜ì • | 10 | 724 | 10 | 98.62% |
| 100ê°œ ìˆ˜ì • | 100 | 724 | 100 | 86.19% |
| ë³€ê²½ ì—†ìŒ | 0 | 724 | 0 | 100% |

**Rate Limit ì˜í–¥:**
- Notion API: 3 req/sec
- 1ê°œ ì¶”ê°€ ì‹œ: 0.33ì´ˆ vs 241ì´ˆ (728ë°° ë¹ ë¦„)
- 10ê°œ ìˆ˜ì • ì‹œ: 3.33ì´ˆ vs 241ì´ˆ (72ë°° ë¹ ë¦„)

---

### Database Query Efficiency

**Current:**
- 724 upsert queries (ë§¤ë²ˆ)

**Optimized:**
- 1 SELECT query (change detection)
- N upsert queries (N = ë³€ê²½ëœ í˜ì´ì§€ ìˆ˜ë§Œ)

**Example:**
```
10ê°œ ë³€ê²½ ì‹œ:
- 1 SELECT (724ê°œ page_id IN ì¿¼ë¦¬) ~50ms
- 10 UPSERT ~200ms
Total: ~250ms vs í˜„ì¬ 724 upsert (~1500ms)
```

---

## Critical Issues & Deep Analysis

### ğŸš¨ Issue 1: Microseconds Precision Mismatch
**Problem Discovery:**
- Notion API: `"2023-02-28T14:29:00.000Z"` (milliseconds precision)
- Supabase DB: `"2023-02-28T14:29:00.123456+00:00"` (microseconds precision)
- ë¹„êµ ì‹œ: `notion_time != db_time` â†’ ë¶ˆí•„ìš”í•œ fetch ë°œìƒ!

**ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
```python
notion_time = datetime(2023, 2, 28, 14, 29, 0)          # .000
db_time     = datetime(2023, 2, 28, 14, 29, 0, 123456)  # .123456
notion_time == db_time  # False!
```

**Impact:**
- ë³€ê²½ë˜ì§€ ì•Šì€ í˜ì´ì§€ë„ "updated"ë¡œ ì¸ì‹
- ì¦ë¶„ ì—…ë°ì´íŠ¸ íš¨ê³¼ ê°ì†Œ (worst case: ëª¨ë“  í˜ì´ì§€ ì¬fetch)

**Solution: Truncate to Seconds**
```python
# In get_pages_to_fetch()
def truncate_to_seconds(dt: datetime) -> datetime:
    """Remove microseconds for comparison."""
    return dt.replace(microsecond=0)

# Compare timestamps
notion_time_trunc = truncate_to_seconds(notion_time)
db_time_trunc = truncate_to_seconds(db_time)

if notion_time_trunc > db_time_trunc:  # âœ… Now accurate
    updated_page_ids.append(page_id)
```

**Alternative: Use >= instead of >**
```python
# Less strict: consider "equal" as "unchanged"
if notion_time > db_time:  # Only if Notion timestamp is STRICTLY newer
    updated_page_ids.append(page_id)
```

**Recommended:** Truncate to seconds (more robust)

---

### ğŸš¨ Issue 2: Notion API Block Structure Change
**Problem:**
- `fetch_child_pages_from_parent()` uses `blocks.children.list()`
- Returns `child_page` **blocks**, not full page objects
- `last_edited_time`ì€ **blockì˜ ìˆ˜ì • ì‹œê°„**ì´ì§€ **page contentì˜ ìˆ˜ì • ì‹œê°„**ì´ ì•„ë‹ ìˆ˜ ìˆìŒ!

**Code Analysis (notion_service.py:454-468):**
```python
for block in blocks:
    if block.get("type") == "child_page":
        child_page_id = block.get("id")
        # ...
        page_data = {
            "id": child_page_id,
            "created_time": block.get("created_time"),        # âœ… Block ìƒì„± ì‹œê°„
            "last_edited_time": block.get("last_edited_time"), # âš ï¸ Block ìˆ˜ì • ì‹œê°„
            "properties": {"ì œëª©": child_page_title}
        }
```

**Critical Question:**
- `block.get("last_edited_time")` = block metadata ìˆ˜ì • ì‹œê°„?
- vs. **page content ìˆ˜ì • ì‹œê°„**?

**Test Verification Needed:**
1. Notionì—ì„œ í˜ì´ì§€ Aì˜ ë‚´ìš© ìˆ˜ì •
2. API í˜¸ì¶œí•˜ì—¬ `last_edited_time` í™•ì¸
3. ë‚´ìš© ìˆ˜ì •ì´ `last_edited_time`ì— ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸

**If NOT reflected:**
- ì¦ë¶„ ì—…ë°ì´íŠ¸ê°€ ì‘ë™í•˜ì§€ ì•ŠìŒ
- Content ë³€ê²½ì„ ê°ì§€ ëª»í•¨
- **Plan ì „ì²´ê°€ ë¬´íš¨í™”ë¨**

**Solution if problem exists:**
- Use `pages.retrieve(page_id)` API to get accurate last_edited_time
- Trade-off: 724 API calls (but no content fetch)
- Still faster than fetching content

---

### ğŸš¨ Issue 3: DBì— ì €ì¥ëœ notion_last_edited_timeì˜ ì¶œì²˜
**Problem:**
í˜„ì¬ ì½”ë“œì—ì„œ `notion_last_edited_time`ì„ ì–´ë””ì„œ ê°€ì ¸ì˜¤ëŠ”ì§€ í™•ì¸:

**Current code (pipeline.py:179-181):**
```python
notion_last_edited_time=datetime.fromisoformat(
    page.get("last_edited_time").replace("Z", "+00:00")
),
```

**Key Point:**
- `page`ëŠ” `fetch_child_pages_from_parent()`ì˜ ê²°ê³¼
- ì¦‰, **blockì˜ last_edited_timeì„ ì €ì¥**í•˜ê³  ìˆìŒ
- Content ë³€ê²½ê³¼ ë¬´ê´€í•  ìˆ˜ ìˆìŒ!

**Verification:**
```python
# Test: í˜ì´ì§€ ë‚´ìš© ìˆ˜ì • í›„
pages = await notion_service.fetch_child_pages_from_parent(parent_id)
for page in pages:
    if page["id"] == "modified_page_id":
        print(f"last_edited_time: {page['last_edited_time']}")
        # ë‚´ìš© ìˆ˜ì • ì „í›„ ë¹„êµ
```

---

### ğŸš¨ Issue 4: Timezone Handling Edge Cases
**Problem:**
- Notion API: Always UTC with "Z" suffix
- Supabase TIMESTAMPTZ: May store with different timezone
- Python comparison: timezone-aware vs timezone-naive

**Current Protection:**
```python
notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
```

**Additional Edge Case:**
- DB might return timezone-naive datetime
- Comparison will fail: `TypeError: can't compare offset-naive and offset-aware datetimes`

**Solution:**
```python
from datetime import timezone

def ensure_timezone_aware(dt: datetime) -> datetime:
    """Ensure datetime is timezone-aware (UTC)."""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt

# In get_pages_to_fetch()
notion_time = ensure_timezone_aware(
    datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
)
db_time = ensure_timezone_aware(existing_map[page_id])
```

---

### ğŸš¨ Issue 5: Race Condition in Multi-Stage Comparison
**Problem Scenario:**
1. Import Job A starts: Fetches metadata (724 pages)
2. User modifies Page X in Notion
3. Import Job A: Compares timestamps (Page X not marked as changed)
4. Import Job A: Skips Page X
5. Result: **Page X update missed**

**Time Window:**
- Metadata fetch: ~2 seconds
- Comparison: ~0.1 seconds
- Content fetch loop: ~9 minutes
- **Total window: 9+ minutes** where changes can be missed

**Frequency:**
- Low for manual edits (unlikely during 9-min window)
- High for automated scripts/integrations

**Solution Options:**

**Option A: Accept eventual consistency**
- Next import will catch the change
- Acceptable for most use cases
- Document in README

**Option B: Re-check before skip**
- When about to skip, fetch latest timestamp from Notion
- Trade-off: Extra API call per skipped page
- Defeats purpose of optimization

**Option C: Timestamp cache invalidation**
- Cache metadata timestamps with TTL (e.g., 5 minutes)
- Refresh if cache expired
- Complex implementation

**Recommended:** Option A (document limitation)

---

### ğŸš¨ Issue 6: Pagination Edge Cases in Change Detection
**Problem:**
`fetch_child_pages_from_parent()` uses pagination:
```python
# Batch 1: Pages 1-100
# Batch 2: Pages 101-200
# ...
```

**Edge Case:**
- During pagination, new page added in Notion
- Pagination cursor may skip or duplicate pages
- Known Notion API limitation

**Impact on Change Detection:**
- New page might be missed (not in initial metadata fetch)
- Or: Same page appears twice (cursor shift)

**Current Protection:**
- UNIQUE constraint on `notion_page_id` in DB
- Duplicate insert will be ignored (upsert)

**Additional Protection Needed:**
```python
# Deduplicate before processing
seen_page_ids = set()
unique_pages = []
for page in all_pages:
    page_id = page["id"]
    if page_id not in seen_page_ids:
        seen_page_ids.add(page_id)
        unique_pages.append(page)
    else:
        logger.warning(f"Duplicate page detected during pagination: {page_id}")

pages = unique_pages
```

---

### ğŸš¨ Issue 7: Memory Consumption for Large Sets
**Current Approach:**
```python
# Load all page_ids into memory
existing = await self.client.table("raw_notes").select(
    "notion_page_id, notion_last_edited_time"
).in_("notion_page_id", list(page_map.keys())).execute()
```

**Problem:**
- 724 pages: ~50KB (negligible)
- 10,000 pages: ~700KB (still OK)
- 100,000 pages: ~7MB (concerning)

**Query Limit:**
- Supabase `.in_()` clause has limit (~1000 items in PostgreSQL)
- 724 pages: âœ… Safe
- 10,000 pages: âŒ Will fail

**Solution: Batch Query**
```python
async def get_pages_to_fetch(self, notion_pages: List[Dict]) -> tuple:
    page_map = {...}

    # Batch query in chunks of 1000
    BATCH_SIZE = 1000
    page_ids = list(page_map.keys())
    existing_map = {}

    for i in range(0, len(page_ids), BATCH_SIZE):
        batch_ids = page_ids[i:i+BATCH_SIZE]
        response = await self.client.table("raw_notes").select(
            "notion_page_id, notion_last_edited_time"
        ).in_("notion_page_id", batch_ids).execute()

        for row in response.data:
            existing_map[row["notion_page_id"]] = row["notion_last_edited_time"]

    # ... rest of comparison logic
```

---

### ğŸš¨ Issue 8: Database Index Performance
**Current Schema:**
```sql
CREATE INDEX idx_raw_notes_notion_page_id ON raw_notes(notion_page_id);
```

**Query Pattern:**
```sql
SELECT notion_page_id, notion_last_edited_time
FROM raw_notes
WHERE notion_page_id IN ('id1', 'id2', ..., 'id724');
```

**Performance:**
- 724 pages: < 50ms (fast)
- 10,000 pages: ~500ms (acceptable)
- Index is used efficiently

**Potential Issue:**
- If `notion_page_id` index is not UNIQUE, duplicates may exist
- Query returns multiple rows per page_id
- Comparison logic breaks

**Verification:**
```sql
-- Check for duplicates
SELECT notion_page_id, COUNT(*)
FROM raw_notes
GROUP BY notion_page_id
HAVING COUNT(*) > 1;
```

**If duplicates exist:**
```python
# Handle in query
existing_map = {}
for row in response.data:
    page_id = row["notion_page_id"]
    # Keep most recent
    if page_id not in existing_map:
        existing_map[page_id] = row["notion_last_edited_time"]
    else:
        existing_time = existing_map[page_id]
        new_time = row["notion_last_edited_time"]
        if new_time > existing_time:
            existing_map[page_id] = new_time
```

---

## Edge Cases & Error Handling

### Edge Case 1: Notion API Returns Stale last_edited_time
**Problem:** Notion API ìºì‹œë¡œ ì¸í•´ ìˆ˜ì •í–ˆëŠ”ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì•ˆ ë°”ë€œ

**Solution:**
- Force fetch ì˜µì…˜ ì¶”ê°€ (query parameter)
- `?force=true` ì‹œ ë³€ê²½ ê°ì§€ ë¬´ì‹œí•˜ê³  ì „ì²´ fetch

**Implementation:**
```python
@router.post("/import-from-notion")
async def import_from_notion(
    page_size: int = Query(default=100, ...),
    force: bool = Query(default=False, description="Force fetch all pages (ignore change detection)"),
    ...
):
    # ...
    if force:
        logger.info("Force mode: skipping change detection")
        fetch_targets = set(page["id"] for page in pages)
    else:
        new_ids, updated_ids = await supabase_service.get_pages_to_fetch(pages)
        fetch_targets = set(new_ids + updated_ids)
```

---

### Edge Case 2: DBì—ëŠ” ìˆëŠ”ë° Notionì—ì„œ ì‚­ì œëœ í˜ì´ì§€
**Problem:** í˜ì´ì§€ê°€ Notionì—ì„œ ì‚­ì œë˜ì—ˆëŠ”ë° DBì— ë‚¨ì•„ìˆìŒ

**Solution:**
- Soft delete êµ¬í˜„ (is_deleted flag)
- ë˜ëŠ” ì‚­ì œ ê°ì§€ í›„ DBì—ì„œ ì œê±°

**Implementation (Optional):**
```python
# In _background_import_task()
all_notion_page_ids = set(page["id"] for page in pages)
existing_page_ids = set(existing_map.keys())
deleted_page_ids = existing_page_ids - all_notion_page_ids

if deleted_page_ids:
    logger.info(f"[Job {job_id}] Found {len(deleted_page_ids)} deleted pages")
    # Option A: Soft delete
    await supabase_service.client.table("raw_notes").update({
        "is_deleted": True
    }).in_("notion_page_id", list(deleted_page_ids)).execute()

    # Option B: Hard delete (not recommended)
    # await supabase_service.client.table("raw_notes").delete().in_("notion_page_id", list(deleted_page_ids)).execute()
```

**Schema Change (if soft delete):**
```sql
ALTER TABLE raw_notes ADD COLUMN is_deleted BOOLEAN DEFAULT FALSE;
CREATE INDEX idx_raw_notes_is_deleted ON raw_notes(is_deleted) WHERE is_deleted = FALSE;
```

---

### Edge Case 3: Timezone Handling
**Problem:** Notion uses UTC, Supabase may use different timezone

**Solution:**
- Always normalize to UTC with timezone-aware datetime
- Already implemented in `get_pages_to_fetch()`:
  ```python
  notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
  ```

---

### Edge Case 4: Race Condition (ë™ì‹œ Import)
**Problem:** ë‘ import jobì´ ë™ì‹œì— ì‹¤í–‰ë˜ì–´ ì¤‘ë³µ ì²˜ë¦¬

**Solution:**
- Job locking mechanism (ì´ë¯¸ êµ¬í˜„ë¨ via import_jobs table)
- ë˜ëŠ” check if processing job exists:
  ```python
  # Before creating new job
  active_jobs = await supabase_service.client.table("import_jobs").select("id").eq("status", "processing").execute()
  if active_jobs.data:
      raise HTTPException(400, "Import already in progress")
  ```

---

## Migration Path

### Option 1: Direct Replacement (ì¶”ì²œ)
**Approach:** ê¸°ì¡´ ì½”ë“œë¥¼ ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´

**Pros:**
- ì½”ë“œ ë‹¨ìˆœí™”
- í–¥í›„ ìœ ì§€ë³´ìˆ˜ ì‰¬ì›€
- ëª¨ë“  importê°€ ìë™ìœ¼ë¡œ ìµœì í™”

**Cons:**
- ì´ˆê¸° import ë™ì‘ ë³€ê²½ ì—†ìŒ (ì—¬ì „íˆ ì „ì²´ fetch)

---

### Option 2: Feature Flag
**Approach:** í™˜ê²½ ë³€ìˆ˜ë¡œ ì¦ë¶„/ì „ì²´ ëª¨ë“œ ì„ íƒ

```python
# config.py
INCREMENTAL_IMPORT: bool = Field(default=True, description="Enable incremental import")

# pipeline.py
if settings.incremental_import:
    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(pages)
    fetch_targets = set(new_ids + updated_ids)
else:
    fetch_targets = set(page["id"] for page in pages)  # Fetch all
```

**Pros:**
- ì•ˆì „í•œ ë¡¤ë°±
- A/B í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

**Cons:**
- ë³µì¡ë„ ì¦ê°€
- ë‘ ê²½ë¡œ ëª¨ë‘ ìœ ì§€ë³´ìˆ˜ í•„ìš”

---

## Rollback Plan

**If issues occur:**

1. **Add force parameter:**
   ```bash
   curl -X POST "http://localhost:8000/pipeline/import-from-notion?force=true"
   ```

2. **Revert code:**
   ```bash
   git revert <commit-hash>
   ```

3. **Emergency fix:**
   ```python
   # Temporarily disable change detection
   fetch_targets = set(page["id"] for page in pages)  # Fetch all
   ```

---

## Files to Modify

| File | Type | Changes | Lines |
|------|------|---------|-------|
| `backend/services/supabase_service.py` | MODIFY | Add `get_pages_to_fetch()` method | +70 |
| `backend/routers/pipeline.py` | MODIFY | Update `_background_import_task()` | +15, ~5 modified |
| `backend/services/supabase_service.py` | MODIFY | Update `increment_job_progress()` | +3 |

**Total:** ~90 lines added/modified

---

## Verification Checklist

- [ ] Test Case 1: Initial import (724 pages, all fetched)
- [ ] Test Case 2: Re-import without changes (0 fetched, 724 skipped)
- [ ] Test Case 3: Single page added (1 fetched, 724 skipped)
- [ ] Test Case 4: 10 pages updated (10 fetched, 715 skipped)
- [ ] Test Case 5: Mixed changes (new + updated + unchanged)
- [ ] Performance: Import time reduced by >95% for incremental updates
- [ ] Logs: Show skip counts correctly
- [ ] Job status: `skipped_pages` field accurate
- [ ] Force mode: `?force=true` bypasses change detection
- [ ] Error handling: Invalid timestamps handled gracefully

---

## Timeline Estimate

| Phase | Time | Complexity |
|-------|------|------------|
| Phase 1: Add change detection | 30 min | Low |
| Phase 2: Update background task | 45 min | Medium |
| Phase 3: Update progress tracking | 15 min | Low |
| Phase 4: Testing (5 test cases) | 60 min | Medium |
| **Total** | **2.5 hours** | - |

---

## Expected Benefits

### Quantitative
- **API í˜¸ì¶œ 99% ì ˆê°** (í‰ê·  10ê°œ ë³€ê²½ ì‹œ)
- **Import ì‹œê°„ 98% ë‹¨ì¶•** (9ë¶„ â†’ 3ì´ˆ)
- **DB ì“°ê¸° 98% ê°ì†Œ**
- **Rate limit ì—¬ìœ  ì¦ê°€** (ë‹¤ë¥¸ ì‘ì—…ì— í• ë‹¹ ê°€ëŠ¥)

### Qualitative
- **ì‚¬ìš©ì ê²½í—˜ ê°œì„ **: ì¦‰ê°ì ì¸ ë™ê¸°í™”
- **ë¹„ìš© ì ˆê°**: API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ
- **í™•ì¥ì„±**: í˜ì´ì§€ ìˆ˜ ì¦ê°€í•´ë„ ì„±ëŠ¥ ìœ ì§€
- **ì•ˆì •ì„±**: ë¶ˆí•„ìš”í•œ API í˜¸ì¶œ ê°ì†Œë¡œ rate limit ì—ëŸ¬ ë°©ì§€

---

## Future Enhancements

### 1. Webhook-based Real-time Sync
**Concept:** Notion webhook â†’ ì¦‰ì‹œ import (polling ë¶ˆí•„ìš”)

```python
@router.post("/webhook/notion")
async def notion_webhook(payload: Dict):
    page_id = payload["page_id"]
    # Trigger import for single page
    background_tasks.add_task(import_single_page, page_id)
```

---

### 2. Batch Change Detection
**Concept:** ë³€ê²½ í˜ì´ì§€ë¥¼ ë¨¼ì € ê·¸ë£¹í™”í•˜ì—¬ batch API í˜¸ì¶œ

```python
# Instead of:
for page_id in fetch_targets:
    content = await fetch_page_blocks(page_id)  # N calls

# Use batch API (if available):
contents = await fetch_page_blocks_batch(fetch_targets)  # 1 call
```

---

### 3. Cache Layer (Redis)
**Concept:** last_edited_timeì„ Redisì— ìºì‹±í•˜ì—¬ DB ì¿¼ë¦¬ ìƒëµ

```python
# Check Redis first
cached_times = await redis.mget([f"page:{id}" for id in page_ids])
# Only query DB for cache misses
```

---

## Conclusion

ì´ ì¦ë¶„ ì—…ë°ì´íŠ¸ ë°©ì‹ì€:
- âœ… **íš¨ìœ¨ì„±**: API í˜¸ì¶œ 99% ì ˆê°
- âœ… **í™•ì¥ì„±**: í˜ì´ì§€ ìˆ˜ ì¦ê°€í•´ë„ ì„±ëŠ¥ ìœ ì§€
- âœ… **ë‹¨ìˆœì„±**: 90ì¤„ ì¶”ê°€ë¡œ êµ¬í˜„ ê°€ëŠ¥
- âœ… **ì•ˆì •ì„±**: ê¸°ì¡´ ë¡œì§ ìœ ì§€, force ì˜µì…˜ìœ¼ë¡œ fallback
- âœ… **í˜¸í™˜ì„±**: ê¸°ì¡´ API ë³€ê²½ ì—†ìŒ (backward compatible)

**ì¶”ì²œ:** ì¦‰ì‹œ êµ¬í˜„ ì§„í–‰.

---

## ğŸ“Š í˜„ì¬ API í˜¸ì¶œ ë¡œì§ êµ¬ì¡°ë„ (Current Implementation)

### ì „ì²´ íë¦„ë„ (Overall Flow)

```
[Client Request]
    â†“
POST /pipeline/import-from-notion?page_size=100
    â†“
[FastAPI Router: pipeline.py]
    â†“
    â”œâ”€ Create import job (DB)
    â”œâ”€ Launch background task
    â””â”€ Return job_id immediately (202 Accepted)

[Background Task: _background_import_task()]
    â†“
    â”œâ”€ Mark job as "processing"
    â”œâ”€ Determine mode (database vs parent_page)
    â””â”€ Fetch metadata
        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 1. Notion API: blocks.children.list  â”‚
        â”‚    (Metadataë§Œ ê°€ì ¸ì˜¤ê¸° - lightweight)â”‚
        â”‚    Input: parent_page_id             â”‚
        â”‚    Output: 726 child_page blocks     â”‚
        â”‚    Time: ~2-3 minutes (rate limited) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2. Change Detection (NEW)            â”‚
        â”‚    get_pages_to_fetch()              â”‚
        â”‚    - Query DB for existing pages     â”‚
        â”‚    - Compare last_edited_time        â”‚
        â”‚    - Return (new_ids, updated_ids)   â”‚
        â”‚    Time: <1 second                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 3. Content Fetching Loop             â”‚
        â”‚    FOR each page in pages:           â”‚
        â”‚      IF page_id in fetch_targets:    â”‚
        â”‚        - Notion API: fetch_page_blocksâ”‚
        â”‚        - Upsert to DB                â”‚
        â”‚      ELSE:                           â”‚
        â”‚        - Skip (log + counter)        â”‚
        â”‚    Time: depends on changed pages    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 4. Update job status                 â”‚
        â”‚    - imported_pages                  â”‚
        â”‚    - skipped_pages                   â”‚
        â”‚    - failed_pages                    â”‚
        â”‚    - Mark job as "completed"         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ìƒì„¸ API í˜¸ì¶œ ì‹œí€€ìŠ¤ (Detailed API Call Sequence)

#### ğŸ”µ Phase 1: Metadata Fetch (Lightweight)

```
[notion_service.fetch_child_pages_from_parent()]
    â†“
    WHILE has_more:
        â†“
        await rate_limiter.acquire()  # 3 req/sec limit
        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Notion API: blocks.children.list        â”‚
        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚ URL: POST /v1/blocks/{parent_id}/childrenâ”‚
        â”‚ Params:                                 â”‚
        â”‚   - page_size: 100                      â”‚
        â”‚   - start_cursor: (for pagination)      â”‚
        â”‚                                         â”‚
        â”‚ Response per batch:                     â”‚
        â”‚   {                                     â”‚
        â”‚     "results": [                        â”‚
        â”‚       {                                 â”‚
        â”‚         "type": "child_page",           â”‚
        â”‚         "id": "abc-123-...",           â”‚
        â”‚         "created_time": "2024-01-01...",â”‚
        â”‚         "last_edited_time": "2024-01-15â”‚
        â”‚         "child_page": {"title": "..."}  â”‚
        â”‚       },                                â”‚
        â”‚       ...                               â”‚
        â”‚     ],                                  â”‚
        â”‚     "has_more": true,                   â”‚
        â”‚     "next_cursor": "..."                â”‚
        â”‚   }                                     â”‚
        â”‚                                         â”‚
        â”‚ âš ï¸  ì£¼ì˜: last_edited_timeì€ BLOCKì˜    â”‚
        â”‚          ìˆ˜ì • ì‹œê°„ (content ë³€ê²½ ë°˜ì˜?)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        Extract child_page blocks only
        Build page_data objects:
          {
            "id": "page-id",
            "url": "https://notion.so/...",
            "created_time": "...",
            "last_edited_time": "...",  # ğŸ”‘ Key for comparison
            "properties": {"ì œëª©": "..."}
          }
        â†“
        Append to all_child_pages[]
        â†“
        [NEXT BATCH if has_more=true]

    RETURN all_child_pages  # 726ê°œ page objects
```

**API í˜¸ì¶œ íšŸìˆ˜:**
- Batches: 726 pages Ã· 100 per batch = 8 batches
- Calls: 8 API calls
- Time: 8 calls Ã· 3 req/sec = ~2.7 seconds

---

#### ğŸŸ¢ Phase 2: Change Detection (NEW - Database Query)

```
[supabase_service.get_pages_to_fetch(pages)]
    â†“
    # Step 1: Parse Notion timestamps
    page_map = {}
    FOR each page in pages:
        notion_time = parse(page["last_edited_time"])
        notion_time = notion_time.replace(microsecond=0)  # Truncate
        page_map[page["id"]] = notion_time

    # Step 2: Query DB in batches
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Supabase Query: raw_notes table        â”‚
    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚ URL: GET /rest/v1/raw_notes            â”‚
    â”‚ Params:                                 â”‚
    â”‚   ?select=notion_page_id,notion_last_edited_timeâ”‚
    â”‚   &notion_page_id=in.(id1,id2,...,id1000)â”‚  âŒ URL TOO LONG
    â”‚                                         â”‚
    â”‚ BATCH_SIZE: 1000 (currently)            â”‚
    â”‚ With 726 pages:                         â”‚
    â”‚   - Batch 1: 726 IDs in URL             â”‚
    â”‚   - URL length: ~27KB                   â”‚
    â”‚   - Result: 400 Bad Request âŒ          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    âŒ ERROR: URL length exceeds limit
    â†“
    FALLBACK: return (all_page_ids, [])  # Treat all as "new"
    â†“
    RESULT: 726 pages marked as "new"
    NO PAGES SKIPPED âŒ
```

**ğŸš¨ BUG LOCATION: Line 932 in supabase_service.py**
```python
.in_("notion_page_id", batch_ids)  # HTTP GET with 726 UUIDs in URL
# URL: ?notion_page_id=in.(uuid1,uuid2,...,uuid726)
# Length: 36 chars Ã— 726 + separators = ~27,000 chars
# HTTP GET limit: ~8,192 chars (most servers)
# Result: 400 Bad Request
```

---

#### ğŸŸ¡ Phase 3: Content Fetching Loop (Selective - Should Work But Doesn't Due To Bug)

```
[_background_import_task() - Loop]
    â†“
    fetch_targets = set(new_page_ids + updated_page_ids)
    # Currently: fetch_targets = all 726 pages (due to bug)
    â†“
    FOR idx, page in enumerate(pages):  # 726 iterations
        page_id = page["id"]
        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Conditional Check                       â”‚
        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚ IF page_id NOT IN fetch_targets:        â”‚
        â”‚   â†’ Skip (log + increment skipped)      â”‚
        â”‚   â†’ Time: ~0.001 sec                    â”‚
        â”‚                                         â”‚
        â”‚ ELSE:  (page_id IN fetch_targets)       â”‚
        â”‚   â†“                                     â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚   â”‚ Notion API: blocks.children.listâ”‚   â”‚
        â”‚   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â”‚
        â”‚   â”‚ URL: GET /v1/blocks/{page_id}/  â”‚   â”‚
        â”‚   â”‚       children                  â”‚   â”‚
        â”‚   â”‚                                 â”‚   â”‚
        â”‚   â”‚ Extracts:                       â”‚   â”‚
        â”‚   â”‚ - Paragraphs                    â”‚   â”‚
        â”‚   â”‚ - Headings                      â”‚   â”‚
        â”‚   â”‚ - Lists                         â”‚   â”‚
        â”‚   â”‚ - Quotes                        â”‚   â”‚
        â”‚   â”‚ - Code blocks                   â”‚   â”‚
        â”‚   â”‚ - etc.                          â”‚   â”‚
        â”‚   â”‚                                 â”‚   â”‚
        â”‚   â”‚ Rate limit: 3 req/sec           â”‚   â”‚
        â”‚   â”‚ Time per call: ~0.33 sec        â”‚   â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â”‚   â†“                                     â”‚
        â”‚   Upsert to DB (raw_notes)              â”‚
        â”‚   â†“                                     â”‚
        â”‚   Increment imported_pages              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì˜ˆìƒ ì„±ëŠ¥ (ë²„ê·¸ ìˆ˜ì • í›„):**

| Scenario | Fetch Targets | API Calls | Time |
|----------|---------------|-----------|------|
| ì´ˆê¸° import | 726 (all new) | 726 | ~4 min |
| 1ê°œ ë³€ê²½ | 1 | 1 | <1 sec |
| 10ê°œ ë³€ê²½ | 10 | 10 | ~3 sec |
| ë³€ê²½ ì—†ìŒ | 0 | 0 | <1 sec |

**í˜„ì¬ ì‹¤ì œ ì„±ëŠ¥ (ë²„ê·¸ë¡œ ì¸í•´):**

| Scenario | Fetch Targets | API Calls | Time |
|----------|---------------|-----------|------|
| ëª¨ë“  ê²½ìš° | 726 (all "new") | 726 | ~4 min |

---

### ğŸ”´ í•µì‹¬ ë²„ê·¸ ë¶„ì„ (Root Cause Analysis)

#### ë²„ê·¸ ìœ„ì¹˜: `supabase_service.py:932`

```python
# Line 927-934
for i in range(0, len(page_ids), BATCH_SIZE):  # BATCH_SIZE = 1000
    batch_ids = page_ids[i:i+BATCH_SIZE]  # First batch: all 726 IDs
    response = await (
        self.client.table("raw_notes")
        .select("notion_page_id, notion_last_edited_time")
        .in_("notion_page_id", batch_ids)  # âŒ HTTP GET with long URL
        .execute()
    )
```

**HTTP Request ìƒì„±:**
```http
GET /rest/v1/raw_notes?select=notion_page_id%2Cnotion_last_edited_time&notion_page_id=in.%28556603bc-bad1-4f3a-af19-64619edbe24c%2C255b94e5-8350-49a3-a03f-57fa98bde45c%2C84f78b4c-3d22-42fa-99f6-f34beb84452d%2C...%2C2e92686c-2113-81df-b45e-dbea55faa2dc%29 HTTP/2
Host: zqrbrddmwrpogabizton.supabase.co
```

**URL ê¸¸ì´ ê³„ì‚°:**
```
Base URL + params: ~200 chars
Each UUID: 36 chars
Separators (,): 725 Ã— 1 = 725 chars
Total UUIDs: 726 Ã— 36 = 26,136 chars
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~27,061 chars
```

**HTTP ì„œë²„ ì œí•œ:**
- RFC 7230: No hard limit (êµ¬í˜„ì²´ ì˜ì¡´)
- ì‹¤ì œ êµ¬í˜„:
  - Nginx: 4KB-8KB (default)
  - Apache: 8KB (default)
  - Browsers: 2KB (Chrome, Firefox)
  - Supabase/PostgREST: Unknown but < 27KB

**ê²°ê³¼:**
```
HTTP/2 400 Bad Request
{
  "message": "JSON could not be generated",
  "code": 400,
  "details": "Bad Request"
}
```

**Fallback ë¡œì§ ì‘ë™:**
```python
# Line 954-957
except Exception as e:
    logger.error(f"Failed to fetch existing pages: {e}")
    # On error, treat all as new (safe fallback)
    return list(page_map.keys()), []  # ğŸ”¥ All 726 pages as "new"
```

---

### ğŸ”„ ì„±ëŠ¥ ë¹„êµ: ì „ì²´ ì¡°íšŒ vs ë°°ì¹˜ ì¡°íšŒ

#### ë°©ì‹ A: ì „ì²´ í…Œì´ë¸” ì¡°íšŒ (Full Table Scan)
```python
# DBì—ì„œ ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (í•„í„° ì—†ìŒ)
response = await self.client.table("raw_notes").select(
    "notion_page_id, notion_last_edited_time"
).execute()

# Pythonì—ì„œ í•„í„°ë§
existing_map = {row["notion_page_id"]: row["notion_last_edited_time"]
                for row in response.data}
```

**ë¹„ìš© ë¶„ì„:**
- DB Query: 1íšŒ (ì „ì²´ ìŠ¤ìº”)
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡: 732 rows Ã— ~60 bytes = ~44KB
- DB ì²˜ë¦¬ ì‹œê°„: ~50-100ms (ì¸ë±ìŠ¤ ë¬´ì‹œ, ì „ì²´ ìŠ¤ìº”)
- ë„¤íŠ¸ì›Œí¬ ì‹œê°„: ~50ms
- Python ì²˜ë¦¬: ~10ms (ë”•ì…”ë„ˆë¦¬ ìƒì„±)
- **ì´ ì‹œê°„: ~110-160ms**

**ì¥ì :**
- ë‹¨ìˆœí•œ ë¡œì§
- URL ê¸¸ì´ ì œí•œ ì—†ìŒ
- DB ì¿¼ë¦¬ 1íšŒ

**ë‹¨ì :**
- ë¶ˆí•„ìš”í•œ ë°ì´í„° ì „ì†¡ (í˜„ì¬ importì™€ ë¬´ê´€í•œ í˜ì´ì§€ë„ ê°€ì ¸ì˜´)
- í˜ì´ì§€ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë¹„íš¨ìœ¨ ì¦ê°€
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

---

#### ë°©ì‹ B: ë°°ì¹˜ í•„í„°ë§ ì¡°íšŒ (Batched Filtered Query)
```python
# BATCH_SIZE = 100
# 8 batches for 726 pages
for i in range(0, len(page_ids), BATCH_SIZE):
    batch_ids = page_ids[i:i+BATCH_SIZE]  # 100 IDs
    response = await self.client.table("raw_notes").select(
        "notion_page_id, notion_last_edited_time"
    ).in_("notion_page_id", batch_ids).execute()

    # Accumulate results
```

**ë¹„ìš© ë¶„ì„:**
- DB Query: 8íšŒ (ì¸ë±ìŠ¤ ì‚¬ìš©)
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡: 726 rows Ã— ~60 bytes = ~44KB (ë™ì¼)
- DB ì²˜ë¦¬ ì‹œê°„: 8 Ã— 10ms = 80ms (ì¸ë±ìŠ¤ lookup)
- ë„¤íŠ¸ì›Œí¬ ì‹œê°„: 8 Ã— 50ms = 400ms (ì™•ë³µ ì§€ì—°)
- Python ì²˜ë¦¬: ~10ms
- **ì´ ì‹œê°„: ~490ms**

**ì¥ì :**
- í•„ìš”í•œ í˜ì´ì§€ë§Œ ì¡°íšŒ (Notion APIì—ì„œ ê°€ì ¸ì˜¨ 726ê°œë§Œ)
- ì¸ë±ìŠ¤ í™œìš© (idx_raw_notes_notion_page_id)
- í™•ì¥ì„± ì¢‹ìŒ (DBì— 10ë§Œê°œ í˜ì´ì§€ ìˆì–´ë„ ì†ë„ ë™ì¼)

**ë‹¨ì :**
- ë„¤íŠ¸ì›Œí¬ ì™•ë³µ 8íšŒ
- ë¡œì§ ë³µì¡ë„ ì¦ê°€
- URL ê¸¸ì´ ì œí•œ ì£¼ì˜ í•„ìš”

---

### ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ

#### ì‹œë‚˜ë¦¬ì˜¤ 1: í˜„ì¬ ìƒí™© (DB 732í˜ì´ì§€, Import 726í˜ì´ì§€)

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | ë°©ì‹ B (ë°°ì¹˜ ì¡°íšŒ) | ìŠ¹ì |
|------|-------------------|-------------------|------|
| DB ì¿¼ë¦¬ ìˆ˜ | 1 | 8 | A |
| DB ì²˜ë¦¬ ì‹œê°„ | 50-100ms | 80ms | B |
| ë„¤íŠ¸ì›Œí¬ ì™•ë³µ | 1 | 8 | A |
| ë„¤íŠ¸ì›Œí¬ ì‹œê°„ | 50ms | 400ms | A |
| ì´ ì‹œê°„ | **~110-160ms** | **~490ms** | **A ìŠ¹ë¦¬** |
| ë°ì´í„° ì „ì†¡ëŸ‰ | 44KB | 44KB | ë™ì¼ |

**ê²°ë¡ : ë°©ì‹ Aê°€ **3ë°° ë¹ ë¦„**

---

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ëŒ€ê·œëª¨ DB (DB 10,000í˜ì´ì§€, Import 726í˜ì´ì§€)

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | ë°©ì‹ B (ë°°ì¹˜ ì¡°íšŒ) | ìŠ¹ì |
|------|-------------------|-------------------|------|
| DB ì¿¼ë¦¬ ìˆ˜ | 1 | 8 | A |
| DB ì²˜ë¦¬ ì‹œê°„ | 200-300ms | 80ms | B |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ | 10,000 Ã— 60 = 600KB | 726 Ã— 60 = 44KB | **B (13ë°° ì ìŒ)** |
| ë„¤íŠ¸ì›Œí¬ ì‹œê°„ | 300ms | 400ms | A |
| ì´ ì‹œê°„ | **~500-600ms** | **~490ms** | **B ìŠ¹ë¦¬** |

**ê²°ë¡ : ë°©ì‹ Bê°€ ì•½ê°„ ë¹ ë¦„, ë°ì´í„° ì „ì†¡ëŸ‰ **13ë°° ì ìŒ**

---

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ì†Œê·œëª¨ Import (DB 732í˜ì´ì§€, Import 10í˜ì´ì§€)

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | ë°©ì‹ B (ë°°ì¹˜ ì¡°íšŒ) | ìŠ¹ì |
|------|-------------------|-------------------|------|
| DB ì¿¼ë¦¬ ìˆ˜ | 1 | 1 | ë™ì¼ |
| DB ì²˜ë¦¬ ì‹œê°„ | 50-100ms | 10ms | B |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ | 732 Ã— 60 = 44KB | 10 Ã— 60 = 0.6KB | **B (73ë°° ì ìŒ)** |
| ë„¤íŠ¸ì›Œí¬ ì‹œê°„ | 50ms | 50ms | ë™ì¼ |
| ì´ ì‹œê°„ | **~110-160ms** | **~70ms** | **B ìŠ¹ë¦¬** |

**ê²°ë¡ : ë°©ì‹ Bê°€ **2ë°° ë¹ ë¦„**, ë°ì´í„° ì „ì†¡ëŸ‰ **73ë°° ì ìŒ**

---

### ğŸ¯ ìµœì¢… ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

#### í˜„ì¬ ìƒí™© (DB 732, Import 726)
- **ê¶Œì¥: ë°©ì‹ A (ì „ì²´ ì¡°íšŒ)**
- ì´ìœ : 3ë°° ë¹ ë¦„ (110ms vs 490ms)
- DBì™€ Import í¬ê¸°ê°€ ê±°ì˜ ë™ì¼í•˜ë¯€ë¡œ ë¶ˆí•„ìš”í•œ ë°ì´í„° ê±°ì˜ ì—†ìŒ

#### ë¯¸ë˜ í™•ì¥ì„± ê³ ë ¤
- **ê¶Œì¥: ë°©ì‹ B (ë°°ì¹˜ ì¡°íšŒ)**
- ì´ìœ :
  1. DBê°€ 10,000ê°œë¡œ ì¦ê°€ ì‹œ ì„±ëŠ¥ ìœ ì§€
  2. ì†Œê·œëª¨ import ì‹œ í›¨ì”¬ íš¨ìœ¨ì 
  3. ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ˆì•½

---

### ğŸ† ì¶”ì²œ êµ¬í˜„: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹

```python
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict[str, Any]]
) -> tuple[List[str], List[str]]:
    """
    Smart change detection with adaptive strategy.
    """
    await self._ensure_initialized()

    page_map = {...}  # Parse Notion pages

    # Get total DB count
    count_response = await self.client.table("raw_notes").select(
        "notion_page_id", count="exact"
    ).execute()
    total_db_pages = count_response.count

    # Adaptive strategy
    if len(page_map) >= total_db_pages * 0.8:
        # Import covers >80% of DB â†’ Full scan cheaper
        logger.info(f"Using full scan strategy ({len(page_map)}/{total_db_pages} pages)")
        response = await self.client.table("raw_notes").select(
            "notion_page_id, notion_last_edited_time"
        ).execute()
        existing_map = {row["notion_page_id"]: row["notion_last_edited_time"]
                       for row in response.data}
    else:
        # Import is subset â†’ Batched query more efficient
        logger.info(f"Using batched query strategy ({len(page_map)}/{total_db_pages} pages)")
        BATCH_SIZE = 100
        existing_map = {}
        for i in range(0, len(page_ids), BATCH_SIZE):
            batch_ids = page_ids[i:i+BATCH_SIZE]
            response = await self.client.table("raw_notes").select(
                "notion_page_id, notion_last_edited_time"
            ).in_("notion_page_id", batch_ids).execute()
            for row in response.data:
                existing_map[row["notion_page_id"]] = row["notion_last_edited_time"]

    # Compare and return
    # ...
```

**ë¡œì§:**
- Importê°€ DBì˜ 80% ì´ìƒ ì»¤ë²„ â†’ ì „ì²´ ì¡°íšŒ (í˜„ì¬: 726/732 = 99%)
- Importê°€ DBì˜ 80% ë¯¸ë§Œ â†’ ë°°ì¹˜ ì¡°íšŒ
- ë¯¸ë˜ì— DB 10,000ê°œ, Import 100ê°œ â†’ ìë™ìœ¼ë¡œ ë°°ì¹˜ ì¡°íšŒ ì‚¬ìš©

**ì¥ì :**
- í˜„ì¬: ìµœì  ì„±ëŠ¥ (110ms)
- ë¯¸ë˜: í™•ì¥ì„± ë³´ì¥
- ìë™ ìµœì í™” (ì½”ë“œ ë³€ê²½ ì—†ìŒ)

---

### í•´ê²°ì±… (Solutions)

#### âœ… Solution 1: Reduce BATCH_SIZE (Immediate Fix)

```python
# Line 922
BATCH_SIZE = 100  # Was: 1000

# URL length with 100 UUIDs:
# 36 Ã— 100 + 99 + 200 = ~3,800 chars âœ… Safe
```

**ì ìš© í›„ ì„±ëŠ¥:**
- Batches: 726 Ã· 100 = 8 batches
- DB queries: 8
- Query time: 8 Ã— 50ms = 400ms
- Total: <1 second âœ…

#### âœ… Solution 2: Supabase RPC with Custom Function (Better)

**ê°œë…:** Supabaseì— ì»¤ìŠ¤í…€ PostgreSQL í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³ , Pythonì—ì„œ RPCë¡œ í˜¸ì¶œ

**ì¥ì :**
- HTTP POST ì‚¬ìš© (URL ê¸¸ì´ ì œí•œ ì—†ìŒ)
- ë°°ì—´ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ (JSON bodyì— í¬í•¨)
- ì„œë²„ì—ì„œ ì²˜ë¦¬ (ë„¤íŠ¸ì›Œí¬ ì™•ë³µ 1íšŒ)

**Step 1: Supabaseì— SQL í•¨ìˆ˜ ìƒì„±**
```sql
CREATE OR REPLACE FUNCTION get_pages_by_ids(page_ids text[])
RETURNS TABLE(notion_page_id text, notion_last_edited_time timestamptz)
AS $$
BEGIN
    RETURN QUERY
    SELECT rn.notion_page_id, rn.notion_last_edited_time
    FROM raw_notes rn
    WHERE rn.notion_page_id = ANY(page_ids);  -- ë°°ì—´ë¡œ í•„í„°ë§
END;
$$ LANGUAGE plpgsql;
```

**Step 2: Pythonì—ì„œ RPC í˜¸ì¶œ**
```python
# POST ìš”ì²­ìœ¼ë¡œ ì „í™˜ (URLì— ë°ì´í„° ì—†ìŒ)
response = await self.client.rpc('get_pages_by_ids', {
    'page_ids': batch_ids  # JSON bodyì— í¬í•¨
}).execute()

# HTTP Request ì˜ˆì‹œ:
# POST /rest/v1/rpc/get_pages_by_ids HTTP/2
# Content-Type: application/json
# Body: {"page_ids": ["uuid1", "uuid2", ..., "uuid726"]}
```

**ë¹„ìš© ë¶„ì„:**
- DB Query: 1íšŒ (ì„œë²„ì—ì„œ ì‹¤í–‰)
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡: 726 rows Ã— ~60 bytes = ~44KB
- HTTP Method: **POST** (bodyì— ë°ì´í„°, URL ì§§ìŒ)
- DB ì²˜ë¦¬ ì‹œê°„: ~80ms (ì¸ë±ìŠ¤ ì‚¬ìš©)
- ë„¤íŠ¸ì›Œí¬ ì‹œê°„: ~50ms (1íšŒ ì™•ë³µ)
- **ì´ ì‹œê°„: ~130ms**

**ë°©ì‹ A vs ë°©ì‹ B (RPC):**

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | ë°©ì‹ B (RPC ë°°ì¹˜) | ìŠ¹ì |
|------|-------------------|------------------|------|
| DB ì¿¼ë¦¬ | 1 | 1 | ë™ì¼ |
| ë„¤íŠ¸ì›Œí¬ ì™•ë³µ | 1 | 1 | ë™ì¼ |
| ì´ ì‹œê°„ | 110ms | **130ms** | A |
| URL ê¸¸ì´ ì œí•œ | ì—†ìŒ | ì—†ìŒ (POST) | ë™ì¼ |
| í™•ì¥ì„± | DB ì¦ê°€ ì‹œ ëŠë ¤ì§ | DB ì¦ê°€í•´ë„ ë™ì¼ | **B** |

**ê²°ë¡ :**
- í˜„ì¬: ë°©ì‹ Aê°€ 20ms ë¹ ë¦„ (ë¬´ì‹œí•  ì •ë„)
- ë¯¸ë˜: DB 10,000ê°œ ì‹œ ë°©ì‹ Bê°€ í›¨ì”¬ íš¨ìœ¨ì 

---

#### âœ… Solution 3: Server-Side Comparison (Best - All Logic on Server)

**ê°œë…:** ë¹„êµ ë¡œì§ê¹Œì§€ DBì—ì„œ ì²˜ë¦¬ (Pythonì€ ê²°ê³¼ë§Œ ë°›ìŒ)

**ì¥ì :**
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ìµœì†Œí™” (ê²°ê³¼ë§Œ ì „ì†¡)
- DBì—ì„œ timestamp ë¹„êµ (ë” ë¹ ë¦„)
- Python ì²˜ë¦¬ ë¶ˆí•„ìš”

**Step 1: Supabaseì— ë¹„êµ í•¨ìˆ˜ ìƒì„±**
```sql
CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb
AS $$
DECLARE
    result jsonb;
    new_ids text[];
    updated_ids text[];
    page_record jsonb;
    notion_id text;
    notion_time timestamptz;
    db_time timestamptz;
BEGIN
    new_ids := ARRAY[]::text[];
    updated_ids := ARRAY[]::text[];

    -- Notionì—ì„œ ê°€ì ¸ì˜¨ ê° í˜ì´ì§€ ì²˜ë¦¬
    FOR page_record IN SELECT * FROM jsonb_array_elements(pages_data)
    LOOP
        notion_id := page_record->>'id';
        notion_time := (page_record->>'last_edited')::timestamptz;

        -- DBì—ì„œ í•´ë‹¹ í˜ì´ì§€ ì¡°íšŒ
        SELECT notion_last_edited_time INTO db_time
        FROM raw_notes
        WHERE notion_page_id = notion_id;

        IF NOT FOUND THEN
            -- ì‹ ê·œ í˜ì´ì§€
            new_ids := array_append(new_ids, notion_id);
        ELSIF notion_time > db_time THEN
            -- ìˆ˜ì •ëœ í˜ì´ì§€
            updated_ids := array_append(updated_ids, notion_id);
        END IF;
        -- ELSE: unchanged (skip)
    END LOOP;

    -- ê²°ê³¼ ë°˜í™˜
    result := jsonb_build_object(
        'new_page_ids', to_jsonb(new_ids),
        'updated_page_ids', to_jsonb(updated_ids)
    );

    RETURN result;
END;
$$ LANGUAGE plpgsql;
```

**Step 2: Pythonì—ì„œ RPC í˜¸ì¶œ**
```python
# Notion í˜ì´ì§€ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
pages_json = [
    {
        "id": page["id"],
        "last_edited": page["last_edited_time"]
    }
    for page in notion_pages
]

# RPC í˜¸ì¶œ (ë¹„êµ ë¡œì§ ì „ì²´ë¥¼ DBì—ì„œ ì²˜ë¦¬)
response = await self.client.rpc('get_changed_pages', {
    'pages_data': json.dumps(pages_json)
}).execute()

# ê²°ê³¼ë§Œ ë°›ìŒ (ì´ë¯¸ ë¶„ë¥˜ëœ ìƒíƒœ)
result = response.data
new_page_ids = result['new_page_ids']      # ["uuid1", "uuid2", ...]
updated_page_ids = result['updated_page_ids']  # ["uuid3", "uuid4", ...]

# HTTP Request ì˜ˆì‹œ:
# POST /rest/v1/rpc/get_changed_pages HTTP/2
# Body: {
#   "pages_data": [
#     {"id": "uuid1", "last_edited": "2024-01-15T14:30:00Z"},
#     {"id": "uuid2", "last_edited": "2024-01-15T15:00:00Z"},
#     ...
#   ]
# }
#
# Response:
# {
#   "new_page_ids": ["uuid1", "uuid10", ...],
#   "updated_page_ids": ["uuid5", "uuid20", ...]
# }
```

**ë¹„ìš© ë¶„ì„:**
- DB Query: 726íšŒ (ê° í˜ì´ì§€ë§ˆë‹¤ 1íšŒ lookup, í•˜ì§€ë§Œ ì„œë²„ ë‚´ë¶€ë¼ ë¹ ë¦„)
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ìš”ì²­): 726 Ã— ~80 bytes = ~58KB (id + timestamp)
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ì‘ë‹µ): ë³€ê²½ëœ í˜ì´ì§€ IDë§Œ (ì˜ˆ: 10ê°œ Ã— 36 bytes = 360 bytes)
- DB ì²˜ë¦¬ ì‹œê°„: ~100ms (ë£¨í”„ + ì¸ë±ìŠ¤ lookup)
- ë„¤íŠ¸ì›Œí¬ ì‹œê°„: ~50ms (1íšŒ ì™•ë³µ)
- Python ì²˜ë¦¬: **0ms** (ë¹„êµ ë¡œì§ ì—†ìŒ)
- **ì´ ì‹œê°„: ~150ms**

**ë¹„êµ: ë°©ì‹ A vs RPC ë°°ì¹˜ vs RPC ë¹„êµ**

| í•­ëª© | ë°©ì‹ A (ì „ì²´) | Solution 2 (RPC ë°°ì¹˜) | Solution 3 (RPC ë¹„êµ) | ìŠ¹ì |
|------|--------------|---------------------|---------------------|------|
| ë„¤íŠ¸ì›Œí¬ ì™•ë³µ | 1 | 1 | 1 | ë™ì¼ |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ìš”ì²­) | ìµœì†Œ | 58KB | 58KB | A |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ì‘ë‹µ) | 44KB | 44KB | **0.4KB** (10ê°œë§Œ) | **C** |
| DB ì¿¼ë¦¬ | 1 (full scan) | 1 (filtered) | 726 (indexed) | B |
| Python ì²˜ë¦¬ | ë¹„êµ í•„ìš” | ë¹„êµ í•„ìš” | **ë¶ˆí•„ìš”** | **C** |
| ì´ ì‹œê°„ | 110ms | 130ms | **150ms** | A |
| í™•ì¥ì„± | ë‚˜ì¨ | **ìµœê³ ** | ì¢‹ìŒ | **B** |

**ì‹œë‚˜ë¦¬ì˜¤ë³„ ìŠ¹ì:**

| ì‹œë‚˜ë¦¬ì˜¤ | ë°©ì‹ A | Solution 2 | Solution 3 | ìµœì  |
|---------|-------|-----------|-----------|------|
| í˜„ì¬ (732/726) | **110ms** | 130ms | 150ms | **A** |
| DB 10,000 (10%) | 500ms | **130ms** | **150ms** | **Sol 2/3** |
| ë³€ê²½ 1ê°œë§Œ | 110ms | 130ms | **80ms** (ì‘ë‹µ ì‘ìŒ) | **Sol 3** |

**Solution 3ì˜ ì§„ê°€:**
- **ë³€ê²½ì´ ì ì„ìˆ˜ë¡** ë” íš¨ìœ¨ì  (ì‘ë‹µ í¬ê¸° ìµœì†Œ)
- DB 10,000ê°œ, ë³€ê²½ 10ê°œ: ì‘ë‹µ 360 bytes vs 44KB (122ë°° ì ìŒ)
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ˆì•½ (ëª¨ë°”ì¼ ë“±ì— ìœ ë¦¬)

---

### âš ï¸ í•µì‹¬ ì§ˆë¬¸: "Solution 3ë„ UUID ì „ë¶€ ë³´ë‚´ëŠ”ë° ê¸°ì¡´ê³¼ ë­ê°€ ë‹¬ë¼?"

**ì •ë‹µ: HTTP Methodì˜ ì°¨ì´!**

#### ê¸°ì¡´ ë°©ì‹ (ë°°ì¹˜ ì¡°íšŒ - ë²„ê·¸ ë°œìƒ)
```python
# HTTP GET ë°©ì‹
response = await self.client.table("raw_notes").select(
    "notion_page_id, notion_last_edited_time"
).in_("notion_page_id", batch_ids).execute()

# ìƒì„±ë˜ëŠ” HTTP Request:
# GET /rest/v1/raw_notes?notion_page_id=in.(uuid1,uuid2,...,uuid726)
#     ^^^                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#     GET                      URLì— ë°ì´í„° í¬í•¨ (27KB)
#
# âŒ ë¬¸ì œ: URL ê¸¸ì´ ì œí•œ ì´ˆê³¼ (8KB ì œí•œ)
```

#### Solution 2 & 3 (RPC - POST ë°©ì‹)
```python
# HTTP POST ë°©ì‹ (RPC)
response = await self.client.rpc('get_pages_by_ids', {
    'page_ids': batch_ids  # JSON bodyì— í¬í•¨
}).execute()

# ìƒì„±ë˜ëŠ” HTTP Request:
# POST /rest/v1/rpc/get_pages_by_ids
#      ^^^
#      POST
# Content-Type: application/json
# Body: {"page_ids": ["uuid1", "uuid2", ..., "uuid726"]}
#       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#       Bodyì— ë°ì´í„° í¬í•¨ (í¬ê¸° ì œí•œ ì—†ìŒ)
#
# âœ… í•´ê²°: BodyëŠ” ìˆ˜ì‹­ MBë„ ê°€ëŠ¥
```

---

### ğŸ“Š í•µì‹¬ ì°¨ì´ì  ì •ë¦¬

| í•­ëª© | ê¸°ì¡´ (GET + .in_) | Solution 2/3 (POST + RPC) |
|------|------------------|---------------------------|
| **HTTP Method** | **GET** | **POST** |
| **ë°ì´í„° ìœ„ì¹˜** | **URL ì¿¼ë¦¬ìŠ¤íŠ¸ë§** | **Request Body** |
| **í¬ê¸° ì œí•œ** | **~8KB (URL ì œí•œ)** | **ìˆ˜ì‹­ MB (Body ì œí•œ)** |
| **726ê°œ UUID** | âŒ ë¶ˆê°€ëŠ¥ (27KB) | âœ… ê°€ëŠ¥ (58KB body) |
| **Supabase SDK ì§€ì›** | `.in_()` (GETë§Œ ì§€ì›) | `.rpc()` (POST ì§€ì›) |

---

### ğŸ¤” ê·¸ëŸ¼ ì™œ ë°©ì‹ A (ì „ì²´ ì¡°íšŒ)ëŠ” ë¬¸ì œì—†ì—ˆë‚˜?

```python
# ë°©ì‹ A: ì „ì²´ ì¡°íšŒ
response = await self.client.table("raw_notes").select(
    "notion_page_id, notion_last_edited_time"
).execute()

# HTTP Request:
# GET /rest/v1/raw_notes?select=notion_page_id,notion_last_edited_time
#                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#                        URLì— í•„í„° ì¡°ê±´ ì—†ìŒ! (ì§§ì€ URL)
```

**í•„í„° ì—†ì´ ì „ì²´ ì¡°íšŒ â†’ URL ì§§ìŒ â†’ ë¬¸ì œ ì—†ìŒ!**

---

### ğŸ’¡ ê²°ë¡ 

**ë²„ê·¸ì˜ ì›ì¸:**
- Supabase Python SDKì˜ `.in_()` ë©”ì„œë“œëŠ” **HTTP GET**ì„ ì‚¬ìš©
- 726ê°œ UUIDë¥¼ **URL ì¿¼ë¦¬ìŠ¤íŠ¸ë§**ì— ë„£ìŒ (27KB)
- HTTP GET URL ì œí•œ ì´ˆê³¼ (8KB)

**Solution 2/3ì˜ í•µì‹¬:**
- `.rpc()` ë©”ì„œë“œëŠ” **HTTP POST**ë¥¼ ì‚¬ìš©
- 726ê°œ UUIDë¥¼ **JSON Body**ì— ë„£ìŒ (ì œí•œ ì—†ìŒ)
- ê°™ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ë³´ë‚´ì§€ë§Œ **ì „ì†¡ ë°©ì‹ì´ ë‹¤ë¦„**

**ë¹„ìœ :**
- GET: ì—½ì„œì— ì£¼ì†Œë¥¼ ì ìŒ (ê¸€ì ìˆ˜ ì œí•œ ìˆìŒ)
- POST: íƒë°° ìƒìì— ë¬¼ê±´ì„ ë„£ìŒ (ë¬´ê²Œ ì œí•œ í›¨ì”¬ í¼)

---

### ğŸ¯ ìµœì¢… ì •ë¦¬: ì™œ ì—¬ëŸ¬ ì†”ë£¨ì…˜ì´ ìˆë‚˜?

| Solution | HTTP | ë°ì´í„° ì „ì†¡ | ì¥ì  | í˜„ì¬ ìƒí™© ì¶”ì²œ |
|----------|------|-----------|------|--------------|
| **ë°©ì‹ A** | GET | ì—†ìŒ (ì „ì²´ ì¡°íšŒ) | **ê°€ì¥ ë¹ ë¦„** (110ms) | âœ… **1ìˆœìœ„** |
| **Solution 1** | GET | URL (100ê°œì”©) | ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ìµœì†Œ | ì°¨ì„ ì±… |
| **Solution 2** | POST | Body (726ê°œ) | í™•ì¥ì„± ì¢‹ìŒ | ë¯¸ë˜ ê³ ë ¤ ì‹œ |
| **Solution 3** | POST | Body (726ê°œ) | ì‘ë‹µ ìµœì†Œí™” | ë³€ê²½ ì ì„ ë•Œ ìµœì  |

**í•µì‹¬ ì°¨ì´:**
- **GET (URL)**: ì‘ì€ ë°ì´í„°ë§Œ (< 8KB)
- **POST (Body)**: í° ë°ì´í„° ê°€ëŠ¥ (> ìˆ˜ì‹­ MB)
- **ë°©ì‹ A**: ì•„ì˜ˆ í•„í„° ì•ˆ ì”€ (ì „ì²´ ì¡°íšŒ)

---

### ğŸ¤” ì¶”ê°€ ì§ˆë¬¸: "ê·¸ëŸ¼ ì²˜ìŒë¶€í„° Solution 3ìœ¼ë¡œ ë§Œë“¤ë©´?"

**ë‹µë³€: ë§ìŠµë‹ˆë‹¤! ì¥ê¸°ì ìœ¼ë¡œëŠ” Solution 3ì´ ìµœì ì…ë‹ˆë‹¤.**

#### ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ì˜ˆì¸¡

| DB í¬ê¸° | Import í¬ê¸° | ë³€ê²½ ë¹„ìœ¨ | ë°©ì‹ A | Solution 3 | ìŠ¹ì |
|---------|-----------|----------|--------|-----------|------|
| 732 | 726 | 99% | **110ms** | 150ms | A |
| 1,000 | 1,000 | 100% | 150ms | 170ms | A |
| 10,000 | 726 | 7% | 500ms | **150ms** | **Sol 3** |
| 10,000 | 100 | 1% | 500ms | **120ms** | **Sol 3** |
| 100,000 | 1,000 | 1% | 5,000ms | **200ms** | **Sol 3** |

**ê²°ë¡ : DBê°€ ì»¤ì§ˆìˆ˜ë¡ Solution 3ì´ ì••ë„ì ìœ¼ë¡œ ë¹ ë¦„!**

---

#### ğŸ’¡ ì™œ ì§€ê¸ˆ ë‹¹ì¥ Solution 3ì„ ì¶”ì²œí•˜ì§€ ì•Šë‚˜?

**1. ì¶”ê°€ ì¸í”„ë¼ í•„ìš”**
```sql
-- Supabaseì— ì»¤ìŠ¤í…€ í•¨ìˆ˜ ë°°í¬ í•„ìš”
CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb AS $$ ... $$;
```
- ìš´ì˜ ë³µì¡ë„ ì¦ê°€
- Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬ í•„ìš”
- í•¨ìˆ˜ ë²„ê·¸ ì‹œ ë””ë²„ê¹… ì–´ë ¤ì›€ (DB ë¡œê·¸ í™•ì¸ í•„ìš”)

**2. í˜„ì¬ëŠ” ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸**
- ë°©ì‹ A: 110ms
- Solution 3: 150ms
- ì°¨ì´: 40ms (0.04ì´ˆ - ì‚¬ìš©ìê°€ ëŠë¼ì§€ ëª»í•¨)

**3. ì½”ë“œ ë‹¨ìˆœì„±**
```python
# ë°©ì‹ A: 3ì¤„
response = await self.client.table("raw_notes").select(
    "notion_page_id, notion_last_edited_time"
).execute()

# Solution 3: SQL í•¨ìˆ˜ + Python ì½”ë“œ + ì—ëŸ¬ í•¸ë“¤ë§
# â†’ ì´ ~100ì¤„
```

---

#### ğŸ¯ ìµœì¢… ê¶Œì¥ ì „ëµ

**Phase 1 (ì¦‰ì‹œ): ë°©ì‹ A**
- ì½”ë“œ ë‹¨ìˆœ
- ì¸í”„ë¼ ë³€ê²½ ì—†ìŒ
- ì¶©ë¶„íˆ ë¹ ë¦„ (110ms)

**Phase 2 (DB 5,000ê°œ ë„ë‹¬ ì‹œ): Solution 3ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜**
- SQL í•¨ìˆ˜ ë°°í¬
- Python ì½”ë“œ êµì²´
- ì„±ëŠ¥ ê·¹ëŒ€í™”

**íŠ¸ë¦¬ê±° ì¡°ê±´:**
```python
# config.py
USE_RPC_CHANGE_DETECTION = os.getenv("USE_RPC_CHANGE_DETECTION", "false").lower() == "true"

# supabase_service.py
if settings.use_rpc_change_detection:
    # Solution 3: RPC í˜¸ì¶œ
    return await self._get_pages_to_fetch_rpc(notion_pages)
else:
    # ë°©ì‹ A: ì „ì²´ ì¡°íšŒ
    return await self._get_pages_to_fetch_full_scan(notion_pages)
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì :**
- DB í˜ì´ì§€ 5,000ê°œ ì´ˆê³¼ ì‹œ
- ë˜ëŠ” ì„±ëŠ¥ ì´ìŠˆ ë°œìƒ ì‹œ
- í™˜ê²½ë³€ìˆ˜ë¡œ ê°„ë‹¨íˆ ì „í™˜

---

### ğŸ” ì§ˆë¬¸ 2: "Supabase Python SDKëŠ” ìë™ìœ¼ë¡œ POST ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"ì˜ ì˜ë¯¸

**ë°°ê²½: Supabase SDKì˜ HTTP Method ì„ íƒ ë¡œì§**

#### Supabase Python SDKì˜ ë‚´ë¶€ ë™ì‘

```python
# supabase-py ë‚´ë¶€ ì½”ë“œ (simplified)
class PostgrestClient:
    def select(self, columns):
        # SELECTëŠ” í•­ìƒ GET ì‚¬ìš©
        return self._request("GET", "/table_name", params={"select": columns})

    def in_(self, column, values):
        # IN ì¡°ê±´ë„ GETì˜ ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì— ì¶”ê°€
        # URL: GET /table?column=in.(value1,value2,...)
        self.params[column] = f"in.({','.join(values)})"
        return self

    def rpc(self, function_name, params):
        # RPCëŠ” í•­ìƒ POST ì‚¬ìš©
        return self._request("POST", f"/rpc/{function_name}", json=params)
```

#### í•µì‹¬: SDKê°€ ìë™ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ì•ŠìŒ

**ë¬¸ì œ ìƒí™©:**
```python
# ê°œë°œìê°€ ì›í•˜ëŠ” ê²ƒ: "ë°ì´í„° ë§ìœ¼ë©´ ìë™ìœ¼ë¡œ POST ì“°ë©´ ì¢‹ê² ë‹¤"
batch_ids = ["uuid1", "uuid2", ..., "uuid726"]  # ë§ì€ ë°ì´í„°

response = await client.table("raw_notes").select("*").in_("id", batch_ids).execute()
# âŒ SDKëŠ” ë¬´ì¡°ê±´ GET ì‚¬ìš© (ë°ì´í„° í¬ê¸° ì‹ ê²½ ì•ˆ ì”€)
# âŒ URL: GET /raw_notes?id=in.(uuid1,uuid2,...,uuid726) â†’ 27KB URL
```

**SDKê°€ í•˜ì§€ ì•ŠëŠ” ê²ƒ:**
```python
# âœ— ì´ëŸ° ë¡œì§ì´ ì—†ìŒ:
if len(batch_ids) > 100:  # ë°ì´í„°ê°€ ë§ìœ¼ë©´
    use_post_request()    # ìë™ìœ¼ë¡œ POSTë¡œ ì „í™˜
else:
    use_get_request()     # ì ìœ¼ë©´ GET ì‚¬ìš©
```

#### í•´ê²°: ê°œë°œìê°€ ëª…ì‹œì ìœ¼ë¡œ POST ì‚¬ìš©

```python
# ë°©ë²• 1: RPC ì‚¬ìš© (POST ê°•ì œ)
response = await client.rpc('get_pages_by_ids', {
    'page_ids': batch_ids  # âœ… ìë™ìœ¼ë¡œ POST bodyì— í¬í•¨
}).execute()

# ë°©ë²• 2: ì§ì ‘ HTTP POST ìš”ì²­ (low-level)
import httpx
response = await httpx.post(
    f"{SUPABASE_URL}/rest/v1/raw_notes",
    json={"id": {"in": batch_ids}},  # âœ… Bodyì— í¬í•¨
    headers={"apikey": SUPABASE_KEY}
)
```

#### ì™œ SDKê°€ ìë™ ì „í™˜í•˜ì§€ ì•Šë‚˜?

**1. RESTful ê·œì•½**
- GET: ì½ê¸° (ë©±ë“±ì„±, ìºì‹œ ê°€ëŠ¥)
- POST: ì“°ê¸°/RPC (ë¶€ì‘ìš© ê°€ëŠ¥)
- SELECT ì¿¼ë¦¬ëŠ” ì˜ë¯¸ìƒ GETì´ ë§ìŒ

**2. í•˜ìœ„ í˜¸í™˜ì„±**
- ê¸°ì¡´ ì½”ë“œê°€ GETìœ¼ë¡œ ì‘ë™
- ê°‘ìê¸° POSTë¡œ ë°”ë€Œë©´ ìºì‹±/ë¡œê¹… ë“±ì— ì˜í–¥

**3. ëª…ì‹œì„±**
- ê°œë°œìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•˜ê²Œ í•¨
- "í° ë°ì´í„°ëŠ” RPC ì“°ì„¸ìš”"ê°€ ì„¤ê³„ ì˜ë„

---

### ğŸ“Š ì •ë¦¬

| ì§ˆë¬¸ | ë‹µë³€ |
|------|------|
| **ì²˜ìŒë¶€í„° Solution 3?** | ì¥ê¸°ì ìœ¼ë¡œ ë§ì§€ë§Œ, í˜„ì¬ëŠ” ì˜¤ë²„ì—”ì§€ë‹ˆì–´ë§. DB 5,000ê°œ ë„˜ìœ¼ë©´ ì „í™˜ ì¶”ì²œ. |
| **SDK POST ìë™ ì „í™˜?** | ì•ˆ í•¨. `.in_()`ì€ í•­ìƒ GET. `.rpc()`ë§Œ POST. ê°œë°œìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•´ì•¼ í•¨. |
| **ìµœì„ ì±…?** | Phase 1: ë°©ì‹ Aë¡œ ì‹œì‘ â†’ Phase 2: DB ì»¤ì§€ë©´ Solution 3ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ |

---

## ğŸ¯ Final Recommendation

### Immediate Action: Implement ë°©ì‹ A (Full Table Scan)

**Why:**
1. **Simplest solution** - 3 lines of code change
2. **Fastest for current scale** - 110ms vs 490ms (batched) or 150ms (RPC)
3. **No infrastructure changes** - No SQL functions to deploy
4. **Zero risk** - Proven pattern, no URL length issues
5. **Sufficient performance** - 110ms is imperceptible to users

**Implementation:**
```python
# backend/services/supabase_service.py (MODIFY)
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict[str, Any]]
) -> tuple[List[str], List[str]]:
    """Compare Notion pages with DB to detect changes."""
    await self._ensure_initialized()

    # Parse Notion timestamps
    page_map = {}
    for p in notion_pages:
        page_id = p.get("id")
        last_edited = p.get("last_edited_time")
        if not page_id or not last_edited:
            continue
        notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
        notion_time = notion_time.replace(microsecond=0)  # Truncate
        page_map[page_id] = notion_time

    if not page_map:
        return [], []

    # ë°©ì‹ A: Full table scan (SIMPLE & FAST)
    try:
        response = await (
            self.client.table("raw_notes")
            .select("notion_page_id, notion_last_edited_time")
            .execute()
        )

        existing_map = {}
        for row in response.data:
            db_time = row["notion_last_edited_time"]
            if isinstance(db_time, str):
                db_time = datetime.fromisoformat(db_time.replace("Z", "+00:00"))
            db_time = db_time.replace(microsecond=0)
            existing_map[row["notion_page_id"]] = db_time

    except Exception as e:
        logger.error(f"Failed to fetch existing pages: {e}")
        return list(page_map.keys()), []

    # Compare
    new_page_ids = []
    updated_page_ids = []

    for page_id, notion_time in page_map.items():
        if page_id not in existing_map:
            new_page_ids.append(page_id)
        elif notion_time > existing_map[page_id]:
            updated_page_ids.append(page_id)

    logger.info(
        f"Change detection: {len(new_page_ids)} new, {len(updated_page_ids)} updated, "
        f"{len(page_map) - len(new_page_ids) - len(updated_page_ids)} unchanged"
    )

    return new_page_ids, updated_page_ids
```

**Testing:**
1. Initial import: 726 pages â†’ All fetched (expected)
2. Re-import immediately: 0 pages fetched, 726 skipped (success metric)
3. Add 1 page in Notion: 1 fetched, 726 skipped
4. Modify 10 pages: 10 fetched, 716 skipped

**Performance Gains:**
- Current: 9 minutes every import
- After fix: <5 seconds for re-import without changes (99.1% reduction)

### Future Migration Path (When DB > 5,000 pages)

**Trigger:** DB performance degradation or page count > 5,000

**Approach:** Migrate to Solution 3 (Server-side RPC comparison)

**Steps:**
1. Deploy SQL function to Supabase
2. Add environment variable: `USE_RPC_CHANGE_DETECTION=true`
3. Update `get_pages_to_fetch()` to call RPC when enabled
4. Monitor performance (should be ~150ms regardless of DB size)

**No action needed now** - Implement when scaling issues occur.

---

## ğŸ”® Future Scenario: ì—¬ëŸ¬ ë¶€ëª¨ í˜ì´ì§€ ì§€ì› ì‹œ ì˜í–¥ ë¶„ì„

### ì‹œë‚˜ë¦¬ì˜¤ ê°€ì •

**í˜„ì¬:** 1ê°œ ë¶€ëª¨ í˜ì´ì§€ â†’ 726ê°œ í•˜ìœ„ í˜ì´ì§€
**ë¯¸ë˜:** Nê°œ ë¶€ëª¨ í˜ì´ì§€ â†’ ê°ê° ìˆ˜ë°± ê°œ í•˜ìœ„ í˜ì´ì§€

**ì˜ˆì‹œ:**
- ë¶€ëª¨ í˜ì´ì§€ A: 500ê°œ í•˜ìœ„ í˜ì´ì§€
- ë¶€ëª¨ í˜ì´ì§€ B: 300ê°œ í•˜ìœ„ í˜ì´ì§€
- ë¶€ëª¨ í˜ì´ì§€ C: 200ê°œ í•˜ìœ„ í˜ì´ì§€
- **ì´ DB:** 1,000ê°œ í˜ì´ì§€

### Import íŒ¨í„´ ë³€í™”

#### íŒ¨í„´ 1: ì „ì²´ ë¶€ëª¨ ë™ì‹œ Import
```python
# ì‚¬ìš©ìê°€ "ëª¨ë“  ë¶€ëª¨ ë™ê¸°í™”" ë²„íŠ¼ í´ë¦­
# â†’ 1,000ê°œ ì „ì²´ í˜ì´ì§€ ì²´í¬
for parent_id in parent_page_ids:
    pages = await fetch_child_pages_from_parent(parent_id)
    # 1,000ê°œ í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
```

#### íŒ¨í„´ 2: íŠ¹ì • ë¶€ëª¨ë§Œ ì„ íƒ Import (ë” ì¼ë°˜ì )
```python
# ì‚¬ìš©ìê°€ "ë¶€ëª¨ Aë§Œ ë™ê¸°í™”" ì„ íƒ
# â†’ 500ê°œë§Œ ì²´í¬ (ë¶€ëª¨ Aì˜ í•˜ìœ„ í˜ì´ì§€ë§Œ)
pages = await fetch_child_pages_from_parent(parent_A_id)
# 500ê°œ í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
```

### ì„±ëŠ¥ ë¹„êµ: ë°©ì‹ A vs Solution 3

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ì „ì²´ ë™ê¸°í™” (1,000ê°œ ì¤‘ 1,000ê°œ ì²´í¬)

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | Solution 3 (RPC) | ìŠ¹ì |
|------|-------------------|------------------|------|
| DB ì¿¼ë¦¬ | 1íšŒ (ì „ì²´ 1,000ê°œ) | 1íšŒ (RPC) | ë™ì¼ |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ìš”ì²­) | ìµœì†Œ | 1,000 Ã— 80 bytes = 80KB | A |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ì‘ë‹µ) | 1,000 Ã— 60 = 60KB | ë³€ê²½ëœ ê²ƒë§Œ (~10ê°œ = 0.4KB) | **Sol 3** |
| DB ì²˜ë¦¬ ì‹œê°„ | 150ms (full scan) | 120ms (indexed lookup) | Sol 3 |
| ì´ ì‹œê°„ | **~200ms** | **~170ms** | **Sol 3 (ì•½ê°„ ë¹ ë¦„)** |

**ê²°ë¡ :** ê±°ì˜ ë¹„ìŠ·. ë°©ì‹ A ì¶©ë¶„íˆ ë¹ ë¦„.

---

#### ì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • ë¶€ëª¨ë§Œ ë™ê¸°í™” (1,000ê°œ ì¤‘ 500ê°œë§Œ ì²´í¬) â­ ê°€ì¥ ì¼ë°˜ì 

| í•­ëª© | ë°©ì‹ A (ì „ì²´ ì¡°íšŒ) | Solution 3 (RPC) | ìŠ¹ì |
|------|-------------------|------------------|------|
| DB ì¿¼ë¦¬ | 1íšŒ (ì „ì²´ 1,000ê°œ) | 1íšŒ (RPC, 500ê°œë§Œ ë¹„êµ) | Sol 3 |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ìš”ì²­) | ìµœì†Œ | 500 Ã— 80 = 40KB | A |
| ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (ì‘ë‹µ) | **1,000 Ã— 60 = 60KB** âŒ | **ë³€ê²½ëœ ê²ƒë§Œ (~5ê°œ = 0.2KB)** âœ… | **Sol 3 (300ë°° ì ìŒ)** |
| DB ì²˜ë¦¬ ì‹œê°„ | 150ms (1,000ê°œ ìŠ¤ìº”) | 80ms (500ê°œë§Œ lookup) | **Sol 3** |
| ì´ ì‹œê°„ | **~200ms** | **~120ms** | **Sol 3 (1.7ë°° ë¹ ë¦„)** |

**ê²°ë¡ :** Solution 3ì´ í™•ì‹¤íˆ ìœ ë¦¬!

---

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ì—¬ëŸ¬ ë¶€ëª¨ ìˆœì°¨ ë™ê¸°í™” (5ê°œ ë¶€ëª¨ Ã— ê° 200ê°œ = 1,000ê°œ DB)

**ì‚¬ìš©ì ì›Œí¬í”Œë¡œìš°:**
```
1. ë¶€ëª¨ A ë™ê¸°í™” (200ê°œ ì²´í¬) â†’ 5ì´ˆ í›„
2. ë¶€ëª¨ B ë™ê¸°í™” (200ê°œ ì²´í¬) â†’ 1ë¶„ í›„
3. ë¶€ëª¨ C ë™ê¸°í™” (200ê°œ ì²´í¬) â†’ 3ë¶„ í›„
...
```

**ë°©ì‹ A (ì „ì²´ ì¡°íšŒ):**
```python
# ë§¤ë²ˆ 1,000ê°œ ì „ì²´ ì¡°íšŒ (ë¶ˆí•„ìš”í•œ 800ê°œ í¬í•¨)
ë™ê¸°í™” 1íšŒë‹¹: 200ms
5íšŒ ë™ê¸°í™”: 5 Ã— 200ms = 1,000ms
ë¶ˆí•„ìš”í•œ ë°ì´í„° ì „ì†¡: 5 Ã— 800ê°œ Ã— 60 bytes = 240KB
```

**Solution 3 (RPC):**
```python
# ë§¤ë²ˆ 200ê°œë§Œ ì „ì†¡ ë° ë¹„êµ
ë™ê¸°í™” 1íšŒë‹¹: 100ms
5íšŒ ë™ê¸°í™”: 5 Ã— 100ms = 500ms
ë¶ˆí•„ìš”í•œ ë°ì´í„° ì „ì†¡: 0KB
```

**ì„±ëŠ¥ ì°¨ì´:**
- ì‹œê°„: 2ë°° ë¹ ë¦„ (1,000ms vs 500ms)
- ë„¤íŠ¸ì›Œí¬: 240KB ì ˆì•½

**ê²°ë¡ :** Solution 3 ì••ë„ì  ìŠ¹ë¦¬!

---

### ìŠ¤ì¼€ì¼ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„

| DB í¬ê¸° | ë¶€ëª¨ ê°œìˆ˜ | Import í¬ê¸° (ë‹¨ì¼ ë¶€ëª¨) | Import/DB ë¹„ìœ¨ | ë°©ì‹ A | Solution 3 | ìŠ¹ì |
|---------|----------|----------------------|--------------|--------|-----------|------|
| 1,000 | 2 | 500 | 50% | 200ms | **120ms** | **Sol 3** |
| 5,000 | 5 | 1,000 | 20% | 600ms | **150ms** | **Sol 3 (4ë°°)** |
| 10,000 | 10 | 1,000 | 10% | 1,200ms | **150ms** | **Sol 3 (8ë°°)** |
| 50,000 | 20 | 2,500 | 5% | 5,000ms | **200ms** | **Sol 3 (25ë°°)** |

**íŒ¨í„´:** DBê°€ í¬ê³ , Importê°€ ë¶€ë¶„ ì§‘í•©ì¼ìˆ˜ë¡ Solution 3ì´ ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬!

---

### ê²°ë¡ : ì—¬ëŸ¬ ë¶€ëª¨ í˜ì´ì§€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ”?

#### âœ… Solution 3ì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì •ë‹µ!

**ì´ìœ :**

1. **ë¶€ë¶„ Importê°€ ì¼ë°˜ì :**
   - ì‚¬ìš©ìëŠ” ë³´í†µ "ì „ì²´ ë™ê¸°í™”"ë³´ë‹¤ "íŠ¹ì • ë¶€ëª¨ë§Œ ë™ê¸°í™”" ì‚¬ìš©
   - ë°©ì‹ AëŠ” ë§¤ë²ˆ ì „ì²´ DBë¥¼ ê°€ì ¸ì˜´ (ë¹„íš¨ìœ¨)

2. **ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ˆì•½:**
   - DB 10,000ê°œ, Import 1,000ê°œ ì‹œ: 60KB vs 0.5KB (120ë°° ì°¨ì´)
   - ëª¨ë°”ì¼/ì €ëŒ€ì—­í­ í™˜ê²½ì—ì„œ ìœ ë¦¬

3. **DB ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•:**
   - Full scan: O(ì „ì²´ DB í¬ê¸°)
   - RPC: O(Import í¬ê¸°)

4. **í™•ì¥ì„±:**
   - ë¶€ëª¨ 100ê°œ, DB 100,000ê°œë¡œ ì¦ê°€í•´ë„ ì„±ëŠ¥ ìœ ì§€
   - ë°©ì‹ AëŠ” ìˆ˜ ì´ˆ ë‹¨ìœ„ë¡œ ëŠë ¤ì§

5. **ì‚¬ìš©ì ê²½í—˜:**
   - "ë¶€ëª¨ A ë™ê¸°í™”" í´ë¦­ â†’ ì¦‰ì‹œ ì™„ë£Œ (100ms)
   - vs. "ë¶€ëª¨ A ë™ê¸°í™”" í´ë¦­ â†’ 1ì´ˆ ëŒ€ê¸° (ì „ì²´ DB ìŠ¤ìº”)

---

### ìˆ˜ì •ëœ ê¶Œì¥ì‚¬í•­

#### í˜„ì¬ (ë‹¨ì¼ ë¶€ëª¨ í˜ì´ì§€ë§Œ ì§€ì›)
- **ë°©ì‹ A êµ¬í˜„** (ê°„ë‹¨, ì¶©ë¶„íˆ ë¹ ë¦„)
- ì´ìœ : DB 732 vs Import 726 (99% ì¤‘ë³µ)

#### ë¯¸ë˜ (ì—¬ëŸ¬ ë¶€ëª¨ í˜ì´ì§€ ì§€ì› ì˜ˆì •)
- **Solution 3ì„ ì§€ê¸ˆ ë°”ë¡œ êµ¬í˜„** â­ (ì¶”ì²œ!)
- ì´ìœ :
  1. ë¶€ë¶„ Import ì‹œ 3~25ë°° ë¹ ë¦„
  2. ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ 120ë°° ì ˆì•½
  3. í™•ì¥ì„± ë³´ì¥
  4. í•œ ë²ˆë§Œ êµ¬í˜„í•˜ë©´ ë¨

---

### êµ¬í˜„ ë¹„ìš© ë¹„êµ

| ë°©ì‹ | êµ¬í˜„ ì‹œê°„ | ì½”ë“œ ë¼ì¸ ìˆ˜ | ì¸í”„ë¼ ë³€ê²½ | ìœ ì§€ë³´ìˆ˜ |
|------|----------|------------|-----------|---------|
| ë°©ì‹ A | 30ë¶„ | +50 lines | ì—†ìŒ | ì‰¬ì›€ |
| Solution 3 | 2ì‹œê°„ | +120 lines | SQL í•¨ìˆ˜ ë°°í¬ | ì¤‘ê°„ |
| ì°¨ì´ | +1.5ì‹œê°„ | +70 lines | SQL 1ê°œ | ì•½ê°„ ë³µì¡ |

**ì¶”ê°€ ë¹„ìš©:** 1.5ì‹œê°„ ê°œë°œ ì‹œê°„

**ì–»ëŠ” ê²ƒ:**
- ë¯¸ë˜ í™•ì¥ì„± ë³´ì¥
- ë¶€ë¶„ Import ì‹œ 3~25ë°° ì„±ëŠ¥ í–¥ìƒ
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ 100ë°° ì ˆì•½
- ì¬ì‘ì—… ë¶ˆí•„ìš” (í•œ ë²ˆì— ë)

---

### ìµœì¢… ê¶Œì¥ì‚¬í•­ (ì—¬ëŸ¬ ë¶€ëª¨ í˜ì´ì§€ ê³ ë ¤ ì‹œ)

**ì§€ê¸ˆ ë°”ë¡œ Solution 3 êµ¬í˜„!**

**êµ¬í˜„ ë‹¨ê³„:**

1. **Supabase SQL í•¨ìˆ˜ ë°°í¬:**
```sql
CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb AS $$
DECLARE
    result jsonb;
    new_ids text[];
    updated_ids text[];
    page_record jsonb;
    notion_id text;
    notion_time timestamptz;
    db_time timestamptz;
BEGIN
    new_ids := ARRAY[]::text[];
    updated_ids := ARRAY[]::text[];

    FOR page_record IN SELECT * FROM jsonb_array_elements(pages_data)
    LOOP
        notion_id := page_record->>'id';
        notion_time := (page_record->>'last_edited')::timestamptz;

        SELECT notion_last_edited_time INTO db_time
        FROM raw_notes
        WHERE notion_page_id = notion_id;

        IF NOT FOUND THEN
            new_ids := array_append(new_ids, notion_id);
        ELSIF notion_time > db_time THEN
            updated_ids := array_append(updated_ids, notion_id);
        END IF;
    END LOOP;

    result := jsonb_build_object(
        'new_page_ids', to_jsonb(new_ids),
        'updated_page_ids', to_jsonb(updated_ids)
    );

    RETURN result;
END;
$$ LANGUAGE plpgsql;
```

2. **Python ì½”ë“œ ì—…ë°ì´íŠ¸:**
```python
# backend/services/supabase_service.py
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict[str, Any]]
) -> tuple[List[str], List[str]]:
    """Compare Notion pages with DB using server-side RPC."""
    await self._ensure_initialized()

    # Prepare data for RPC
    pages_json = []
    for p in notion_pages:
        page_id = p.get("id")
        last_edited = p.get("last_edited_time")
        if not page_id or not last_edited:
            continue

        # Truncate to seconds for comparison
        notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
        notion_time = notion_time.replace(microsecond=0)

        pages_json.append({
            "id": page_id,
            "last_edited": notion_time.isoformat()
        })

    if not pages_json:
        return [], []

    try:
        # Call RPC function (HTTP POST, no URL limit)
        response = await self.client.rpc('get_changed_pages', {
            'pages_data': pages_json
        }).execute()

        result = response.data
        new_page_ids = result.get('new_page_ids', [])
        updated_page_ids = result.get('updated_page_ids', [])

        logger.info(
            f"Change detection: {len(new_page_ids)} new, {len(updated_page_ids)} updated, "
            f"{len(pages_json) - len(new_page_ids) - len(updated_page_ids)} unchanged"
        )

        return new_page_ids, updated_page_ids

    except Exception as e:
        logger.error(f"RPC change detection failed: {e}")
        # Fallback: treat all as new
        return [p["id"] for p in pages_json], []
```

3. **í…ŒìŠ¤íŠ¸:**
   - ì´ˆê¸° import: 726ê°œ ì „ì²´ fetch
   - ì¬ì‹¤í–‰: 0ê°œ fetch, 726 skipped
   - 1ê°œ ì¶”ê°€: 1ê°œ fetch
   - í–¥í›„ ë¶€ëª¨ B ì¶”ê°€ ì‹œ: ë¶€ëª¨ Bë§Œ fetch

**ROI (íˆ¬ì ëŒ€ë¹„ íš¨ê³¼):**
- íˆ¬ì: 1.5ì‹œê°„ ê°œë°œ ì‹œê°„
- íš¨ê³¼:
  - í˜„ì¬: ë°©ì‹ Aì™€ ë¹„ìŠ· (110ms vs 150ms)
  - ë¯¸ë˜: 3~25ë°° ë¹ ë¦„ + ì¬ì‘ì—… ë¶ˆí•„ìš”

**ê²°ë¡ :** ì—¬ëŸ¬ ë¶€ëª¨ í˜ì´ì§€ ì§€ì› ì˜ˆì •ì´ë¼ë©´ **Solution 3ì„ ì§€ê¸ˆ êµ¬í˜„í•˜ëŠ” ê²ƒì´ í˜„ëª…í•¨!**

---

## ğŸ”´ CRITICAL PRE-IMPLEMENTATION VERIFICATION REQUIRED

### Must-Test Before Implementation

**Test 1: Verify last_edited_time reflects content changes**

```bash
# Manual test procedure:
1. Notionì—ì„œ ì„ì˜ì˜ í˜ì´ì§€ ì„ íƒ (ì˜ˆ: ì²« ë²ˆì§¸ í˜ì´ì§€)
2. í˜ì´ì§€ ID í™•ì¸
3. APIë¡œ í˜„ì¬ last_edited_time í™•ì¸
4. Notionì—ì„œ í˜ì´ì§€ ë‚´ìš© ìˆ˜ì • (í…ìŠ¤íŠ¸ ì¶”ê°€)
5. 1ë¶„ ëŒ€ê¸° (Notion indexing)
6. APIë¡œ ë‹¤ì‹œ last_edited_time í™•ì¸
7. ë¹„êµ: ìˆ˜ì • ì „ vs ìˆ˜ì • í›„

Expected: last_edited_timeì´ ë³€ê²½ë¨
If NOT: Plan ë¬´íš¨í™”, ëŒ€ì•ˆ í•„ìš”
```

**Test Code:**
```python
import asyncio
from services.notion_service import NotionService
from datetime import datetime

async def test_last_edited_time_accuracy():
    service = NotionService()
    parent_id = os.getenv("NOTION_PARENT_PAGE_ID")

    # Get first page
    pages = await service.fetch_child_pages_from_parent(parent_id, page_size=1)
    if not pages:
        print("No pages found")
        return

    page = pages[0]
    page_id = page["id"]
    initial_time = page["last_edited_time"]

    print(f"Page ID: {page_id}")
    print(f"Initial last_edited_time: {initial_time}")
    print(f"\nâš ï¸  NOW: Go to Notion and edit this page's content")
    print(f"URL: https://notion.so/{page_id.replace('-', '')}")
    print(f"\nPress Enter after editing...")
    input()

    # Fetch again
    pages_after = await service.fetch_child_pages_from_parent(parent_id, page_size=100)
    page_after = next(p for p in pages_after if p["id"] == page_id)
    final_time = page_after["last_edited_time"]

    print(f"\nFinal last_edited_time: {final_time}")
    print(f"Changed: {initial_time != final_time}")

    if initial_time == final_time:
        print("\nâŒ CRITICAL: last_edited_time NOT updated after content change!")
        print("   Plan is NOT viable. Need alternative approach.")
    else:
        print("\nâœ… SUCCESS: last_edited_time updated correctly!")
        print("   Plan is viable. Proceed with implementation.")

asyncio.run(test_last_edited_time_accuracy())
```

**Decision Point:**
- âœ… If test passes â†’ Proceed with implementation
- âŒ If test fails â†’ Switch to Alternative Plan B

---

## Alternative Plan B: If last_edited_time Not Reliable

**Approach:** Use `pages.retrieve()` API for accurate timestamps

**Changes to Plan:**

### Phase 1: Metadata Fetch with pages.retrieve()
```python
async def fetch_child_pages_with_accurate_timestamps(
    self,
    parent_page_id: str
) -> List[Dict]:
    """Fetch child pages with accurate last_edited_time."""

    # Step 1: Get child page IDs (lightweight)
    child_blocks = await self.fetch_child_pages_from_parent(parent_page_id)

    # Step 2: Batch retrieve full page objects (for accurate timestamps)
    accurate_pages = []
    for page in child_blocks:
        page_id = page["id"]

        # Call pages.retrieve() API
        try:
            await self.rate_limiter.acquire()
            full_page = await asyncio.to_thread(
                self.client.pages.retrieve,
                page_id=page_id
            )
            accurate_pages.append(full_page)
        except Exception as e:
            logger.warning(f"Failed to retrieve page {page_id}: {e}")
            # Fallback to block timestamp
            accurate_pages.append(page)

    return accurate_pages
```

**Impact:**
- API calls: 724 (one per page for metadata)
- Time: ~241 seconds (4 minutes) at 3 req/sec
- Still faster than content fetch (no block content)
- More API quota usage

**Trade-off Analysis:**
- Current full fetch: 724 content calls (~9 min)
- Plan A (incremental, if viable): 0-100 content calls (~0-30 sec)
- Plan B (pages.retrieve): 724 metadata + N content calls (~4 min + NÃ—0.33 sec)

**Example Scenarios (Plan B):**
- 1 page changed: 724 metadata + 1 content = ~4 min
- 10 pages changed: 724 metadata + 10 content = ~4 min
- No changes: 724 metadata + 0 content = ~4 min

**Conclusion Plan B:**
- NOT as efficient as Plan A
- But still ~2x faster than current (9 min vs 4 min)
- Guaranteed accuracy

---

## Summary of Critical Issues

| Issue | Severity | Impact | Solution Status |
|-------|----------|--------|-----------------|
| 1. Microseconds mismatch | HIGH | False positives | âœ… Solved (truncate) |
| 2. Block vs Page timestamp | **CRITICAL** | Plan viability | âš ï¸  **NEEDS TESTING** |
| 3. DB timestamp source | MEDIUM | Consistency | âœ… Same as Issue 2 |
| 4. Timezone handling | MEDIUM | Comparison errors | âœ… Solved (ensure aware) |
| 5. Race condition | LOW | Missed updates | âœ… Accepted (eventual) |
| 6. Pagination duplicates | LOW | Minor inefficiency | âœ… Solved (dedupe) |
| 7. Memory/query limits | LOW | Scale issues | âœ… Solved (batch) |
| 8. DB index duplicates | LOW | Logic errors | âœ… Solved (handle) |

**Next Step:**
**USER MUST RUN TEST 1 BEFORE PROCEEDING WITH IMPLEMENTATION**

If Test 1 passes â†’ Use Plan A (original incremental update)
If Test 1 fails â†’ Use Plan B (pages.retrieve for timestamps)

---

## ğŸ“‹ Solution 3 êµ¬í˜„ ê³„íš (ì‹¬ì¸µ ë¶„ì„)

### ğŸ¤” êµ¬í˜„ ì „ ê¹Šì€ ê³ ë¯¼ ì‚¬í•­

#### 1. SQL í•¨ìˆ˜ ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­

**ë¬¸ì œ 1: JSON ë°ì´í„° í¬ê¸° ì œí•œ**
```sql
-- PostgreSQL jsonb í¬ê¸°: ë©”ëª¨ë¦¬ ì œí•œë§Œ (ì‹¤ì§ˆì ìœ¼ë¡œ ë¬´ì œí•œ)
-- HTTP POST body: Supabase ê¸°ë³¸ 10MB
-- í˜„ì¬: 726ê°œ Ã— 80 bytes = 58KB âœ… ì•ˆì „
-- ë¯¸ë˜: 10,000ê°œ Ã— 80 bytes = 800KB âœ… ì—¬ì „íˆ ì•ˆì „
```
âœ… **ê²°ë¡ :** í¬ê¸° ì œí•œ ë¬¸ì œ ì—†ìŒ

**ë¬¸ì œ 2: Timestamp í˜•ì‹ ë¶ˆì¼ì¹˜**
```python
# Python â†’ SQL
notion_time.isoformat()  # "2024-01-15T14:30:00+00:00"

# SQL íŒŒì‹±
(page_record->>'last_edited')::timestamptz
# PostgreSQLì´ ISO 8601 ìë™ ì¸ì‹ âœ…
```
âœ… **ê²°ë¡ :** í˜•ì‹ í˜¸í™˜ ë³´ì¥ë¨

**ë¬¸ì œ 3: Microsecond ì •ë°€ë„**
```sql
-- Notion: ë°€ë¦¬ì´ˆ (000)
-- DB: ë§ˆì´í¬ë¡œì´ˆ (123456)

-- í•´ê²°: SQLì—ì„œ truncate
date_trunc('second', notion_time) > date_trunc('second', db_time)
```
âœ… **ê²°ë¡ :** SQL í•¨ìˆ˜ ë‚´ì—ì„œ ì²˜ë¦¬ â†’ Python ì½”ë“œ ê°„ì†Œí™”

**ë¬¸ì œ 4: ì¸ë±ìŠ¤ í™œìš©**
```sql
-- ê¸°ì¡´ ì¸ë±ìŠ¤
CREATE INDEX idx_raw_notes_notion_page_id ON raw_notes(notion_page_id);

-- SQL í•¨ìˆ˜
WHERE notion_page_id = notion_id;  -- âœ… ì¸ë±ìŠ¤ ìë™ í™œìš©

-- ì„±ëŠ¥: O(log n) Ã— 726ë²ˆ = ë¹ ë¦„
```
âœ… **ê²°ë¡ :** ì¶”ê°€ ì¸ë±ìŠ¤ ë¶ˆí•„ìš”

**ë¬¸ì œ 5: NULL ì²˜ë¦¬**
```sql
-- ìŠ¤í‚¤ë§ˆ í™•ì¸
notion_last_edited_time TIMESTAMPTZ NOT NULL,  -- âœ… NOT NULL ì œì•½
```
âœ… **ê²°ë¡ :** NULL ì²˜ë¦¬ ë¶ˆí•„ìš”

---

#### 2. Python ì½”ë“œ ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­

**ë¬¸ì œ 1: RPC ì‹¤íŒ¨ ì‹œ Fallback ì „ëµ**

**Option A: ì „ì²´ë¥¼ "new"ë¡œ ì²˜ë¦¬ (í˜„ì¬)**
```python
except Exception:
    return [all page ids], []  # ë¹„íš¨ìœ¨ì 
```
âŒ **ë¬¸ì œ:** ì „ì²´ ì¬import (9ë¶„)

**Option B: ë°©ì‹ Aë¡œ Fallback (ì œì•ˆ)**
```python
except Exception:
    # Full table scanìœ¼ë¡œ ì „í™˜
    return await self._full_scan_fallback(...)
```
âœ… **ì¥ì :** RPC ì‹¤íŒ¨í•´ë„ ìµœì  ì„±ëŠ¥ ìœ ì§€

**ê²°ì •:** Option B ì±„íƒ

---

**ë¬¸ì œ 2: Timestamp íŒŒì‹± ì—ëŸ¬ ì²˜ë¦¬**

**í˜„ì¬:**
```python
try:
    notion_time = parse(last_edited)
except ValueError:
    continue  # Skip â†’ ë™ê¸°í™” ì•ˆ ë¨! âŒ
```

**ê°œì„ :**
```python
except ValueError:
    logger.warning(f"{page_id} has invalid timestamp, treating as new")
    force_new_ids.append(page_id)  # âœ… ë™ê¸°í™” ë³´ì¥
```

---

**ë¬¸ì œ 3: RPC ì‘ë‹µ ê²€ì¦**

**í•„ìˆ˜ ê²€ì¦ í•­ëª©:**
1. ì‘ë‹µ í˜•ì‹ (`dict`)
2. í•„ìˆ˜ í‚¤ ì¡´ì¬ (`new_page_ids`, `updated_page_ids`)
3. ê°’ íƒ€ì… (`list`)
4. UUID í˜•ì‹ (ì •ê·œí‘œí˜„ì‹)
5. SQL ì—ëŸ¬ ì²´í¬ (`error` í‚¤)

```python
# ê²€ì¦ ë¡œì§
if 'error' in result:
    raise ValueError(f"SQL error: {result['error']}")

if not isinstance(new_ids, list):
    raise ValueError("Invalid response type")

UUID_PATTERN = re.compile(r'^[0-9a-f-]{36}$')
for id in new_ids:
    if not UUID_PATTERN.match(id):
        raise ValueError(f"Invalid UUID: {id}")
```

---

**ë¬¸ì œ 4: ì„±ëŠ¥ ì¸¡ì •**

```python
import time

start = time.time()
response = await self.client.rpc(...)
elapsed = time.time() - start

logger.info(f"RPC completed in {elapsed:.2f}s")
```

**ëª©ì :**
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìµœì í™” íš¨ê³¼ í™•ì¸
- ë¬¸ì œ ì¡°ê¸° ë°œê²¬

---

#### 3. SQL í•¨ìˆ˜ ë°°í¬ ì „ëµ

**ë¬¸ì œ 1: ë²„ì „ ê´€ë¦¬**

**ë°©ë²• 1: SQL Editor (ê°„ë‹¨, ë²„ì „ ê´€ë¦¬ ì•ˆ ë¨)**
- Supabase Dashboard â†’ SQL Editor â†’ ì‹¤í–‰

**ë°©ë²• 2: ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ (ì¶”ì²œ)**
```bash
backend/docs/supabase_migrations/001_get_changed_pages.sql
```

âœ… **ì¥ì :**
- Git ë²„ì „ ê´€ë¦¬
- íŒ€ì› ê³µìœ  ì‰¬ì›€
- Dev/Staging/Prod ì¬ë°°í¬ ê°€ëŠ¥

**ê²°ì •:** ë°©ë²• 2 ì±„íƒ (íŒŒì¼ë¡œ ê´€ë¦¬)

---

**ë¬¸ì œ 2: ë°°í¬ ì „ í…ŒìŠ¤íŠ¸**

**í…ŒìŠ¤íŠ¸ SQL:**
```sql
-- 1. ì‹ ê·œ í˜ì´ì§€
SELECT get_changed_pages('[
    {"id": "new-page-id", "last_edited": "2024-01-15T14:30:00+00:00"}
]'::jsonb);
-- ì˜ˆìƒ: {"new_page_ids": ["new-page-id"]}

-- 2. ê¸°ì¡´ í˜ì´ì§€ (ë³€ê²½ ì—†ìŒ)
SELECT get_changed_pages('[
    {"id": "existing-unchanged", "last_edited": "ê¸°ì¡´ timestamp"}
]'::jsonb);
-- ì˜ˆìƒ: {"new_page_ids": [], "updated_page_ids": []}

-- 3. ìˆ˜ì •ëœ í˜ì´ì§€
SELECT get_changed_pages('[
    {"id": "existing-modified", "last_edited": "ìµœì‹  timestamp"}
]'::jsonb);
-- ì˜ˆìƒ: {"updated_page_ids": ["existing-modified"]}

-- 4. ë¹ˆ ë°°ì—´
SELECT get_changed_pages('[]'::jsonb);
-- ì˜ˆìƒ: ì—ëŸ¬ ì—†ì´ ë¹ˆ ê²°ê³¼

-- 5. ì˜ëª»ëœ í˜•ì‹
SELECT get_changed_pages('[{"invalid": "data"}]'::jsonb);
-- ì˜ˆìƒ: EXCEPTION ì²˜ë¦¬ë¡œ ì—ëŸ¬ ì •ë³´ ë°˜í™˜
```

**í•„ìˆ˜ í™•ì¸ í•­ëª©:**
âœ… ì‹ ê·œ í˜ì´ì§€ ê°ì§€
âœ… ìˆ˜ì • í˜ì´ì§€ ê°ì§€
âœ… ë³€ê²½ ì—†ëŠ” í˜ì´ì§€ skip
âœ… ë¹ˆ ì…ë ¥ ì²˜ë¦¬
âœ… ì—ëŸ¬ í•¸ë“¤ë§

---

#### 4. ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘

**ì‹œë‚˜ë¦¬ì˜¤ 1: SQL í•¨ìˆ˜ ë¯¸ë°°í¬**
```python
try:
    response = await self.client.rpc('get_changed_pages', ...)
except Exception as e:
    if "does not exist" in str(e).lower():
        logger.error("RPC function not deployed!")
        # Fallback to ë°©ì‹ A
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: SQL ì‹¤í–‰ ì˜¤ë¥˜**
```sql
EXCEPTION
    WHEN OTHERS THEN
        result := jsonb_build_object(
            'error', SQLERRM,  -- ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨
            'new_page_ids', '[]'::jsonb,
            'updated_page_ids', '[]'::jsonb
        );
        RETURN result;
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ**
```python
import asyncio

try:
    response = await asyncio.wait_for(
        self.client.rpc(...),
        timeout=30.0  # 30ì´ˆ
    )
except asyncio.TimeoutError:
    logger.error("RPC timeout, using fallback")
    # Fallback
```

---

#### 5. ì„±ëŠ¥ ìµœì í™” ê³ ë ¤ì‚¬í•­

**í˜„ì¬ êµ¬í˜„: LOOP ë°©ì‹**
```sql
FOR page_record IN SELECT * FROM jsonb_array_elements(...)
LOOP
    SELECT ... WHERE notion_page_id = notion_id;  -- 726ë²ˆ ì‹¤í–‰
    ...
END LOOP;
```
- **ì„±ëŠ¥:** O(n log n) - ì¸ë±ìŠ¤ í™œìš©
- **ì‹œê°„:** ~100ms (726ê°œ)
- **ì¥ì :** ì½”ë“œ ê°„ë‹¨, ì´í•´ ì‰¬ì›€

**ëŒ€ì•ˆ: Bulk JOIN ë°©ì‹**
```sql
WITH notion_data AS (
    SELECT
        elem->>'id' AS notion_id,
        date_trunc('second', (elem->>'last_edited')::timestamptz) AS notion_time
    FROM jsonb_array_elements(pages_data) AS elem
)
SELECT
    nd.notion_id,
    CASE
        WHEN rn.notion_page_id IS NULL THEN 'new'
        WHEN nd.notion_time > date_trunc('second', rn.notion_last_edited_time) THEN 'updated'
        ELSE 'unchanged'
    END AS status
FROM notion_data nd
LEFT JOIN raw_notes rn ON rn.notion_page_id = nd.notion_id;
```
- **ì„±ëŠ¥:** O(n) - í•œ ë²ˆì˜ JOIN
- **ì‹œê°„:** ~80ms ì˜ˆìƒ
- **ë‹¨ì :** ì½”ë“œ ë³µì¡, ë””ë²„ê¹… ì–´ë ¤ì›€

**ê²°ì •:**
- **Phase 1:** LOOP ë°©ì‹ êµ¬í˜„ (ê°„ë‹¨)
- **Phase 2 (ì„ íƒ):** ì„±ëŠ¥ ì´ìŠˆ ì‹œ JOIN ë°©ì‹ìœ¼ë¡œ ìµœì í™”

---

### ğŸ“ êµ¬í˜„ ìˆœì„œ (Phaseë³„ ìƒì„¸)

#### Phase 1: SQL í•¨ìˆ˜ ìƒì„± (30ë¶„)

**íŒŒì¼:** `backend/docs/supabase_migrations/001_get_changed_pages.sql`

```sql
-- Incremental Import: Change Detection Function
-- Created: 2024-01-XX
-- Purpose: ë…¸ì…˜ í˜ì´ì§€ ë©”íƒ€ë°ì´í„°ì™€ DB ë¹„êµí•˜ì—¬ ë³€ê²½ ê°ì§€

CREATE OR REPLACE FUNCTION get_changed_pages(pages_data jsonb)
RETURNS jsonb
LANGUAGE plpgsql
AS $$
DECLARE
    result jsonb;
    new_ids text[] := ARRAY[]::text[];
    updated_ids text[] := ARRAY[]::text[];
    page_record jsonb;
    notion_id text;
    notion_time timestamptz;
    db_time timestamptz;
BEGIN
    -- ê° Notion í˜ì´ì§€ ì²˜ë¦¬
    FOR page_record IN SELECT * FROM jsonb_array_elements(pages_data)
    LOOP
        -- JSONì—ì„œ ë°ì´í„° ì¶”ì¶œ
        notion_id := page_record->>'id';
        notion_time := (page_record->>'last_edited')::timestamptz;

        -- ì´ˆ ë‹¨ìœ„ë¡œ truncate (microsecond ì°¨ì´ ë¬´ì‹œ)
        notion_time := date_trunc('second', notion_time);

        -- DBì—ì„œ ê¸°ì¡´ í˜ì´ì§€ ì¡°íšŒ (ì¸ë±ìŠ¤ í™œìš©)
        SELECT date_trunc('second', notion_last_edited_time) INTO db_time
        FROM raw_notes
        WHERE notion_page_id = notion_id;

        -- ë¹„êµ ë° ë¶„ë¥˜
        IF NOT FOUND THEN
            -- ì‹ ê·œ í˜ì´ì§€
            new_ids := array_append(new_ids, notion_id);
        ELSIF notion_time > db_time THEN
            -- ìˆ˜ì •ëœ í˜ì´ì§€ (Notion timestampê°€ ë” ìµœì‹ )
            updated_ids := array_append(updated_ids, notion_id);
        END IF;
        -- ELSE: unchanged (skip)
    END LOOP;

    -- ê²°ê³¼ ë°˜í™˜
    result := jsonb_build_object(
        'new_page_ids', to_jsonb(new_ids),
        'updated_page_ids', to_jsonb(updated_ids),
        'total_checked', jsonb_array_length(pages_data),
        'unchanged_count', jsonb_array_length(pages_data) - COALESCE(array_length(new_ids, 1), 0) - COALESCE(array_length(updated_ids, 1), 0)
    );

    RETURN result;

EXCEPTION
    WHEN OTHERS THEN
        -- ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ì •ë³´ì™€ í•¨ê»˜ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        -- Pythonì—ì„œ Fallback ë¡œì§ ì‘ë™
        result := jsonb_build_object(
            'error', SQLERRM,
            'error_detail', SQLSTATE,
            'new_page_ids', '[]'::jsonb,
            'updated_page_ids', '[]'::jsonb
        );
        RETURN result;
END;
$$;

-- í•¨ìˆ˜ ì„¤ëª…
COMMENT ON FUNCTION get_changed_pages(jsonb) IS
'Notion í˜ì´ì§€ ë©”íƒ€ë°ì´í„°ì™€ DB ë¹„êµí•˜ì—¬ ì‹ ê·œ/ìˆ˜ì • í˜ì´ì§€ ID ë°˜í™˜.

ì…ë ¥ í˜•ì‹:
[
  {"id": "page-uuid", "last_edited": "2024-01-15T14:30:00+00:00"},
  ...
]

ì¶œë ¥ í˜•ì‹:
{
  "new_page_ids": ["uuid1", ...],
  "updated_page_ids": ["uuid2", ...],
  "total_checked": 726,
  "unchanged_count": 700
}

ì—ëŸ¬ ì‹œ:
{
  "error": "ì—ëŸ¬ ë©”ì‹œì§€",
  "new_page_ids": [],
  "updated_page_ids": []
}
';
```

**ë°°í¬ ì ˆì°¨:**
1. Supabase Dashboard â†’ SQL Editor
2. ìœ„ SQL ë³µì‚¬ â†’ Run
3. í™•ì¸: `SELECT * FROM pg_proc WHERE proname = 'get_changed_pages';`
4. Git commit

---

#### Phase 2: Python ì½”ë“œ ì¬êµ¬í˜„ (45ë¶„)

**íŒŒì¼:** `backend/services/supabase_service.py`
**ë¼ì¸:** 881-979 (ê¸°ì¡´ `get_pages_to_fetch()` ì „ì²´ êµì²´)

```python
async def get_pages_to_fetch(
    self,
    notion_pages: List[Dict[str, Any]]
) -> tuple[List[str], List[str]]:
    """
    Compare Notion pages with DB using server-side RPC.

    Uses PostgreSQL function for efficient change detection.
    Falls back to full table scan if RPC fails.

    Args:
        notion_pages: List of page metadata from Notion API
            Each page must have: id, last_edited_time

    Returns:
        Tuple of (new_page_ids, updated_page_ids)

    Performance:
        - RPC mode: ~150ms (constant time, scales to 100k pages)
        - Fallback mode: ~110ms (current size)
        - Network: Only changed pages (0.5KB vs 60KB)

    Example:
        >>> pages = [{"id": "abc", "last_edited_time": "2024-01-15T14:30:00.000Z"}]
        >>> new, updated = await service.get_pages_to_fetch(pages)
        >>> print(f"New: {len(new)}, Updated: {len(updated)}")
    """
    await self._ensure_initialized()

    # Prepare data for RPC
    pages_json = []
    force_new_ids = []  # Pages with invalid timestamps â†’ treat as new

    for p in notion_pages:
        page_id = p.get("id")
        last_edited = p.get("last_edited_time")

        if not page_id:
            logger.warning("Page missing 'id' field, skipping")
            continue

        if not last_edited:
            logger.warning(f"Page {page_id} missing 'last_edited_time', treating as new")
            force_new_ids.append(page_id)
            continue

        try:
            # Parse ISO 8601 timestamp
            notion_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
            
            # Truncate to seconds (match SQL function behavior)
            notion_time = notion_time.replace(microsecond=0)

            pages_json.append({
                "id": page_id,
                "last_edited": notion_time.isoformat()
            })
        except (ValueError, AttributeError, TypeError) as e:
            logger.warning(f"Invalid timestamp for {page_id}: {e}, treating as new")
            force_new_ids.append(page_id)

    if not pages_json and not force_new_ids:
        logger.warning("No valid pages to check")
        return [], []

    logger.info(f"Change detection: checking {len(pages_json)} pages via RPC (sample: {[p['id'] for p in pages_json[:3]]})")

    # Try RPC change detection (Solution 3)
    try:
        import time
        start_time = time.time()

        response = await self.client.rpc('get_changed_pages', {
            'pages_data': pages_json
        }).execute()

        elapsed = time.time() - start_time

        # Validate response structure
        if not response.data or not isinstance(response.data, dict):
            raise ValueError("Invalid RPC response format: expected dict")

        result = response.data

        # Check for SQL function error
        if 'error' in result:
            raise ValueError(f"SQL function error: {result['error']} (SQLSTATE: {result.get('error_detail', 'unknown')})")

        # Extract results
        new_page_ids = result.get('new_page_ids', [])
        updated_page_ids = result.get('updated_page_ids', [])

        # Validate types
        if not isinstance(new_page_ids, list):
            raise ValueError(f"Invalid type for new_page_ids: {type(new_page_ids)}")
        if not isinstance(updated_page_ids, list):
            raise ValueError(f"Invalid type for updated_page_ids: {type(updated_page_ids)}")

        # Add force_new pages
        new_page_ids.extend(force_new_ids)

        # Validate UUIDs
        import re
        UUID_PATTERN = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
        
        for page_id in new_page_ids + updated_page_ids:
            if not UUID_PATTERN.match(page_id):
                raise ValueError(f"Invalid UUID format: {page_id}")

        logger.info(
            f"âœ… RPC change detection completed in {elapsed:.2f}s: "
            f"{len(new_page_ids)} new, {len(updated_page_ids)} updated, "
            f"{result.get('unchanged_count', len(pages_json) - len(new_page_ids) - len(updated_page_ids))} unchanged"
        )

        return new_page_ids, updated_page_ids

    except Exception as rpc_error:
        logger.error(f"âŒ RPC change detection failed: {rpc_error}, falling back to full table scan")

        # Fallback: Full table scan (ë°©ì‹ A)
        try:
            logger.info("Using fallback: full table scan")
            
            response = await (
                self.client.table("raw_notes")
                .select("notion_page_id, notion_last_edited_time")
                .execute()
            )

            # Build existing_map from DB
            existing_map = {}
            for row in response.data:
                db_page_id = row["notion_page_id"]
                db_time = row["notion_last_edited_time"]

                # Parse timestamp
                if isinstance(db_time, str):
                    db_time = datetime.fromisoformat(db_time.replace("Z", "+00:00"))

                # Ensure timezone-aware
                if db_time.tzinfo is None:
                    db_time = db_time.replace(tzinfo=timezone.utc)

                # Truncate to seconds
                db_time = db_time.replace(microsecond=0)
                existing_map[db_page_id] = db_time

            # Build page_map from Notion pages
            page_map = {}
            for p_json in pages_json:
                page_id = p_json["id"]
                notion_time = datetime.fromisoformat(p_json["last_edited"])
                page_map[page_id] = notion_time

            # Compare
            new_ids = []
            updated_ids = []

            for page_id, notion_time in page_map.items():
                if page_id not in existing_map:
                    new_ids.append(page_id)
                elif notion_time > existing_map[page_id]:
                    updated_ids.append(page_id)

            # Add force_new pages
            new_ids.extend(force_new_ids)

            logger.info(
                f"âœ… Fallback completed: {len(new_ids)} new, {len(updated_ids)} updated, "
                f"{len(page_map) - len(new_ids) - len(updated_ids)} unchanged"
            )

            return new_ids, updated_ids

        except Exception as fallback_error:
            logger.error(f"âŒ Fallback also failed: {fallback_error}, treating all as new (last resort)")
            
            # Last resort: treat all as new
            all_ids = [p["id"] for p in pages_json] + force_new_ids
            return all_ids, []
```

**ë³€ê²½ ì‚¬í•­:**
- âœ… RPC ìš°ì„  ì‚¬ìš©
- âœ… ìƒì„¸í•œ ê²€ì¦ (ì‘ë‹µ í˜•ì‹, UUID)
- âœ… 3ë‹¨ê³„ Fallback (RPC â†’ ë°©ì‹ A â†’ ì „ì²´ new)
- âœ… ì„±ëŠ¥ ì¸¡ì • ë° ë¡œê¹…
- âœ… ì—ëŸ¬ë³„ ëª…í™•í•œ ë¡œê·¸

---

#### Phase 3: Startup ê²€ì¦ ì¶”ê°€ (15ë¶„)

**íŒŒì¼ 1:** `backend/services/supabase_service.py` (ë©”ì„œë“œ ì¶”ê°€)

```python
async def validate_rpc_function_exists(self) -> bool:
    """
    Check if RPC function is deployed in Supabase.
    
    Returns:
        bool: True if function exists and works, False otherwise
    """
    try:
        # Test with empty array
        response = await self.client.rpc('get_changed_pages', {
            'pages_data': []
        }).execute()
        
        # Validate response
        if not response.data or not isinstance(response.data, dict):
            logger.warning("âš ï¸  RPC function returned unexpected format")
            return False
            
        logger.info("âœ… RPC function 'get_changed_pages' is available and working")
        return True
        
    except Exception as e:
        logger.warning(f"âš ï¸  RPC function 'get_changed_pages' not available: {e}")
        logger.warning("   Import will use fallback mode (full table scan)")
        return False
```

**íŒŒì¼ 2:** `backend/config.py` (ì„¤ì • ì¶”ê°€)

```python
class Settings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...
    
    VALIDATE_RPC_ON_STARTUP: bool = Field(
        default=True,
        description="Validate RPC function availability on startup"
    )
```

**íŒŒì¼ 3:** `backend/main.py` (startup ì´ë²¤íŠ¸ ì¶”ê°€)

```python
@app.on_event("startup")
async def startup_validation():
    """Validate critical dependencies on startup."""
    logger.info("=" * 80)
    logger.info("STARTUP VALIDATION")
    logger.info("=" * 80)
    
    supabase_service = SupabaseService()
    
    # Validate RPC function
    if settings.validate_rpc_on_startup:
        await supabase_service.validate_rpc_function_exists()
    
    logger.info("=" * 80)
```

---

#### Phase 4: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (45ë¶„)

**íŒŒì¼:** `backend/tests/unit/test_supabase_change_detection_rpc.py` (ìƒˆë¡œ ìƒì„±)

```python
"""
Unit tests for RPC-based change detection.
Tests the get_pages_to_fetch() method with Solution 3 (RPC).
"""
import pytest
from datetime import datetime, timezone
from unittest.mock import AsyncMock, MagicMock

@pytest.mark.asyncio
async def test_rpc_detects_new_pages(supabase_service):
    """ì‹ ê·œ í˜ì´ì§€ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    notion_pages = [
        {"id": "new-page-1", "last_edited_time": "2024-01-15T14:30:00.000Z"},
        {"id": "new-page-2", "last_edited_time": "2024-01-15T15:00:00.000Z"}
    ]

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert "new-page-1" in new_ids
    assert "new-page-2" in new_ids
    assert len(updated_ids) == 0

@pytest.mark.asyncio
async def test_rpc_detects_updated_pages(supabase_service):
    """ìˆ˜ì •ëœ í˜ì´ì§€ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    # Setup: Insert existing page with old timestamp
    old_time = datetime(2024, 1, 1, 12, 0, 0, tzinfo=timezone.utc)
    await supabase_service.upsert_raw_note({
        "notion_page_id": "existing-page",
        "notion_url": "https://notion.so/existing-page",
        "title": "Test Page",
        "content": "Old content",
        "properties_json": {},
        "notion_created_time": old_time,
        "notion_last_edited_time": old_time
    })

    # Test: Notion shows newer timestamp
    notion_pages = [{
        "id": "existing-page",
        "last_edited_time": "2024-01-15T14:30:00.000Z"
    }]

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert len(new_ids) == 0
    assert "existing-page" in updated_ids

@pytest.mark.asyncio
async def test_rpc_skips_unchanged_pages(supabase_service):
    """ë³€ê²½ ì—†ëŠ” í˜ì´ì§€ skip í…ŒìŠ¤íŠ¸"""
    timestamp = datetime(2024, 1, 15, 14, 30, 0, tzinfo=timezone.utc)

    # Setup: Insert page with exact timestamp
    await supabase_service.upsert_raw_note({
        "notion_page_id": "unchanged-page",
        "notion_url": "https://notion.so/unchanged-page",
        "title": "Unchanged",
        "content": "Same content",
        "properties_json": {},
        "notion_created_time": timestamp,
        "notion_last_edited_time": timestamp
    })

    # Test: Same timestamp in Notion
    notion_pages = [{
        "id": "unchanged-page",
        "last_edited_time": timestamp.isoformat()
    }]

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert len(new_ids) == 0
    assert len(updated_ids) == 0
    # Page should be skipped

@pytest.mark.asyncio
async def test_rpc_handles_mixed_scenarios(supabase_service):
    """ì‹ ê·œ + ìˆ˜ì • + ë³€ê²½ì—†ìŒ í˜¼í•© ì‹œë‚˜ë¦¬ì˜¤"""
    old_time = datetime(2024, 1, 1, 12, 0, 0, tzinfo=timezone.utc)
    same_time = datetime(2024, 1, 15, 14, 0, 0, tzinfo=timezone.utc)

    # Setup: 2 existing pages
    await supabase_service.upsert_raw_note({
        "notion_page_id": "old-page",
        "notion_url": "https://notion.so/old",
        "title": "Old",
        "content": "Old",
        "properties_json": {},
        "notion_created_time": old_time,
        "notion_last_edited_time": old_time
    })
    await supabase_service.upsert_raw_note({
        "notion_page_id": "same-page",
        "notion_url": "https://notion.so/same",
        "title": "Same",
        "content": "Same",
        "properties_json": {},
        "notion_created_time": same_time,
        "notion_last_edited_time": same_time
    })

    # Test: 1 new, 1 updated, 1 unchanged
    notion_pages = [
        {"id": "new-page", "last_edited_time": "2024-01-15T14:30:00.000Z"},  # New
        {"id": "old-page", "last_edited_time": "2024-01-15T15:00:00.000Z"},  # Updated
        {"id": "same-page", "last_edited_time": same_time.isoformat()}  # Unchanged
    ]

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert "new-page" in new_ids
    assert "old-page" in updated_ids
    assert "same-page" not in new_ids
    assert "same-page" not in updated_ids

@pytest.mark.asyncio
async def test_rpc_fallback_on_function_missing(supabase_service, monkeypatch):
    """RPC í•¨ìˆ˜ ì—†ì„ ë•Œ Fallback ì‘ë™ í…ŒìŠ¤íŠ¸"""
    # Mock: RPC í˜¸ì¶œ ì‹¤íŒ¨ (function does not exist)
    async def mock_rpc(*args, **kwargs):
        raise Exception("function get_changed_pages does not exist")

    monkeypatch.setattr(supabase_service.client, "rpc", mock_rpc)

    notion_pages = [
        {"id": "test-page", "last_edited_time": "2024-01-15T14:30:00.000Z"}
    ]

    # Should not raise, should use fallback
    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert isinstance(new_ids, list)
    assert isinstance(updated_ids, list)
    # Fallback treats unknown pages as new
    assert "test-page" in new_ids

@pytest.mark.asyncio
async def test_rpc_handles_invalid_timestamp(supabase_service):
    """ì˜ëª»ëœ timestamp í˜•ì‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    notion_pages = [
        {"id": "valid-page", "last_edited_time": "2024-01-15T14:30:00.000Z"},
        {"id": "invalid-page", "last_edited_time": "invalid-format"},
        {"id": "missing-page"}  # No last_edited_time
    ]

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    # All should be treated as new (invalid = force new)
    assert "valid-page" in new_ids
    assert "invalid-page" in new_ids
    assert "missing-page" in new_ids

@pytest.mark.asyncio
async def test_rpc_validates_uuid_format(supabase_service, monkeypatch):
    """UUID í˜•ì‹ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    # Mock: RPC returns invalid UUID
    async def mock_rpc(*args, **kwargs):
        mock_response = MagicMock()
        mock_response.data = {
            "new_page_ids": ["not-a-uuid", "also-invalid"],
            "updated_page_ids": []
        }
        return mock_response

    monkeypatch.setattr(supabase_service.client, "rpc", mock_rpc)

    notion_pages = [{"id": "test", "last_edited_time": "2024-01-15T14:30:00.000Z"}]

    # Should fallback due to invalid UUID
    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    # Fallback mode should work
    assert isinstance(new_ids, list)
    assert isinstance(updated_ids, list)

@pytest.mark.asyncio
async def test_rpc_handles_empty_input(supabase_service):
    """ë¹ˆ ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    notion_pages = []

    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert new_ids == []
    assert updated_ids == []

@pytest.mark.asyncio
async def test_rpc_handles_sql_error(supabase_service, monkeypatch):
    """SQL í•¨ìˆ˜ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    # Mock: SQL function returns error
    async def mock_rpc(*args, **kwargs):
        mock_response = MagicMock()
        mock_response.data = {
            "error": "division by zero",
            "error_detail": "22012",
            "new_page_ids": [],
            "updated_page_ids": []
        }
        return mock_response

    monkeypatch.setattr(supabase_service.client, "rpc", mock_rpc)

    notion_pages = [{"id": "test", "last_edited_time": "2024-01-15T14:30:00.000Z"}]

    # Should fallback due to SQL error
    new_ids, updated_ids = await supabase_service.get_pages_to_fetch(notion_pages)

    assert isinstance(new_ids, list)
    assert isinstance(updated_ids, list)
```

---

#### Phase 5: í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (30ë¶„)

**íŒŒì¼:** `backend/tests/integration/test_incremental_import_rpc.py` (ìƒˆë¡œ ìƒì„±)

```python
"""
Integration tests for RPC-based incremental import.
Tests end-to-end import flow with real API calls.
"""
import pytest
from httpx import AsyncClient
import asyncio

@pytest.mark.integration
@pytest.mark.asyncio
async def test_initial_import_all_pages(client: AsyncClient):
    """ì´ˆê¸° import: ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
    response = await client.post("/pipeline/import-from-notion?page_size=100")
    
    assert response.status_code == 202
    job_id = response.json()["job_id"]

    # Wait for completion
    for _ in range(60):  # ìµœëŒ€ 10ë¶„
        status_response = await client.get(f"/pipeline/import-status/{job_id}")
        job = status_response.json()
        
        if job["status"] == "completed":
            break
        elif job["status"] == "failed":
            pytest.fail(f"Job failed: {job.get('error_message')}")
        
        await asyncio.sleep(10)
    
    # Assertions
    assert job["status"] == "completed"
    assert job["imported_pages"] == 726  # All pages
    assert job["skipped_pages"] == 0  # None skipped (first import)
    assert job["processed_pages"] == 726

@pytest.mark.integration
@pytest.mark.asyncio
async def test_reimport_without_changes(client: AsyncClient):
    """ì¬ì‹¤í–‰: ë³€ê²½ ì—†ìŒ (ì „ì²´ skip ì˜ˆìƒ)"""
    # Run import twice
    response1 = await client.post("/pipeline/import-from-notion?page_size=100")
    job_id_1 = response1.json()["job_id"]
    
    # Wait for first to complete
    await wait_for_job(client, job_id_1)
    
    # Run again immediately
    response2 = await client.post("/pipeline/import-from-notion?page_size=100")
    job_id_2 = response2.json()["job_id"]
    
    await wait_for_job(client, job_id_2)
    
    # Check second job
    status = await client.get(f"/pipeline/import-status/{job_id_2}")
    job = status.json()
    
    # Assertions
    assert job["status"] == "completed"
    assert job["imported_pages"] == 0  # âœ… Nothing changed
    assert job["skipped_pages"] == 726  # âœ… All skipped
    assert job["processed_pages"] == 726

@pytest.mark.integration
@pytest.mark.asyncio
async def test_import_with_one_new_page(client: AsyncClient, notion_api_mock):
    """1ê°œ í˜ì´ì§€ ì¶”ê°€ í›„ ì¬ì‹¤í–‰"""
    # First import
    response1 = await client.post("/pipeline/import-notion?page_size=100")
    await wait_for_job(client, response1.json()["job_id"])
    
    # Simulate: User adds 1 page in Notion
    notion_api_mock.add_page({
        "id": "new-page-id",
        "last_edited_time": "2024-01-15T16:00:00.000Z",
        "title": "New Page"
    })
    
    # Second import
    response2 = await client.post("/pipeline/import-from-notion?page_size=100")
    await wait_for_job(client, response2.json()["job_id"])
    
    status = await client.get(f"/pipeline/import-status/{response2.json()['job_id']}")
    job = status.json()
    
    # Assertions
    assert job["imported_pages"] == 1  # Only new page
    assert job["skipped_pages"] == 726  # Old pages skipped

@pytest.mark.integration
@pytest.mark.asyncio
async def test_import_with_ten_modified_pages(client: AsyncClient, notion_api_mock):
    """10ê°œ í˜ì´ì§€ ìˆ˜ì • í›„ ì¬ì‹¤í–‰"""
    # First import
    response1 = await client.post("/pipeline/import-from-notion?page_size=100")
    await wait_for_job(client, response1.json()["job_id"])
    
    # Simulate: User modifies 10 pages
    for i in range(10):
        notion_api_mock.update_page(
            page_id=f"existing-page-{i}",
            last_edited_time="2024-01-15T17:00:00.000Z"
        )
    
    # Second import
    response2 = await client.post("/pipeline/import-from-notion?page_size=100")
    await wait_for_job(client, response2.json()["job_id"])
    
    status = await client.get(f"/pipeline/import-status/{response2.json()['job_id']}")
    job = status.json()
    
    # Assertions
    assert job["imported_pages"] == 10  # 10 modified
    assert job["skipped_pages"] == 716  # 726 - 10

@pytest.mark.integration
@pytest.mark.asyncio
async def test_rpc_performance(client: AsyncClient):
    """RPC ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    import time
    
    # Measure import time with no changes (should be fast)
    response1 = await client.post("/pipeline/import-from-notion?page_size=100")
    await wait_for_job(client, response1.json()["job_id"])
    
    start_time = time.time()
    response2 = await client.post("/pipeline/import-from-notion?page_size=100")
    await wait_for_job(client, response2.json()["job_id"])
    elapsed = time.time() - start_time
    
    # Should complete in < 5 seconds (no content fetching)
    assert elapsed < 5.0, f"Import took {elapsed:.2f}s, expected < 5s"

# Helper function
async def wait_for_job(client: AsyncClient, job_id: str, timeout: int = 600):
    """Wait for job to complete."""
    for _ in range(timeout // 10):
        response = await client.get(f"/pipeline/import-status/{job_id}")
        job = response.json()
        
        if job["status"] in ["completed", "failed"]:
            return job
        
        await asyncio.sleep(10)
    
    pytest.fail(f"Job {job_id} timeout after {timeout}s")
```

---

### âš ï¸ ì˜ˆìƒ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ | ëŒ€ì‘ ë°©ì•ˆ |
|-------|------|------|---------|
| SQL í•¨ìˆ˜ ë°°í¬ ì‹¤íŒ¨ | ë‚®ìŒ | ì¤‘ê°„ | Fallback ìë™ ì‘ë™, ìˆ˜ë™ ì¬ë°°í¬ |
| RPC í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ | ë‚®ìŒ | ë‚®ìŒ | Fallback to ë°©ì‹ A |
| UUID ê²€ì¦ ì˜¤íƒì§€ | ë§¤ìš° ë‚®ìŒ | ë‚®ìŒ | ë¡œê·¸ í™•ì¸ í›„ íŒ¨í„´ ìˆ˜ì • |
| ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬ (>200ms) | ë‚®ìŒ | ì¤‘ê°„ | SQL ìµœì í™” (JOIN ë°©ì‹ ì „í™˜) |
| Notion API í˜•ì‹ ë³€ê²½ | ë§¤ìš° ë‚®ìŒ | ë†’ìŒ | ëª¨ë‹ˆí„°ë§ + ê¸´ê¸‰ Hotfix |
| DB ì—°ê²° ëŠê¹€ | ë‚®ìŒ | ë†’ìŒ | Retry ë¡œì§ + ì—ëŸ¬ í•¸ë“¤ë§ |

---

### ğŸ”„ ë¡¤ë°± ê³„íš

**ê¸´ê¸‰ ë¡¤ë°± ì˜µì…˜ 1: í™˜ê²½ ë³€ìˆ˜**
```bash
# .env
USE_RPC_CHANGE_DETECTION=false

# supabase_service.py ìˆ˜ì •
if not os.getenv("USE_RPC_CHANGE_DETECTION", "true").lower() == "true":
    logger.warning("RPC disabled by env var")
    return await self._full_scan_fallback(notion_pages)
```

**ê¸´ê¸‰ ë¡¤ë°± ì˜µì…˜ 2: Git revert**
```bash
git revert <commit-hash>
git push origin main
```

**ë¡¤ë°± íŠ¸ë¦¬ê±°:**
- RPC í•¨ìˆ˜ ë°°í¬ ì‹¤íŒ¨í•˜ê³  Fallbackë„ ì‘ë™ ì•ˆ í•¨
- ì„±ëŠ¥ì´ ê¸°ì¡´ë³´ë‹¤ ëŠë ¤ì§ (>10ì´ˆ)
- ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ ë°œê²¬ (í˜ì´ì§€ ëˆ„ë½)

---

### âœ… ì„±ê³µ ê¸°ì¤€

**ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­:**
- âœ… ì´ˆê¸° import: 726ê°œ ì „ì²´ ì²˜ë¦¬
- âœ… ì¬ì‹¤í–‰ (ë³€ê²½ ì—†ìŒ): 0 imported, 726 skipped
- âœ… 1ê°œ ì¶”ê°€: 1 imported, 726 skipped
- âœ… 10ê°œ ìˆ˜ì •: 10 imported, 716 skipped
- âœ… RPC ì‹¤íŒ¨ ì‹œ Fallback ì‘ë™

**ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­:**
- âœ… RPC ì‘ë‹µ: < 200ms
- âœ… Fallback ì‘ë‹µ: < 300ms
- âœ… ì „ì²´ import (ë³€ê²½ ì—†ìŒ): < 5ì´ˆ

**ì•ˆì •ì„± ìš”êµ¬ì‚¬í•­:**
- âœ… RPC ì‹¤íŒ¨í•´ë„ import ì™„ë£Œ
- âœ… ëª¨ë“  ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
- âœ… ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

**í…ŒìŠ¤íŠ¸ ìš”êµ¬ì‚¬í•­:**
- âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ 10ê°œ í†µê³¼
- âœ… í†µí•© í…ŒìŠ¤íŠ¸ 5ê°œ í†µê³¼
- âœ… ì»¤ë²„ë¦¬ì§€ > 80%

---

### ğŸ“Š êµ¬í˜„ ì‹œê°„ ì˜ˆìƒ

| Phase | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ëˆ„ì  ì‹œê°„ |
|-------|------|----------|----------|
| 1 | SQL í•¨ìˆ˜ ìƒì„± ë° ë°°í¬ | 30ë¶„ | 30ë¶„ |
| 2 | Python ì½”ë“œ ì¬êµ¬í˜„ | 45ë¶„ | 1ì‹œê°„ 15ë¶„ |
| 3 | Startup ê²€ì¦ ì¶”ê°€ | 15ë¶„ | 1ì‹œê°„ 30ë¶„ |
| 4 | ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± | 45ë¶„ | 2ì‹œê°„ 15ë¶„ |
| 5 | í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ | 30ë¶„ | 2ì‹œê°„ 45ë¶„ |
| **ì´ê³„** | | **2ì‹œê°„ 45ë¶„** | |

**ë²„í¼ í¬í•¨:** 3ì‹œê°„ (ë””ë²„ê¹… 15ë¶„ ì¶”ê°€)

---

## ğŸ¯ ìµœì¢… í™•ì¸ ì‚¬í•­

êµ¬í˜„ ì‹œì‘ ì „ í™•ì¸:
- [ ] Supabase ì ‘ê·¼ ê¶Œí•œ í™•ì¸
- [ ] SQL Editor ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
- [ ] ê¸°ì¡´ ì½”ë“œ ë°±ì—… ì™„ë£Œ
- [ ] í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„
- [ ] Git ë¸Œëœì¹˜ ìƒì„± (`feature/incremental-import-rpc`)

êµ¬í˜„ ì¤‘ ì£¼ì˜:
- [ ] ê° Phase ì™„ë£Œ í›„ Git commit
- [ ] í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸ í›„ ë‹¤ìŒ Phase ì§„í–‰
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ ìº¡ì²˜
- [ ] ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ê¸°ë¡

êµ¬í˜„ ì™„ë£Œ í›„:
- [ ] ì „ì²´ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡
- [ ] Pull Request ìƒì„±
- [ ] íŒ€ì› ë¦¬ë·° ìš”ì²­

---

## ğŸ”´ CRITICAL PRE-IMPLEMENTATION VERIFICATION REQUIRED
